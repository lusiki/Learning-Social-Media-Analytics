---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 5: Statistics"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=TRUE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
```


# OUTLINE

1. DESCRIPTIVE STATISTICS

2. COMPARING MEANS

3. CATEGORICAL ANALYSIS

4. ANOVA

5. LINEAR REGRESSION



# DESCRIPTIVE STATISTICS




```{r, echo=FALSE, eval = T}
# FULL SAMPLE
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel) %>% data.table()
#all_raw <- all_raw  %>%  mutate(DATE = as.Date(DATE,"%Y-%m-%d" ))
```

```{r}
# sneek peak
glimpse(all_raw[sample(nrow(all_raw)),1:8])
# size of the data
dim(all_raw)
# time rage
range(all_raw$DATE)
```





```{r}
# how much activity
all_raw[SOURCE_TYPE == "web", .N]
# how many authors
all_raw[SOURCE_TYPE == "web", length(unique(all_raw$AUTHOR))]
# how many domains?
all_raw[SOURCE_TYPE == "web", length(unique(all_raw$FROM))]
# how many domains?
all_raw[SOURCE_TYPE == "web",
        .(Number = length(unique(FROM)))]
# how many CRO domains?
all_raw[SOURCE_TYPE == "web" & grepl(".hr", unique(all_raw$FROM)),
        .(Number = length(unique(FROM)))]

```

```{r, echo = FALSE}
web <- all_raw[SOURCE_TYPE == "web",]
```



```{r}
# some domains?
web[,.(UniqueDomains = unique(FROM))] %>% sample_n(5)
# some CRO domains?
web[str_detect(web$FROM, "hr"),.(UniqueDomains = unique(FROM))] %>% sample_n(5)
```



```{r}
# check all domains
web[str_detect(web$FROM, ".hr"),.N,
        FROM][order(-N),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))
```



```{r}
web[str_detect(web$FROM, ".hr"),.N,FROM][N >= 50,][order(-N),] %>%
  ggplot(., aes(N)) +
  geom_histogram(bins = 10, color = "#000000", fill = "#0099F8") + 
  geom_vline(aes(xintercept = mean(N)), color = "#000000", size = 1.25) +
  geom_vline(aes(xintercept = mean(N) + sd(N)), color = "#000000", size = 1, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(N) - sd(N)), color = "#000000", size = 1, linetype = "dashed") +
  labs(
    title = "Histogram of portal activity in Croatia",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "Number of articles",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  ) -> gg1
```




```{r , echo=FALSE}
gg1
```




```{r}
# just another view
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][, avg := (N / All)*100][order(-N),] 
```




```{r}

# check overall descriptives (number of articles)
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]

# check ranking by No. of articles
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-N),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )


# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
    .(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(COMMENT),
                                                        stdev = sd(COMMENT),
                                                        total = sum(COMMENT))]

# check ranking by No. of comments
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-COMMENT),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))


# check overall descriptives (number of likes)
web[str_detect(web$FROM, ".hr"),
    .(LIKE = sum(LIKE_COUNT, na.rm = T)),FROM][,.(mean = mean(LIKE),
                                                        stdev = sd(LIKE),
                                                        total = sum(LIKE))]
# check ranking by No. of likes
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-LIKE),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))


# check overall descriptives (reach)
web[str_detect(web$FROM, ".hr"),
    .(REACH = sum(REACH, na.rm = T)),FROM][,.(mean = mean(REACH),
                                                        stdev = sd(REACH),
                                                        total = sum(REACH))]
# check ranking by reach
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-REACH),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))





```

# COMPARING MEANS

Lets compare everage activity of first 100th portals by different criteria(comments,likes,reach).

We first have to subset these rankings:

```{r}
# select data to object
webRanking <- web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM]
# filter 100 biggest by criteria
portals100 <-  webRanking[head(order(-N),100),.(FROM,N)]
reach100 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)]
like100 <- webRanking[head(order(-LIKE),100),.(FROM,N, LIKE)]
comment100 <- webRanking[head(order(-COMMENT),100),.(FROM,N, COMMENT)]

```

Now lets use one sided t-test to check if 100 biggest potals publish average number of articles (482;see before).

```{r}
# library from the lsr boook
library(lsr)
# do the test
oneSampleTTest(portals100$N, mu=482 )

```


Our data is not intependent so we use paired test:

```{r}
# first check reach vs. like
 t.test( 
   x = reach100$N, # variable 1 is the number of articles for 100 biggest portals by reach
   y = like100$N, # variable 2 is the number of articles for 100 biggest portals by like
   paired = TRUE # paired test
 )

# second check reach vs. comment

t.test( 
   x = reach100$N, 
   y = comment100$N, 
   paired = TRUE 
 )

# third check comment vs. like

t.test( 
   x = comment100$N,
   y = like100$N, 
   paired = TRUE 
 )

# fourth check comment vs. reach

t.test( 
   x = comment100$N, 
   y = reach100$N,
   paired = TRUE # paired test
 )


```


Runings  independent t tests requires some indexing:

```{r}
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[order(-REACH),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[order(-LIKE),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[order(-COMMENT),.(FROM,N, COMMENT)] %>% slice(101:201)
```

Now we can compare if the activity (mean published articles) of 1th and 2nd hundred biggest portals is the same:

```{r}
# first we need to check the date to chose the right test 
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)

# then run the test
t.test( 
   x = portals100$N, 
   y = portals200$N,
   var.equal = FALSE, # Welch test
   paired = FALSE # independent test
 )

# second check for the reach
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)

# then run the test
t.test( 
   x = reach100$N, 
   y = reach200$N,
   var.equal = TRUE, # Student test
   paired = FALSE # independent test
 )
```

To see how good these tests are, one could also check the distributional properties (normality) with histogram, QQ plot and/or Shapiro-Wilk test.


# CATEGORICAL ANALYSIS

Here we have two (famous) tests: **Goodnes of Fit** and **Independence of association**.


### GOODNES OF FIT (GOF)

To run the GOF test we will use the full dataset (not just web) to see weather all of the networks are equally important. Lets explore the data once again:

```{r}
# check the activity per network
table(all_raw$SOURCE_TYPE)
# run the GOF test
goodnessOfFitTest(as.factor(all_raw$SOURCE_TYPE))
```

Different kind of probability specification is possible. For example, we want to test if the probablility of the web activity in the whole social media space equals 40%. To complete the example lets first do some data tweaking:

```{r}
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
# check the size of web vs. other media
table(all_raw$Network)
```


Now, lets run the test:


```{r}
# specify the probablities
probs = c(other = 0.60, web = 0.40)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
```

### CHISQ TEST OF INDEPENDENCE

For this test we need to have at least two factorial variables. Lets make some new variables:

```{r}
# package
library(anytime)
library(lubridate)
# convert chr date to date format
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Night", "Morning", "Afternoon", "Evening")
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
                         breaks = breaks,
                         labels = labels,
                         include.lowest=TRUE)][,INTERDAYTIME := as.factor()]
```

Now, we should check how these variables look like:

```{r}

# check the result
table(all_raw$INTERDAYTIME)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)

# we are interested in cross-tabulated View
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)

```

Finally, lets run the test:

```{r}
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
```



































# Thank you for your attention!





















