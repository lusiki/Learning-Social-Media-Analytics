---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 7: Text Analysis"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
```


# OUTLINE
<br>
<br>
<br>
- DATA IMPORT 
<br>
<br>
- PREPARE TEXT DATA
<br>
<br>
- 
<br>
<br>



# DATA IMPORT

#### ARTICLE DATA

```{r, echo=T}
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
```


```{r}
# some basic data wrangle
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
```

```{r}
# some basic data overview
options(digits = 2)
# data size 
dim(posts)
# time span
range(posts$DATETIME)
# descriptives
posts[,.N,SOURCE_TYPE][order(-N)]
# how many letters in a title
posts[,
       .(Avg = mean(nchar(TITLE), na.rm = T),
         STD = sd(nchar(TITLE), na.rm = T),
         min = min(nchar(TITLE), na.rm = T),
         max = max(nchar(TITLE), na.rm = T)),
      SOURCE_TYPE][order(-Avg),]
# how many letters in a text
posts[,
       .(Avg = mean(nchar(FULL_TEXT)),
         STD = sd(nchar(FULL_TEXT)),
         min = min(nchar(FULL_TEXT)),
         max = max(nchar(FULL_TEXT))),
      SOURCE_TYPE][order(-Avg),]
```

#### LEXICON DATA

```{r leksikoni, message=F, warning=F}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
head(sample_n(CroSentilex_Gold,100),15)
 
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae"
  ),
  lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
```


# PREPARE TEXT DATA

### CHECK DATA AND TOKENIZE 

```{r tokeni}
#web <- posts[SOURCE_TYPE == "web",]
# fb <- posts[SOURCE_TYPE == "facebook",]
# youtube <- posts[SOURCE_TYPE == "youtube",]
instagram <- posts[SOURCE_TYPE == "instagram",]
rm(posts)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
printMD(instagram[,.N,FROM][order(-N)], big.mark=",")
# influencers by REACH
printMD(unique(instagram[,REACH := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,REACH)][order(-REACH),]), big.mark=",")
# influencers by LIKE
printMD(unique(instagram[,LIKE := sum(LIKE_COUNT), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE),]), big.mark=",")
# influencers by INTERACTIONS
printMD(unique(instagram[,INTERACTIONS := sum(INTERACTIONS), FROM][FROM != "anonymous_user",.(FROM,INTERACTIONS)][order(-INTERACTIONS),]), big.mark=",")
# influencers by COMMENT
printMD(unique(instagram[,COMMENT := sum(COMMENT_COUNT), FROM][FROM != "anonymous_user",.(FROM,COMMENT)][order(-COMMENT),]), big.mark=",")
# dim before tokenize
dim(instagram)

# tokenize
instagram %>% 
  unnest_tokens(word, FULL_TEXT) -> insta
# dim after tokenize
dim(insta)
# check
insta %>% 
  sample_n(.,10)
```


### CLEAN

```{r}
# remove stop words, numbers, single letters, NA
insta %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> insta_tokenTidy

insta_tokenTidy %>%
  filter(!is.na(word)) -> insta_tokenTidy


# check
insta_tokenTidy %>%
  sample_n(.,15) %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))

```


# ANALYSE


### BASIC FREQUENCIES
```{r}
## Most common words
printMD(insta_tokenTidy[,.N,by = word][order(-N),][N > 1500], big.mark=",")

## Vizualize most common words
insta_tokenTidy[,.N,by = word][N>2000][order(-N),][,word := reorder(word,N)] %>%
  ggplot(aes(word, N)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme_economist()

## Vizualize most common words over time
insta_tokenTidy[,DAY:=floor_date(DATE,"day")][,N:=.N,by=DAY][,gn:=sum(N)][
  word %in% c("akcija", "ponuda", "poklon", "gratis", "kože"),] %>%
  ggplot(., aes(DAY,  N / gn)) + 
   geom_point() +
   ggtitle("Učestalost korištenja riječi") +
   ylab("% ukupnih riječi") +
   geom_smooth() +
   facet_wrap(~ word, scales = "free_y") +
   scale_y_continuous(labels = scales::percent_format())+
   theme_economist()

```

```{r dekriptivnoDom, warning=F,message=F, fig.height=6, fig.width=10}

## Articles per influnecer
#printMD(insta[ ,.N,FROM][N>1500][order(-N)], big.mark=",")


## Articles per domain over time

insta_tokenTidy %>% 
   filter(word == "poklon" & FROM %in% c("elladvornik", "beautypharmacy_hr", "lucija_lugomer_official")) %>%
   mutate(WEEK = floor_date(DATE, "week")) %>%
   group_by(WEEK, FROM) %>%
   summarise(n = n()) %>%
   ungroup() %>%
   arrange(desc(n)) %>%
   ggplot(., aes(WEEK,  n)) + 
   geom_line() +
   ggtitle("Članci o poklonima na najvažnijim IG profilima") +
   ylab("Broj objava") +
   geom_smooth() +
   facet_wrap(~ FROM, scales = "free_y") +
   theme_economist()
```


### SENTIMENT

```{r sentimentTempus, message=F, warning=F}
## Sentiment over time
vizualiziraj_sentiment <- function(dataset, frq = "week") {
dataset %>%
  inner_join( Crosentilex_sve, by = "word") %>%
  filter(!is.na(word)) %>%
  select(word, brija, DATE, sentiment) %>% 
  unique() %>%
  spread(. , brija, sentiment) %>%
  mutate(sentiment = POZ - NEG) %>%
  select(word, DATE, sentiment) %>% 
  group_by(word) %>% 
  mutate(count = n()) %>%
  arrange(desc(count)) %>%
  mutate( score = sentiment*count) %>%
  ungroup() %>%
  group_by(DATE) %>%
  arrange(desc(DATE)) -> sm
 
sm %>%
  select(DATE, score) %>%
  group_by(DATE = floor_date(DATE, frq)) %>%
  summarise(Dnevni_sent = sum(score, na.rm = TRUE)) %>%
  ggplot(., aes(DATE, Dnevni_sent)) +
  geom_bar(stat = "identity") + 
  ggtitle(paste0("Sentiment over time;freqency:", frq)) +
  ylab("SentimentScore") +
  theme_economist()-> gg_sentiment_kroz_vrijeme_qv
gg_sentiment_kroz_vrijeme_qv
}
vizualiziraj_sentiment(insta_tokenTidy,"week")
```



```{r doprinoSentimentu, message=F, warning=F, fig.width=10}
## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
                                 sentiment == 1 ~ "NEGATIVE",
                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  theme_economist() -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(insta_tokenTidy,15)
```



Simple WordCloud:

```{r WCloud, message=F, warning=F}
## WordCloud(vulgaris)
insta_tokenTidy %>%
  anti_join(CroSentilex_Gold,by="word") %>% 
  count(word) %>% 
  arrange(desc(n)) %>%
  top_n(100) %>%
  with(wordcloud(word, n, max.words = 80)) 
```

WordCloud sentimenta:

```{r WCloutSent, warning=F, message=F}
## ComparisonCloud
insta_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)
```


Negativity analysis of influencers:

```{r negDomen, warning=F, message=F}
## Najnegativniji portali
wCount <- insta_tokenTidy %>% 
  group_by(FROM) %>%
  summarise(word = n())
CroSentilex_Gold_neg <- CroSentilex_Gold %>% filter(sentiment == 1)
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
insta_tokenTidy %>% 
  semi_join(CroSentilex_Gold_neg, by= "word") %>%
  group_by(FROM) %>% 
  summarise(negWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(negativnostIndex = (negWords/word)*100) %>%
  arrange(desc(negativnostIndex)) %>%
  printMD(., big.mark=",")
```

...also positivity:

```{r pozDomen, warning=F, message=F}
## Najpozitivniji portali
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
insta_tokenTidy %>% 
  semi_join(CroSentilex_Gold_poz, by= "word") %>%
  group_by(FROM) %>% 
  summarise(pozWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(pozitivnostIndex = (pozWords/word)*100) %>%
  arrange(desc(pozitivnostIndex)) %>%
  printMD(., big.mark=",") 
```


### TERM IMPORTANCE



```{r frekvencija, message=F, warning=F, fig.height=10, fig.width=10}
## Udio riječi po domenama
domenaWords <- insta_tokenTidy %>%
  filter(FROM %in% c("beautypharmacy_hr", "jutarnji.hr", "vecernji.list" )) %>% 
  count(FROM, word, sort = T)
  
ukupnoWords <- domenaWords %>%
  group_by(FROM) %>%
  summarise(totWords = sum(n))
domenaWords <- left_join(domenaWords, ukupnoWords)
# domenaWords %>% head(15)
# domenaWords %>% 
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
  bind_tf_idf(word, FROM, n)
idf %>% head(10)
# idf %>% 
#   select(-totWords) %>%
#   arrange(desc(tf_idf))
idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  mutate(FROM = factor(FROM)) %>%
  group_by(FROM) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() +
  theme_economist()
```



### PHRASES

```{r nGRAMI, message=F, warning=F, fig.height=10, fig.width=10}
insta_bigram <- instagram %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
insta_bigram %>% head(10)
insta_bigram %>%
  count(bigram, sort = T) %>%
  head(15) %>%
  printMD(., big.mark=",")
insta_bigram_sep <- insta_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
insta_bigram_tidy <- insta_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
insta_bigram_tidy_bigram_counts <- insta_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- insta_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ")
#bigrams_united
bigrams_united %>% 
  count(FROM,bigram,sort = T) -> topicBigram
# Najvažniji bigrami po domenama
 bigram_tf_idf <- bigrams_united %>%
  count(FROM, bigram) %>%
  bind_tf_idf(bigram, FROM, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  filter(FROM %in% c("elladvornik", "imerovic_sandi_budivelik", "elaajerkovic" )) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(FROM) %>% 
  top_n(7) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()

rm(posts,instagram,bigram_tf_idf,insta_bigram_sep,insta_bigram_tidy)
```

#### PHRASES CORRELATION



```{r eval = F,message=F, warning=F, fig.height=10, fig.width=10}

insta_tokenTidy %>% 
#  filter(datum > "2020-02-20") %>%
  group_by(word) %>%
  filter(n() > 20) %>%
  filter(!is.na(word)) %>%
  pairwise_cor(word,DATE, sort = T) -> corsWords
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
  filter(item1 %in% c("kupnja", "akcija", "sex", "poklon")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() + 
  theme_economist()
```

### TEMATIC ANALYSIS



```{r TEME, message=F, warning=F, fig.height=10, fig.width=10}
insta_tokenTidy %>%
  count(FROM, word, sort = TRUE) %>%
  cast_dtm(FROM, word,n) -> dtm
insta_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
  drop_na(.) %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + 
  theme_economist()
```



```{r TEMEbigram, eval=T, message=F, warning=F,fig.height=10, fig.width=10}
# Bigrami 
topicBigram %>%
  cast_dtm(FROM, bigram,n) -> dtmB
insta_LDA <- LDA(dtmB, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
  drop_na(.) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + 
  theme_economist()
```















