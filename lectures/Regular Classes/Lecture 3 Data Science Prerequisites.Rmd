---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 3: Data Science Prerequisites"
author: "Luka Sikic, PhD"
date: "Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)" #"`r format(Sys.time(), '%d %B %Y')`"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(rvest)
library(tidyverse)
library(httr)
```


# FETCHING DATA


### Two main approaches:

1. **webscraping (guerrila)**


- takes some skills
<br>
- everything can be scraped (tailor made solutions)
<br>
- takes more time and effort
<br>


2. **API (gentelman`s way)**

- often not avaliable 
<br>
- usually not for free
<br>
- easier to implement in production


### **webscraping**
  
- lets scrape [this](https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780) article and [this](https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614) from [Večernji list](https://www.vecernji.hr/)


```{r}
# 1. copy urls
url1 <- "https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780"

url2 <- "https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614"
# 2. request page 
page1 <- html_session(url1)
```

- we need to write a function to *grab* parts of the article
- this part takes some routine skill and revolves around `rvest` package
  
```{r}
# 3. write a function to take parts of the article
parseArticle <- function(webpage) {
  
  title <- html_nodes(webpage, xpath = '//h1[@class="article__title"]') %>%
    html_text() %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  date <- html_nodes(webpage, xpath = '//*[@class="article__header_date"]') %>%
    html_text() %>% 
    str_replace_all(pattern = "\\\r\\\n| u", replacement = "") %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  noComment <- html_nodes(webpage, xpath = '//*[@class="article__comments_number"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_extract("\\d+") %>% 
    as.numeric(.) %>% 
    ifelse(length(.) == 0, NA, .)
  
  views <- html_nodes(webpage, xpath = '//*[@class="article__header_views"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_extract("\\d+") %>% 
    as.numeric(.) %>% 
    ifelse(length(.) == 0, NA, .)
  
  articleLabel <- html_nodes(webpage, xpath = '//*[@class="article__label"]') %>%
    html_text() %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  author <- html_nodes(webpage, xpath = '//*[@class="article__author--link"]') %>%
    html_text() %>% 
    trimws()
  if (length(author) == 0) {author <- NA}
  
  articletext <- html_nodes(webpage, xpath = '//*[@class="article__body--main_content"]/p') %>%
    html_text() %>% 
    str_flatten(., "\n") %>% 
    ifelse(length(.) == 0, NA, .)
  
  keywords <- html_nodes(webpage, xpath = '//*[@class="article__tag_name"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_flatten(., ";") %>% 
    ifelse(length(.) == 0, NA, .)
  
  
  articles <- cbind.data.frame(title, date, noComment, views, articleLabel, 
                              articleLabel, author, articletext, keywords, stringsAsFactors = FALSE)
  return(articles)
}
```

- finally, lets apply the function and check the results
  
```{r}
# 4. apply the function
data <- parseArticle(page1)
# 5. check the data
str(data)
data$title
data$views
nchar(data$articletext)
```

- we would want to automati this approach and apply it to multiple articles 
- let`s first find a couple of more interesting articles: [I](https://www.vecernji.hr/vijesti/glavni-tajnik-un-a-nuklearni-sukob-ponovno-izgleda-moguc-1570823),
[II](https://www.vecernji.hr/vijesti/sto-je-clanak-5-nato-a-aktiviran-je-samo-jednom-a-ne-moze-se-primijeniti-na-ukrajinu-1570810), [III](https://www.vecernji.hr/vijesti/kotromanovic-ako-je-bila-rijec-o-naoruzanom-dronu-napad-je-to-na-clanicu-nato-a-1570731)


```{r}
# assign urls of the articles
url3 <- "https://www.vecernji.hr/vijesti/glavni-tajnik-un-a-nuklearni-sukob-ponovno-izgleda-moguc-1570823"
url4 <- "https://www.vecernji.hr/vijesti/sto-je-clanak-5-nato-a-aktiviran-je-samo-jednom-a-ne-moze-se-primijeniti-na-ukrajinu-1570810"
url5 <- "https://www.vecernji.hr/vijesti/kotromanovic-ako-je-bila-rijec-o-naoruzanom-dronu-napad-je-to-na-clanicu-nato-a-1570731"
# bind all articles together
urls <- c(url1,url2,url3,url4,url5)
# check
urls
```

- lets automate this procedure for multiple articles now and check the data

```{r}
# read in urls
pages <- lapply(urls,html_session)
# grab all article parts
multipleArticles <- lapply(pages, parseArticle)
#make data.frame
dataArticles <- do.call(rbind, multipleArticles)
# check the data
dim(dataArticles)
glimpse(dataArticles)
dataArticles$title
dataArticles$views
nchar(dataArticles$articletext)

```

### **API**

- lets quickly inspect the [API documentation](https://documenter.getpostman.com/view/7210955/S1EMUebv?version=latest) 
- then we need to compile the full API request and retrieve the data

```{r}
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
              "/keywords/",keywords,
              "/mentions?access_token=",token,
              "&from_time=",from_time,
              "&to_time=",to_time,
              "&count=",count,
              "&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
```

- now we have the retrieved data in a data.frame object
- let`s check what is inside

```{r}
# size of the data
dim(data)
# variables and variable types
glimpse(data)
# check if there are articles from Večernji list
data %>% 
  group_by(response.from) %>%
  count %>%
  arrange(desc(n)) %>%
  head()

data %>%
  filter(response.from == "vecernji.hr") %>%
  select(title = response.title,
         time = response.insert_time,
         author = response.author,
         url = response.url,
         comment = response.comment_count,
         text = response.mention) %>%
  arrange(desc(comment)) %>%
  head

data %>% 
  slice(1) %>% 
  select(title = response.title)

data %>% 
  slice(1) %>%
  select(text = response.mention)

data %>% 
  slice(1) %>%
  select(url = response.url)
        

```



## STORING DATA

## MANIPULATING DATA

## ANALYTICS

## REPORTING

# CONNCLUDING POINTS



