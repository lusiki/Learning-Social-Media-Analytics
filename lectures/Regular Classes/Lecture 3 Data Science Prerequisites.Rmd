---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 3: Data Science Prerequisites"
author: "Luka Sikic, PhD"
date: "Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)" #"`r format(Sys.time(), '%d %B %Y')`"
output:
  xaringan::moon_reader:
    css: [default, metropolis, metropolis-fonts] 
    lib_dir: libs
    nature:
      highlightStyle: googlecode
      highlightLines: true
      countIncrementalSlides: false
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(rvest)
library(tidyverse)
library(httr)
```


# STRUCTURE
<br>
<br>
- FETCHING DATA
<br>
<br>
- STORING DATA
<br>
<br>
- MANIPULATING DATA
<br>
<br>
- ANALYTICS
<br>
<br>
-  REPORTING

---
layout: true
# FETCHING DATA
---

### Two main approaches:

##### guerrila = webscraping

- takes some skills
<br>
- everything can be scraped (tailor made solutions)
<br>
- takes more time and effort
<br>


##### gentelman`s way = API

- often not avaliable 
<br>
- usually not for free
<br>
- easier to implement in production
---

#### webscraping
  
- lets scrape [this](https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780) article and [this](https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614) from [Veƒçernji list](https://www.vecernji.hr/)

```{r}
# 1. copy urls
url1 <- "https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780"

url2 <- "https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614"
# 2. request page 
page1 <- html_session(url1)
page2 <- html_session(url2)
```
---
  
  #### webscraping
  
```{r}
# 3. write a function to take parts of the article
parseArticle <- function(webpage) {
  
  title <- html_nodes(webpage, xpath = '//h1[@class="article__title"]') %>%
    html_text() %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  article <- html_nodes(webpage, xpath = '//*[@class="article__body--main_content"]/p') %>%
    html_text() %>% 
    str_flatten(., "\n") %>% 
    ifelse(length(.) == 0, NA, .)
  
  numberOfComments <- html_nodes(webpage, xpath = '//*[@class="article__comments_number"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_extract("\\d+") %>% 
    as.numeric(.) %>% 
    ifelse(length(.) == 0, NA, .)
  
  articles <- cbind.data.frame(title, article, numberOfComments, stringsAsFactors = FALSE)
  return(articles)
}
```
---
  
  #### webscraping
  
```{r}
# 4. apply the function
data <- parseArticle(page1)
# 5. check the data
str(data)
data$title
data$numberOfComments
nchar(data$article)
```

---
  
  









