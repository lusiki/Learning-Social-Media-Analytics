---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 3: Data Science Prerequisites"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(rvest)
library(tidyverse)
library(httr)
```


# FETCHING DATA


### Two main approaches:

1. **webscraping (guerrila)**


- takes some skills
<br>
- everything can be scraped (tailor made solutions)
<br>
- takes more time and effort
<br>


2. **API (gentelman`s way)**

- often not avaliable 
<br>
- usually not for free
<br>
- easier to implement in production


### **webscraping**
  
- lets scrape [this](https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780) article and [this](https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614) from [Večernji list](https://www.vecernji.hr/)


```{r}
# 1. copy urls
url1 <- "https://www.vecernji.hr/vijesti/umro-bivsi-austrijski-potkancelar-i-prijatelja-hrvatske-dr-erhard-busek-1570780"

url2 <- "https://www.vecernji.hr/vijesti/u-kijevu-pogodena-stambena-zgrada-objavljena-snimka-raketiranja-nebodera-u-mariupolju-1570614"
# 2. request page 
page1 <- html_session(url1)
```

- we need to write a function to *grab* parts of the article
- this part takes some routine skill and revolves around `rvest` package
  
```{r}
# 3. write a function to take parts of the article
parseArticle <- function(webpage) {
  
  title <- html_nodes(webpage, xpath = '//h1[@class="article__title"]') %>%
    html_text() %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  date <- html_nodes(webpage, xpath = '//*[@class="article__header_date"]') %>%
    html_text() %>% 
    str_replace_all(pattern = "\\\r\\\n| u", replacement = "") %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  noComment <- html_nodes(webpage, xpath = '//*[@class="article__comments_number"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_extract("\\d+") %>% 
    as.numeric(.) %>% 
    ifelse(length(.) == 0, NA, .)
  
  views <- html_nodes(webpage, xpath = '//*[@class="article__header_views"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_extract("\\d+") %>% 
    as.numeric(.) %>% 
    ifelse(length(.) == 0, NA, .)
  
  articleLabel <- html_nodes(webpage, xpath = '//*[@class="article__label"]') %>%
    html_text() %>% 
    trimws() %>% 
    ifelse(length(.) == 0, NA, .)
  
  author <- html_nodes(webpage, xpath = '//*[@class="article__author--link"]') %>%
    html_text() %>% 
    trimws()
  if (length(author) == 0) {author <- NA}
  
  articletext <- html_nodes(webpage, xpath = '//*[@class="article__body--main_content"]/p') %>%
    html_text() %>% 
    str_flatten(., "\n") %>% 
    ifelse(length(.) == 0, NA, .)
  
  keywords <- html_nodes(webpage, xpath = '//*[@class="article__tag_name"]') %>%
    html_text() %>% 
    trimws() %>% 
    str_flatten(., ";") %>% 
    ifelse(length(.) == 0, NA, .)
  
  
  articles <- cbind.data.frame(title, date, noComment, views, articleLabel, 
                              articleLabel, author, articletext, keywords, stringsAsFactors = FALSE)
  return(articles)
}
```

- finally, lets apply the function and check the results
  
```{r}
# 4. apply the function
data <- parseArticle(page1)
# 5. check the data
str(data)
data$title
data$views
data$author
nchar(data$articletext)
```

- normally we want to automate this approach and apply it to multiple articles 
- let`s first find a couple of more interesting articles: [I](https://www.vecernji.hr/vijesti/glavni-tajnik-un-a-nuklearni-sukob-ponovno-izgleda-moguc-1570823),
[II](https://www.vecernji.hr/vijesti/sto-je-clanak-5-nato-a-aktiviran-je-samo-jednom-a-ne-moze-se-primijeniti-na-ukrajinu-1570810), [III](https://www.vecernji.hr/vijesti/kotromanovic-ako-je-bila-rijec-o-naoruzanom-dronu-napad-je-to-na-clanicu-nato-a-1570731)


```{r}
# assign urls of the articles
url3 <- "https://www.vecernji.hr/vijesti/glavni-tajnik-un-a-nuklearni-sukob-ponovno-izgleda-moguc-1570823"
url4 <- "https://www.vecernji.hr/vijesti/sto-je-clanak-5-nato-a-aktiviran-je-samo-jednom-a-ne-moze-se-primijeniti-na-ukrajinu-1570810"
url5 <- "https://www.vecernji.hr/vijesti/kotromanovic-ako-je-bila-rijec-o-naoruzanom-dronu-napad-je-to-na-clanicu-nato-a-1570731"
# bind all articles together
urls <- c(url1,url2,url3,url4,url5)
# check
urls
# check
str(urls)
```

- let`s automate this procedure for multiple articles now and check the data

```{r}
# read in urls
pages <- lapply(urls,html_session)
# grab all article parts
multipleArticles <- lapply(pages, parseArticle)
# make data.frame
dataArticles <- do.call(rbind, multipleArticles)
# check the data
dim(dataArticles)
glimpse(dataArticles)
dataArticles$title
dataArticles$views
nchar(dataArticles$articletext)

```

### **API**

- lets quickly inspect the [API documentation](https://documenter.getpostman.com/view/7210955/S1EMUebv?version=latest) 
- then we need to compile the full API request and retrieve the data

```{r, eval = F}
# this is a private info
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
              "/keywords/",keywords,
              "/mentions?access_token=",token,
              "&from_time=",from_time,
              "&to_time=",to_time,
              "&count=",count,
              "&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
```

- now we have the retrieved data in a data.frame object
- let`s check what is inside

```{r}
# size of the data
dim(data)
# variables and variable types
glimpse(data)
# basic descriptives (again)
data %>% 
  group_by(response.type) %>%
  count %>%
  arrange(desc(n)) %>%
  head()
# check if there are articles from Večernji list
data %>%
  filter(response.from == "vecernji.hr") %>%
  select(title = response.title,
         time = response.insert_time,
         author = response.author,
         url = response.url,
         comment = response.comment_count,
         text = response.mention) %>%
  arrange(desc(comment)) %>%
  head
# check title
data %>% 
  slice(1) %>% 
  select(title = response.title)
# check mention
data %>% 
  slice(1) %>%
  select(text = response.mention)
# check url
data %>% 
  slice(1) %>%
  select(url = response.url)
        

```



# STORING DATA

# MANIPULATING DATA



- in R there are three major ways to manipulate data: [base](https://iqss.github.io/dss-workshops/R/Rintro/base-r-cheat-sheet.pdf), [tidyverse](https://www.tidyverse.org/), [data.table](https://www.machinelearningplus.com/data-manipulation/datatable-in-r-complete-guide/)
- you can also combine them together
- we are going to explore and use tidywerse and data.table syntax in this course

#### TIDY WAY

- let`s check some important functions
- check this famous [paper](https://vita.had.co.nz/papers/tidy-data.pdf) (Hadley Wickham, 2014 JSS) to motivate the tidyverse way and check [tidyverse](https://www.tidyverse.org/) ecosystem
- basic dplyr (tidyverse) syntax includes the following:

1. `filter`: filter (i.e. *subset*) rows by value.

```{r}
# get the number of web articles/activity: FILTER
data %>% 
  filter(response.type == "web") %>% # FILTER!
  summarise(NumberOfArticles = n())
```

2. `arrange`: order (i.e. *reorder*) rows by value.

```{r}
# arrange by share
data %>% 
  filter(response.type == "web") %>%
  group_by(response.from) %>% 
  summarise(Share = mean(response.share_count),
            Reach = mean(response.reach),
            Virality = mean(response.virality),
            LikeCount = mean(response.like_count),
            Comment = mean(response.comment_count)) %>%
  arrange(desc(Share)) # ARRANGE!
# arrange by reach  
data %>% 
  filter(response.type == "web") %>%
  group_by(response.from) %>% 
  summarise(Reach = mean(response.reach),
            Share = mean(response.share_count),
            Virality = mean(response.virality),
            LikeCount = mean(response.like_count),
            Comment = mean(response.comment_count)) %>%
  arrange(desc(Reach)) # ARRANGE!
# arrange by comment 
data %>% 
  filter(response.type == "web") %>%
  group_by(response.from) %>% 
  summarise(Comment = mean(response.comment_count)) %>%
  arrange(desc(Comment)) # ARRANGE!

```

3. `select`: Choose (i.e. *subset*) columns by name. 


```{r}
# get to know your data I
data %>% 
  select(response.from, response.title, response.url) %>% # SELECT!
  filter(response.from == "geopolitika.news") 
# get to know your data II
data %>% 
  select(response.from, response.title, response.url) %>% # SELECT!
  filter(response.from == "priznajem.hr") 
# get to know your data III
data %>% 
  select(response.from, response.title, response.url) %>% # SELECT!
  filter(response.from == "sloboda.hr")   
  
```

4. `mutate`: Create new columns.


```{r}
# select biggest portals
data %>% 
  filter(response.type == "web") %>%
  group_by(response.from) %>% 
  count() %>%
  arrange(desc(n)) %>%
  head(10) %>%
  select(response.from) %>%
  pull -> largePortals
# check biggest portals
largePortals
# create negation operator
`%!in%` <- Negate(`%in%`)
# Create new column and check some descriptives
data %>% 
  filter(response.type == "web") %>%
  mutate(PortalSize = case_when(response.from %in% largePortals ~ "Large", # MUTATE!
                                response.from %!in% largePortals ~ "Small")) %>% 
  group_by(PortalSize) %>%
  count
  
```




5. `summarise`: Make a descriptive summary.

- we already saw this function in action :-)
- let`s see another one


```{r}

data %>% 
  filter(response.type == "web") %>%
  summarise(Average = n())

data %>%
  filter(response.type == "web") %>% 
  select()


```

#### DATA.TABLE WAY

- advantages of data.table include:

1. Concise syntax

```{r}
# read in library
library(data.table)
# check class
class(data)
# set the data.table object
dataDT = as.data.table(data) 
# check class again
class(dataDT)
# do some descriptive statistics
dataDT[response.type == "web",
       .(minShare = min(response.share_count),
         maxShare = max(response.share_count),
         avgShare = mean(response.share_count),
         stdShare = sd(response.share_count))][]
# how many letters in a title
dataDT[response.type == "web",
       .(Avg = mean(nchar(response.title)),
         STD = sd(nchar(response.title)),
         min = min(nchar(response.title)),
         max = max(nchar(response.title)))][]
# how many letters in a text
dataDT[response.type == "web",
       .(Avg = mean(nchar(response.mention)),
         STD = sd(nchar(response.mention)),
         min = min(nchar(response.mention)),
         max = max(nchar(response.mention)))][]


```

2. Very fast

```{r}
library(tictoc)

# how many letters in a text by DT
tic()
dataDT[response.type == "web",
       .(Avg = mean(nchar(response.mention)),
         min = min(nchar(response.mention)),
         max = max(nchar(response.mention)))]
toc()

tic()
# how many letters in a text by tidy
data %>% 
  group_by(response.type) %>%
  summarise(Avg = mean(nchar(response.mention)),
         min = min(nchar(response.mention)),
         max = max(nchar(response.mention)))
toc()


# READ IN FULL MEDIATOOLKIT DATA SAMPLE
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
# make data.table object
allDT <- as.data.table(all_raw)


# lets check average activity size across 
allDT[,
       .(Avg = mean(nchar(TITLE)),
         STD = sd(nchar(TITLE)),
         min = min(nchar(TITLE)),
         max = max(nchar(TITLE))),
       by = SOURCE_TYPE]


allDT[SOURCE_TYPE == "twitter",.(Avg = mean(nchar(TITLE)),
         STD = sd(nchar(TITLE)),
         min = min(nchar(TITLE)),
         max = max(nchar(TITLE)))]

```

3. Memory efficiency

Measuring memory efficiency is relatively complicated [thing](https://stackoverflow.com/a/61376971). For details [check](https://jangorecki.gitlab.io/r-talks/2019-06-18_Poznan_why-data.table/why-data.table.pdf) (after 12th slide) check data.table functionality.

4. Lots of possibilities, stability and 5. Low dependancy



- tidyverse works step by step and data.table does it in one step
- one operation is one flid thought
- chaining in DT is also possible
- let`try some of these possibilities

```{r}
# check 10 articles d+from vecernji.hr
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",
      .(TITLE,URL,COMMENT_COUNT)] 
```

- changing columns looks like this

```{r}
# check the date column
str(allDT$DATE)
# change the date column into date format
allDT[, DateColumn := as.Date(DATE,"%Y-%m-%d" )] # modify by reference
# checNoViewk the new date column 
str(allDT$DateColumn)
# show the results
allDT[1:5,.(DateColumn,TITLE,SOURCE_TYPE)][]
```

- combinations of tidy and and DT syntax is also posssible

```{r}
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
  filter(COMMENT_COUNT  > 0) %>%
  distinct(.) %>%
  arrange(desc(COMMENT_COUNT)) %>%
  head(15)
  
```

- grouping in data table


```{r}
allDT[,.(AwerageNoArticles = .N), by = SOURCE_TYPE][order(-AwerageNoArticles)]
```


# ANALYTICS

- we will cover that in three following lectures
- methods used are statistical analysis, machine learning and textutal analysis

# REPORTING

- the [example](https://raw.githack.com/lusiki/WebObradaPodataka/main/XtraPredavanja/AparatiZaKavu/Prez.html) of (almost) production ready .Rmd report




