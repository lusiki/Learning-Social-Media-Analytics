---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 10: Twitter"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
library(anytime)
library(scales)
```


```{r, eval=FALSE, echo=T}
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
```

```{r , eval=F}
# some basic data wrangle
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
# select media
forum <- posts[SOURCE_TYPE == "forum",]
posts <- as.data.table(posts)


range(posts$DATE)
forum[,.N,TITLE][order(-N)]


posts %>% 
  group_by(SOURCE_TYPE) %>%
  mutate(UkBrojObjava = n()) %>%
  group_by(DATE, INTERDAYTIME, SOURCE_TYPE) %>%
  mutate(PerDay = n()) %>%
  ungroup() %>%
  group_by(INTERDAYTIME,SOURCE_TYPE) %>%
  summarise(AVG = mean(PerDay)) %>%
  filter(SOURCE_TYPE != "comment") %>%
  arrange(desc(AVG)) -> activePerDay



library(anytime)
library(lubridate)
# convert chr date to date format
posts[,DTIME := anytime(paste(posts$DATE,posts$TIME))]
# make lubridate
posts[,DTIME := ymd_hms(paste(posts$DATE,posts$TIME))]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Vecer", "Jutro", "Popodne", "Prevecer")
# make a new variable
posts[, INTERDAYTIME := cut(x=hour(posts$DTIME),
                         breaks = breaks,
                         labels = labels,
                         include.lowest=TRUE)][
                           ,INTERDAYTIME := as.factor(INTERDAYTIME)][
                             ,SOURCE_TYPE := as.factor(SOURCE_TYPE)
                             ]

# check the result
table(posts$INTERDAYTIME)
# also peak into activity again (&again)
table(posts$SOURCE_TYPE)

# we are interested in cross-tabulated View
activePerDay <- xtabs(~ INTERDAYTIME + SOURCE_TYPE, posts)

activePerDay <- as.data.frame(activePerDay) %>%
  filter(SOURCE_TYPE != "comment")

library(extrafont)
loadfonts(device = "win")


ggplot(activePerDay, aes(x = INTERDAYTIME, y = AVG, fill = INTERDAYTIME, width=0.75)) + 
    labs(y = "Broj objava ", x = "", title = "Aktivnost na online društvenim medijima U RH",
        subtitle = "Dnevni prosjek za cjelokupni medijski prostor u razdoblju 6 mj",
        caption = "Izvor: Mediatoolkit | Izradio: Lukos") +
    geom_bar(stat = "identity") +
    facet_wrap(~SOURCE_TYPE, scales = "free") +
    coord_flip() +
    guides(fill=FALSE) +
    theme_bw() + theme( strip.background  = element_blank(),
                        panel.grid.major = element_blank(),
                        panel.border = element_blank(),
                        axis.ticks = element_blank(),
                        panel.grid.minor.y = element_blank(),
                        panel.grid.major.y = element_blank(),
                        panel.grid.minor.x = element_blank(),
                        panel.grid.major.x = element_blank(),
                        text=element_text(size=9,  family="Roboto")) +
    theme(legend.position="bottom") +
    scale_fill_brewer(palette="Set2") + 
    scale_fill_grey(start=0,end =0.8)





forum[TITLE == "Potraga za razlogom ili teorije zavjere vol. 9",][]

```

```{r}
tw <- read.csv2("C:/Users/Lukas/OneDrive/Desktop/tw.csv")

```




# Basic descriptives of overall activity

```{r}
# PER INFLUENCER
tw <- tw %>%
  mutate(PROFILE = gsub("^.*\\.com/([^/]+).*", "\\1", URL))
tw <- as.data.table(tw)

# most active profiles
unique(tw[,.N,PROFILE][order(-N)])
# most popular
tw %>% 
  group_by(PROFILE) %>%
  summarise(FOLLOW = mean(FOLLOWERS_COUNT)) %>%
  arrange(desc(FOLLOW)) %>% 
  head(40) 
# most influential
tw %>% 
  group_by(PROFILE) %>%
  summarise(REACH = sum(REACH)) %>%
  arrange(desc(REACH))  %>% 
  head(40)
# most influential II
tw %>% 
  group_by(PROFILE) %>%
  summarise(INTERACTIONS = sum(INTERACTIONS)) %>%
  arrange(desc(INTERACTIONS)) %>% 
  head(40)
# most appreciated
tw %>% 
  group_by(PROFILE) %>%
  summarise(FAVORITE = sum(FAVORITE_COUNT)) %>%
  arrange(desc(FAVORITE)) %>% 
  head(40)
# most appreciated
tw %>% 
  group_by(PROFILE) %>%
  summarise(RETWEET = sum(RETWEET_COUNT)) %>%
  arrange(desc(RETWEET)) %>% 
  head(40)

```

```{r}
# PER TWEET

# most popular
tw %>% 
  select(PROFILE, FULL_TEXT, FOLLOWERS_COUNT,URL) %>%
  arrange(desc(FOLLOWERS_COUNT)) %>% 
  head(40)
# most influential
tw %>% 
  select(PROFILE, FULL_TEXT, REACH,URL) %>%
  arrange(desc(REACH))  %>% 
  head(40)
# most influential II
tw %>% 
  select(PROFILE, FULL_TEXT, INTERACTIONS,URL) %>%
  arrange(desc(INTERACTIONS))  %>% 
  head(40)
# most appreciated
tw %>% 
  select(PROFILE, FULL_TEXT, FAVORITE_COUNT,URL) %>%
  arrange(desc(FAVORITE_COUNT)) %>% 
  head(40)
# most appreciated
tw %>% 
  select(PROFILE, FULL_TEXT, RETWEET_COUNT,URL) %>%
  arrange(desc(RETWEET_COUNT))  %>% 
  head(40)

```


# check twitter activity on CRO supply side 

```{r}
# select relevant CRO profiles
unique(tw[,.N,FROM][order(-N)]) %>%
  filter(N > 5) %>% 
  pull(FROM) -> CRO_TW

tw[FROM %in% CRO_TW,] -> CTW

CTW %>%
  mutate(PROFILE = gsub("^.*\\.com/([^/]+).*", "\\1", URL)) -> CTW

# most popular
CTW %>% 
  group_by(PROFILE) %>%
  summarise(FOLLOW = mean(FOLLOWERS_COUNT)) %>%
  arrange(desc(FOLLOW))  %>% 
  head(100)
# most influential
CTW %>% 
  group_by(PROFILE) %>%
  summarise(REACH = sum(REACH)) %>%
  arrange(desc(REACH))  %>% 
  head(40)
# most influential II
CTW %>% 
  group_by(PROFILE) %>%
  summarise(INTERACTIONS = sum(INTERACTIONS)) %>%
  arrange(desc(INTERACTIONS))  %>% 
  head(40)
# most appreciated
CTW %>% 
  group_by(PROFILE) %>%
  summarise(FAVORITE = sum(FAVORITE_COUNT)) %>%
  arrange(desc(FAVORITE)) %>% 
  head(40)
# most appreciated
CTW %>% 
  group_by(PROFILE) %>%
  summarise(RETWEET = sum(RETWEET_COUNT)) %>%
  arrange(desc(RETWEET))  %>% 
  head(40)
  
```


```{r}
# PER TWEET

# most popular
CTW %>% 
  select(PROFILE, FULL_TEXT, FOLLOWERS_COUNT,URL) %>%
  arrange(desc(FOLLOWERS_COUNT))  %>% 
  head(40)
# most influential
CTW %>% 
  select(PROFILE, FULL_TEXT, REACH,URL) %>%
  arrange(desc(REACH))  %>% 
  head(40)
# most influential II
CTW %>% 
  select(PROFILE, FULL_TEXT, INTERACTIONS,URL) %>%
  arrange(desc(INTERACTIONS))  %>% 
  head(40)
# most appreciated
CTW %>% 
  select(PROFILE, FULL_TEXT, FAVORITE_COUNT,URL) %>%
  arrange(desc(FAVORITE_COUNT))  %>% 
  head(40)
# most appreciated
CTW %>% 
  select(PROFILE, FULL_TEXT, RETWEET_COUNT,URL) %>%
  arrange(desc(RETWEET_COUNT))  %>% 
  head(40)

```

```{r, eval=FALSE, fig.length = 12}
posts[,.N,DATE][order(-DATE)]

all <- posts[DATE > "2021-12-01",]


# library(extrafont)
# loadfonts()
# library(xkcd)
# font_import(pattern="[H/h]umor")
# loadfonts()
# windowsFonts(Times=windowsFont("Times New Roman"))
# library(ggtext)
# library(showtext)
# showtext_auto()
# 
# font_add_google('Roboto')

all[SOURCE_TYPE != "comment",.N,SOURCE_TYPE][,.(SOURCE_TYPE,N = round(N/1000,2))] %>% 
  ggplot(., aes(reorder(SOURCE_TYPE,N), N ,fill=as.factor(SOURCE_TYPE))) + 
  geom_bar(stat = "identity") + 
  scale_fill_grey() +
  theme(legend.position="none") +
  coord_flip() +
  theme(legend.position="none", panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
panel.background = element_blank(), axis.line = element_line(colour = "black"),
text = element_text(family = "serif")) +
  ylab("Tisuca medijskih objva") + 
  xlab("") +
  scale_y_continuous(labels = scales::comma, breaks = c(50,150,300,550,1200)) + 
  labs(title = "Aktivnost na društvenim medijma u Hrvatskoj",
              subtitle = "Cjelokupni medijski prostor u razdoblju 6 mjeseci",
              caption = "Podatci: Mediatoolkit | Izradio: Lukos")

  

```

# Check forum activity 

```{r leksikoni, message=F, warning=F}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
head(sample_n(CroSentilex_Gold,100),15)
 
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
```


```{r}
# read in data
forum <- read.csv2( "C:/Users/Lukas/OneDrive/Desktop/forum.csv")
forum <- as.data.table(forum)


forum[,.N, TITLE][order(-N)] %>% head(50)

forum[TITLE == "Anksioznost - opća tema(Vol.V)",] %>% 
  unnest_tokens(word,FULL_TEXT) -> ZM_token
  
  

# remove stop words, numbers, single letters
ZM_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> ZM_tokenTidy
# remove NA
ZM_tokenTidy %>%
  filter(!is.na(word)) -> ZM_tokenTidy

ZM_tokenTidy[,.N,by = word][order(-N),]

## Vizualize most common words
ZM_tokenTidy[,.N,by = word][N>500][order(-N),][,word := reorder(word,N)] %>%
  ggplot(aes(word, N)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme_economist()
  
```













