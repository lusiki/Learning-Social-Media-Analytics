---
title: "Untitled"
author: "Lux"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Warm Up

```{r}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(purrr)
```


## Import dt

```{r echo=F, eval=F, message=F}
# path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
# raw <- list.files(path = path , pattern="xlsx")
# raw_path <- paste0(path, "/", raw)
# all_raw <- map_df(raw_path, read_excel)
# dt <- distinct(all_raw)
# write.csv(dt, file = "D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv", row.names = T)
```

```{r echo=F, eval=F, message=F}
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
```

## Split into 

```{r echo=F, eval=F, message=F}
n <- nrow(dt)
chunk_size <- n %/% 20

chunks <- split(dt, rep(1:20, each = chunk_size, length.out = n))

for (i in 1:20) {
  fwrite(chunks[[i]], file = paste0("D:/LUKA/Freelance/Mediatoolkit/chunk_", i, ".csv"))
}

```

## Identify articles

```{r echo=F, eval=F, message=F}

path <- "D:/LUKA/Freelance/Mediatoolkit"
files <- list.files(path)
chunk_files <- files[grep("chunk", files)]
filepath <- file.path(path, chunk_files)



tokenize_chunk <- function(filepath) {
 filepath %>%
    read.csv() %>%
    unnest_tokens(word, FULL_TEXT, token = "words") %>%
    filter(str_detect(word, "humanitar")) %>%
    distinct(TITLE, URL, .keep_all = F) 
}


tokenized_chunks <- map(filepath, tokenize_chunk)

tokenized_chunks_df <- bind_rows(tokenized_chunks)



```


## Create "humanitarian" sample

```{r  echo=F, eval=F, message=F}


df_filtered <- dt %>% 
  filter(URL %in% tokenized_chunks_df$URL)

write.csv2(df_filtered, file = "D:/LUKA/Freelance/Mediatoolkit/Humanitar.csv", row.names = T)


```



## Tokenize


```{r}



```



## Identify Sample





```{r}








```



```{r }
humanitarno <- fullDta %>%
#  select(TITLE) %>%
  filter(grepl("humanitar",TITLE,ignore.case = FALSE))
```


# General Description

```{r}
length(unique(humanitarno$FROM))

length(unique(humanitarno$TITLE))
```


- how much activity per media

<br>
<br>
```{r, echo=F, warning=FALSE, message=FALSE}
# GENERAL OVERVIEW 
humanitarno %>%
  group_by(SOURCE_TYPE) %>%
  count() %>%
  arrange(desc(n)) %>%
  mutate_if(is.numeric, format, big.mark = ".") %>%
  kbl() %>%
  kable_styling(font_size = 11)
```

```{r, echo=F, warning=FALSE, message=FALSE}
# GENERAL OVERVIEW 
humanitarno %>%
  filter(SOURCE_TYPE == "web") %>%
  group_by(FROM) %>%
  count() %>%
  arrange(desc(n)) %>%
  mutate_if(is.numeric, format, big.mark = ".") %>%
  kbl() %>%
  kable_styling(font_size = 11)
```

- check some titles (random choice)

```{r, echo=F, warning=FALSE, message=FALSE}
# SAMPLE SOME TXT
humanitarno %>%
  select(SOURCE_TYPE, TITLE) %>%
  unique(.) %>%  
  sample_n(8) %>%
  kbl() %>%
  kable_styling(font_size = 11)
```


















