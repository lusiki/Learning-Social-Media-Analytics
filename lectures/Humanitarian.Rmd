---
title: "Untitled"
author: "Lux"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Warm Up

```{r}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(purrr)
```

## Import dt

```{r echo=F, eval=F, message=F , warning= FALSE}
# path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
# raw <- list.files(path = path , pattern="xlsx")
# raw_path <- paste0(path, "/", raw)
# all_raw <- map_df(raw_path, read_excel)
# dt <- distinct(all_raw)
# write.csv(dt, file = "D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv", row.names = T)
```

```{r echo=F, eval=F, message=F , warning= FALSE}
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
```

```{r echo=F, eval=F, message=F , warning= FALSE}
n <- nrow(dt)
chunk_size <- n %/% 20

chunks <- split(dt, rep(1:20, each = chunk_size, length.out = n))

for (i in 1:20) {
  fwrite(chunks[[i]], file = paste0("D:/LUKA/Freelance/Mediatoolkit/chunk_", i, ".csv"))
}

```

```{r echo=F, eval=F, message=F , warning= FALSE}

path <- "D:/LUKA/Freelance/Mediatoolkit"
files <- list.files(path)
chunk_files <- files[grep("chunk", files)]
filepath <- file.path(path, chunk_files)



tokenize_chunk <- function(filepath) {
 filepath %>%
    read.csv() %>%
    unnest_tokens(word, FULL_TEXT, token = "words") %>%
    filter(str_detect(word, "humanitar")) %>%
    distinct(TITLE, URL, .keep_all = F) 
}


tokenized_chunks <- map(filepath, tokenize_chunk)

tokenized_chunks_df <- bind_rows(tokenized_chunks)



```

```{r echo=F, eval=F, message=F , warning= FALSE}


df_filtered <- dt %>% 
  filter(URL %in% tokenized_chunks_df$URL)

write.csv2(df_filtered, file = "D:/LUKA/Freelance/Mediatoolkit/Humanitar.csv", row.names = T)


```

```{r echo=F, eval=T, message=F , warning= FALSE}
dta <- fread("D:/LUKA/Freelance/Mediatoolkit/Humanitar.csv") 
dta <- dta[,-1]
```


## Basic descriptives

```{r echo=F, eval=T, message=F , warning= FALSE}

# date range
range(dta$DATE)
# number of articles
nrow(dta)
# articles over time
daily_counts <- dta %>%
  group_by(DATE) %>%
  summarise(count = n())

# descriptives 
summ <- daily_counts %>% 
  summarize(min = min(count), max = max(count), 
            mean = mean(count), q1= quantile(count, probs = 0.25), 
            median = median(count), q3= quantile(count, probs = 0.75),
            sd = sd(count)) %>%
  mutate_if(is.numeric, round, digits=2) 

summ

# create plot of articles over time
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
  geom_line() +
  labs(x = "Date", y = "Number of Articles")
 

```


## Bigest profiles

```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by activity
activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by reach
reach <- dta %>%
  group_by(FROM) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by likes
like <- dta %>%
  group_by(FROM) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by comments
comment <- dta %>%
  group_by(FROM) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by shares
shares <- dta %>%
  group_by(FROM) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```


## Authors

```{r echo=F, eval=T, message=F , warning= FALSE}
# Authors by activity
authors <- dta %>%
  group_by(AUTHOR) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(authors, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Authors by reach
reach <- dta %>%
  group_by(AUTHOR) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Authors by likes
like <- dta %>%
  group_by(AUTHOR) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Authors by comments
comment <- dta %>%
  group_by(AUTHOR) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Authors by shares
shares <- dta %>%
  group_by(AUTHOR) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```

## Articles

```{r echo=F, eval=T, message=F , warning= FALSE}
# Articles by activity
articles <-  dta %>%
  group_by(TITLE) %>%
  summarise(count = n(),across(URL)) %>% 
  arrange(desc(count)) %>%
  slice(1:1000)

datatable(articles, options = list(scrollX = TRUE, scrollY = "500px"))

```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Articles by reach
reach <- dta %>%
  group_by(TITLE) %>%
  summarise(reach = sum(REACH),across(URL)) %>%
  arrange(desc(reach)) %>%
  slice(1:1000)

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```


```{r echo=F, eval=T, message=F , warning= FALSE}
# Articles by likes
like <- dta %>%
  group_by(TITLE) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T), across(URL)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like)) %>%
  slice(1:1000)

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Articles by comments
comment <- dta %>%
  group_by(TITLE) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T), acrosss = URL) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment)) %>%
  slice(1:1000)

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# Arcicles by shares
shares <- dta %>%
  group_by(TITLE) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T), acrosss = URL) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares)) %>%
  slice(1:1000)

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))

```


## Text analysis 

```{r echo=F, eval=T, message=F , warning= FALSE}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
head(sample_n(CroSentilex_Gold,100),15)
 
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)
# check stopwords data
#head(sample_n(stop_corpus,100),15)
```



```{r echo=F, eval=F, message=F , warning= FALSE}

# influencers by ACTIVITY
dta[,.N,FROM][order(-N)]

# influencers by FOLLOWERS
dta[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique() 

# influencers by REACH
dta[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique() 

# influencers by REACH II

dta %>% 
  group_by(FROM) %>%
  mutate(ACTIVITY = n(),
         REACH = sum(REACH),
         EFFECT = REACH/ACTIVITY) %>%
  select(FROM,ACTIVITY,REACH,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()
         
#  fb %>% 
#  group_by(FROM) %>%
#  mutate(ACTIVITY = n(),
#         REACH = sum(REACH),
#         EFFECT = REACH/ACTIVITY) %>%
#  filter(ACTIVITY>100) %>%
#  arrange(desc(EFFECT)) %>%
#  unique() -> fb
#fb <- as.data.table(fb)
# influencers by LIKE
dta[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique() 

# influencers by LIKE II

dta %>% 
  group_by(FROM) %>%
  mutate(LIKE = sum(LIKE_COUNT),
         ACTIVITY = n(),
         EFFECT = LIKE/ACTIVITY) %>%
  select(FROM,ACTIVITY,LIKE,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()



# influencers by INTERACTIONS
dta[,INTERACTIONS := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique() 


# influencers by COMMENT
dta[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique() 


# influencers by COMMENT II

dta %>% 
  group_by(FROM) %>%
  mutate(COMMENT = sum(COMMENT_COUNT),
         ACTIVITY = n(),
         ENGAGE = COMMENT/ACTIVITY) %>%
  select(FROM,ACTIVITY,COMMENT,ENGAGE) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(ENGAGE)) %>%
  unique()

#fb[, `:=` (ACTIVITY = .N , COMMENT = sum(COMMENT_COUNT),ENGAGE = COMMENT/ACTIVITY), FROM][,.(FROM,ACTIVITY,COMMENT, ENGAGE)][ACTIVITY >= 100][order(-COMMENT)] %>% unique()

# influencers by SHARE
dta[,SHARE := sum(SHARE_COUNT), FROM][,.(SHARE,FROM)][order(-SHARE)] %>% unique() 


# influencers by SHARE II

dta %>% 
  group_by(FROM) %>%
  mutate(SHARE = sum(SHARE_COUNT),
         ACTIVITY = n(),
         DISPERSION = SHARE/ACTIVITY) %>%
  select(FROM,ACTIVITY,SHARE,DISPERSION) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(DISPERSION)) %>%
  unique()


#fb[, `:=` (ACTIVITY = .N , SHARE = sum(SHARE_COUNT), DISPERSION = SHARE/ACTIVITY), FROM][,.(FROM,ACTIVITY,SHARE,DISPERSION)][ACTIVITY >= 100][order(-DISPERSION)] %>% unique()

# letters by influencer

dta %>% 
  group_by(FROM) %>%
  mutate(LETTERS = sum(SHARE_COUNT),
         ACTIVITY = n(),
         EFFORT = LETTERS/ACTIVITY) %>%
  select(FROM,ACTIVITY,LETTERS,EFFORT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFORT)) %>%
  unique()



#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM]

#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS/ACTIVITY), FROM][,.(FROM,ACTIVITY,LETTERS, EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()


# posts by REACH

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 

# posts by LIKE

dta[,.(LIKE_COUNT,FROM,FULL_TEXT, URL)][order(-LIKE_COUNT)] 

# posts by INTERACTIONS

dta[,.(INTERACTIONS,FROM,FULL_TEXT, URL)][order(-INTERACTIONS)]  

# posts by COMMENT

dta[,.(COMMENT_COUNT,FROM,FULL_TEXT, URL)][order(-COMMENT_COUNT)]  

# posts by SHARE

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 
```

```{r echo=F, eval=T, message=F , warning= FALSE}

# how many letters in a title
dta[,
       .(Avg = mean(nchar(TITLE), na.rm = T),
         STD = sd(nchar(TITLE), na.rm = T),
         min = min(nchar(TITLE), na.rm = T),
         max = max(nchar(TITLE), na.rm = T)),
      SOURCE_TYPE][order(-Avg),]
# how many letters in a text
dta[,
       .(Avg = mean(nchar(FULL_TEXT)),
         STD = sd(nchar(FULL_TEXT)),
         min = min(nchar(FULL_TEXT)),
         max = max(nchar(FULL_TEXT))),
      SOURCE_TYPE][order(-Avg),]


```















## Tokenize


```{r}



```



## Identify Sample





```{r}








```



```{r }
humanitarno <- fullDta %>%
#  select(TITLE) %>%
  filter(grepl("humanitar",TITLE,ignore.case = FALSE))
```


# General Description

```{r}
length(unique(humanitarno$FROM))

length(unique(humanitarno$TITLE))
```


- how much activity per media

<br>
<br>
```{r, echo=F, warning=FALSE, message=FALSE}
# GENERAL OVERVIEW 
humanitarno %>%
  group_by(SOURCE_TYPE) %>%
  count() %>%
  arrange(desc(n)) %>%
  mutate_if(is.numeric, format, big.mark = ".") %>%
  kbl() %>%
  kable_styling(font_size = 11)
```

```{r, echo=F, warning=FALSE, message=FALSE}
# GENERAL OVERVIEW 
humanitarno %>%
  filter(SOURCE_TYPE == "web") %>%
  group_by(FROM) %>%
  count() %>%
  arrange(desc(n)) %>%
  mutate_if(is.numeric, format, big.mark = ".") %>%
  kbl() %>%
  kable_styling(font_size = 11)
```

- check some titles (random choice)

```{r, echo=F, warning=FALSE, message=FALSE}
# SAMPLE SOME TXT
humanitarno %>%
  select(SOURCE_TYPE, TITLE) %>%
  unique(.) %>%  
  sample_n(8) %>%
  kbl() %>%
  kable_styling(font_size = 11)
```


















