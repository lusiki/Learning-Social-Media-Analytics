---
title: "Korupus native tekstova"
author: "Lux"
date: "2023-03-25"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r echo=F, eval=T, message=F , warning= FALSE, message=F}
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)

```

```{r echo=F, eval=F, message=F , warning= FALSE}
# path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
# raw <- list.files(path = path , pattern="xlsx")
# raw_path <- paste0(path, "/", raw)
# all_raw <- map_df(raw_path, read_excel)
# dt <- distinct(all_raw)
# write.csv(dt, file = "D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv", row.names = T)
```

```{r echo=F, eval=T, message=F , warning= FALSE}
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
```

```{r echo=F, eval=F, message=F , warning= FALSE}
n <- nrow(dt)
chunk_size <- n %/% 20

chunks <- split(dt, rep(1:20, each = chunk_size, length.out = n))

for (i in 1:20) {
  fwrite(chunks[[i]], file = paste0("D:/LUKA/Freelance/Mediatoolkit/chunk_", i, ".csv"))
}

```

```{r echo=F, eval=F, message=F , warning= FALSE}

path <- "D:/LUKA/Freelance/Mediatoolkit"
files <- list.files(path)
chunk_files <- files[grep("chunk", files)]
filepath <- file.path(path, chunk_files)



tokenize_chunk <- function(filepath) {
 filepath %>%
    read.csv() %>%
    unnest_tokens(word, FULL_TEXT, token = "words") %>%
    filter(str_detect(word, "humanitar")) %>%
    distinct(TITLE, URL, .keep_all = F) 
}


tokenized_chunks <- map(filepath, tokenize_chunk)

tokenized_chunks_df <- bind_rows(tokenized_chunks)



```

```{r echo=F, eval=F, message=F , warning= FALSE}


df_filtered <- dt %>% 
  filter(URL %in% tokenized_chunks_df$URL)

#write.csv2(df_filtered, file = "D:/LUKA/Freelance/Mediatoolkit/Humanitar.csv", row.names = T)


```

```{r echo=F, eval=F, message=F , warning= FALSE}
dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("Content",ignore_case = TRUE))) 

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("Native tim",ignore_case = TRUE)))


dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("Native",ignore_case = TRUE)))

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(FULL_TEXT,fixed("24Content",ignore_case = TRUE)))

dta <- dt %>%
  filter(SOURCE_TYPE =="web") %>%
  filter(str_detect(FULL_TEXT,fixed("Sponzorirani sadržaj",ignore_case = TRUE)))

# sadržaj donosi (txt) == 205

# sadržaj nastao (txt) == 801

# prilog je napravljen (txt) == 5

# nativ (title) == 11

# Native tim (txt) == 142

# Sponzorirani sadržaj == 439

# 24Content ==




dta <- dta[,-1]
```

#### 24h

- 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi

- 24ContentHaus nema u naslovu

- sadržaj donosi u tekstu ima 205


```{r echo=F, eval=F, message=F , warning= FALSE}

# 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("24ContentHaus",ignore_case = TRUE)))

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(FULL_TEXT,fixed("sadržaj donosi",ignore_case = TRUE)))


activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))

dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
  

```


#### Telegram

- formulacija na kraju članka *sadržaj nastao* ili možemo probati s frazom donosi (ovo ne može je ima previše reziduala) 

- sadržaj nastao u tekstu ima 803

```{r echo=F, eval=F, message=F , warning= FALSE}

# 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi


dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(FULL_TEXT,fixed("sadržaj nastao",ignore_case = TRUE)))


activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))


dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))


```


#### T-portal

- tportal ima potpis tnative i na kraju članka ima formulaciju *prilog je napravljen* u produkciji 

- tnative u naslovu ima 0

- prilog je napravljen u tekstu ima 5


```{r echo=F, eval=F, message=F , warning= FALSE}

# 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi


dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("tnative",ignore_case = TRUE)))

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(FULL_TEXT,fixed("prilog je napravljen",ignore_case = TRUE)))


activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))


dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```



#### VL

- VL ima Native tim 

- native u autoru ima 11


```{r echo=F, eval=F, message=F , warning= FALSE}

# 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi


dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("Native",ignore_case = TRUE)))

# dta <- dt %>%
#   filter(SOURCE_TYPE =="web")  %>%
#   filter(str_detect(FULL_TEXT,fixed("prilog je napravljen",ignore_case = TRUE)))


activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))


dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```


#### JL

- JL ima pod autorstvom Sponzorirani sadržaj 


- Sponzorirani sadržaj u naslovu ima 46


- Sponzorirani sadržaj u teskstu ima 439



```{r echo=F, eval=F, message=F , warning= FALSE}

# 24h kod autorstva ima 24ContentHaus i u tekstu formulaciju sadržaj donosi


dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(AUTHOR,fixed("Sponzorirani sadržaj",ignore_case = TRUE)))

activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))

dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

dta <- dt %>%
  filter(SOURCE_TYPE =="web")  %>%
  filter(str_detect(FULL_TEXT,fixed("Sponzorirani sadržaj",ignore_case = TRUE)))


activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))

dta %>% 
  select(DATE,TIME,TITLE,FROM,AUTHOR,URL, FULL_TEXT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```

#### Select native corpus
```{r echo=F, eval=T, message=F , warning= FALSE}

# 
# dta <- dt %>%
#   filter(SOURCE_TYPE =="web") %>%
#   filter(str_detect(FULL_TEXT,fixed("sadržaj nastao",ignore_case = TRUE)) |
#            str_detect(AUTHOR,fixed("sponzor",ignore_case = TRUE))|
#            str_detect(AUTHOR,fixed("native",ignore_case = TRUE))|
#            str_detect(AUTHOR,fixed("tnative",ignore_case = TRUE))|
#            str_detect(FULL_TEXT,fixed("tnative",ignore_case = TRUE))|
#            str_detect(AUTHOR,fixed("tnative",ignore_case = TRUE)))
  



dta <- dt[SOURCE_TYPE == "web" &
          (str_detect(FULL_TEXT, fixed("sadržaj nastao",ignore_case = TRUE)) |
           str_detect(AUTHOR, fixed("sponzor",ignore_case = TRUE)) |
           str_detect(AUTHOR, fixed("native",ignore_case = TRUE)) |
           str_detect(AUTHOR, fixed("tnative",ignore_case = TRUE)) |
           str_detect(FULL_TEXT, fixed("tnative",ignore_case = TRUE)) |
           str_detect(FULL_TEXT, fixed("sadržaj donosi", ignore_case = TRUE)) |
           str_detect(FULL_TEXT, fixed("sponzorirani sadržaj", ignore_case = TRUE)) | 
           str_detect(AUTHOR, fixed("tnative",ignore_case = TRUE)))]



```


#### Basic descriptives
#### Vremenski raspon
```{r echo=F, eval=F, message=F , warning= FALSE}

# date range
range(dta$DATE)
```


#### Broj članaka
```{r echo=F, eval=F, message=F , warning= FALSE}
# number of articles
nrow(dta)

```


#### Deskriptiva na dnevnoj razini
```{r echo=F, eval=F, message=F , warning= FALSE}
# articles over time
daily_counts <- dta %>%
  group_by(DATE) %>%
  summarise(count = n())

# descriptives 
summ <- daily_counts %>% 
  summarize(min = min(count), max = max(count), 
            mean = mean(count), q1= quantile(count, probs = 0.25), 
            median = median(count), q3= quantile(count, probs = 0.75),
            sd = sd(count)) %>%
  mutate_if(is.numeric, round, digits=2) 

summ
```

#### Broj objava po danima  
```{r echo=F, eval=F, message=F , warning= FALSE}
# create plot of articles over time
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
  geom_line() +
  labs(x = "Date", y = "Number of Articles")
 

```


#### Bigest profiles


#### Broj objava po izvoru 
```{r echo=F, eval=T, message=F , warning= FALSE}
# Portals by activity
activity <- dt %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Doseg objava po izvoru 
```{r echo=F, eval=F, message=F , warning= FALSE}
# Portals by reach
reach <- dta %>%
  group_by(FROM) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```

#### Like-ovi objava po izvoru 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Portals by likes
like <- dta %>%
  group_by(FROM) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Komentari objava po izvoru 
```{r echo=F, eval=F, message=F , warning= FALSE}
# Portals by comments
comment <- dta %>%
  group_by(FROM) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Share-ovi objava po izvoru 
```{r echo=F, eval=F, message=F , warning= FALSE}
# Portals by shares
shares <- dta %>%
  group_by(FROM) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Authors


#### Broj objava po autoru 
```{r echo=F, eval=F, message=F , warning= FALSE}
# Authors by activity
authors <- dta %>%
  group_by(AUTHOR) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

write.xlsx(dta, file = "D:/LUKA/Freelance/Mediatoolkit/native.xlsx", row.names = T)


datatable(authors, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Doseg objava po autoru 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Authors by reach
reach <- dta %>%
  group_by(AUTHOR) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Like-ovi objava po autoru 
```{r echo=F, eval=F, message=F , warning= FALSE}
# Authors by likes
like <- dta %>%
  group_by(AUTHOR) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

#### Komentari objava po autoru 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Authors by comments
comment <- dta %>%
  group_by(AUTHOR) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

#### Broj share-ova po autoru 


```{r echo=F, eval=F, message=F , warning= FALSE}
# Authors by shares
shares <- dta %>%
  group_by(AUTHOR) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```

#### Articles

#### Objave po share-ovima 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Articles by activity
articles <-  dta %>%
  group_by(TITLE) %>%
  summarise(count = n(),across(URL)) %>% 
  arrange(desc(count)) %>%
  slice(1:1000)

datatable(articles, options = list(scrollX = TRUE, scrollY = "500px"))

```

#### Objave po reach-u


```{r echo=F, eval=F, message=F , warning= FALSE}
# Articles by reach
dta %>% 
  ungroup %>%
  arrange(desc(REACH)) %>% 
  slice(1:3000) %>%
  group_by(V1) %>%
  summarise(REACH = sum(REACH),across(c(TITLE, AUTHOR, FROM, URL))) %>% 
  ungroup() %>%
  select(-V1, TITLE, AUTHOR, FROM, URL, REACH) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```




#### Objave po like-ovima 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Articles by likes
dta %>% 
  ungroup %>%
  arrange(desc(LIKE_COUNT)) %>% 
  slice(1:3000) %>%
  group_by(V1) %>%
  summarise(LIKE = sum(LIKE_COUNT),across(c(TITLE, AUTHOR, FROM, URL))) %>% 
  ungroup() %>%
  select(-V1, TITLE, AUTHOR, FROM, URL, LIKE) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))

```


#### Objave po komentarima


```{r echo=F, eval=F, message=F , warning= FALSE}
# Articles by comments
dta %>% 
  ungroup %>%
  arrange(desc(COMMENT_COUNT)) %>% 
  slice(1:3000) %>%
  group_by(V1) %>%
  summarise(COMMENT = sum(COMMENT_COUNT),across(c(TITLE, AUTHOR, FROM, URL))) %>% 
  ungroup() %>%
  select(-V1, TITLE, AUTHOR, FROM, URL, COMMENT) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```

#### Objave po share-ovima 

```{r echo=F, eval=F, message=F , warning= FALSE}
# Arcicles by shares
dta %>% 
  ungroup %>%
  arrange(desc(SHARE_COUNT)) %>% 
  slice(1:3000) %>%
  group_by(V1) %>%
  summarise(SHARE = sum(SHARE_COUNT),across(c(TITLE, AUTHOR, FROM, URL))) %>% 
  ungroup() %>%
  select(-V1, TITLE, AUTHOR, FROM, URL, SHARE) %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Text analysis 

```{r echo=F, eval=T, message=F , warning= FALSE}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
#head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
#head(sample_n(CroSentilex_Gold,100),15)
 
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
#head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)
# check stopwords data
#head(sample_n(stop_corpus,100),15)
```

```{r echo=F, eval=F, message=F , warning= FALSE}

# influencers by ACTIVITY
dta[,.N,FROM][order(-N)]

# influencers by FOLLOWERS
dta[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique() 

# influencers by REACH
dta[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique() 

# influencers by REACH II

dta %>% 
  group_by(FROM) %>%
  mutate(ACTIVITY = n(),
         REACH = sum(REACH),
         EFFECT = REACH/ACTIVITY) %>%
  select(FROM,ACTIVITY,REACH,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()
         
#  fb %>% 
#  group_by(FROM) %>%
#  mutate(ACTIVITY = n(),
#         REACH = sum(REACH),
#         EFFECT = REACH/ACTIVITY) %>%
#  filter(ACTIVITY>100) %>%
#  arrange(desc(EFFECT)) %>%
#  unique() -> fb
#fb <- as.data.table(fb)
# influencers by LIKE
dta[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique() 

# influencers by LIKE II

dta %>% 
  group_by(FROM) %>%
  mutate(LIKE = sum(LIKE_COUNT),
         ACTIVITY = n(),
         EFFECT = LIKE/ACTIVITY) %>%
  select(FROM,ACTIVITY,LIKE,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()



# influencers by INTERACTIONS
dta[,INTERACTIONS := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique() 


# influencers by COMMENT
dta[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique() 


# influencers by COMMENT II

dta %>% 
  group_by(FROM) %>%
  mutate(COMMENT = sum(COMMENT_COUNT),
         ACTIVITY = n(),
         ENGAGE = COMMENT/ACTIVITY) %>%
  select(FROM,ACTIVITY,COMMENT,ENGAGE) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(ENGAGE)) %>%
  unique()

#fb[, `:=` (ACTIVITY = .N , COMMENT = sum(COMMENT_COUNT),ENGAGE = COMMENT/ACTIVITY), FROM][,.(FROM,ACTIVITY,COMMENT, ENGAGE)][ACTIVITY >= 100][order(-COMMENT)] %>% unique()

# influencers by SHARE
dta[,SHARE := sum(SHARE_COUNT), FROM][,.(SHARE,FROM)][order(-SHARE)] %>% unique() 


# influencers by SHARE II

dta %>% 
  group_by(FROM) %>%
  mutate(SHARE = sum(SHARE_COUNT),
         ACTIVITY = n(),
         DISPERSION = SHARE/ACTIVITY) %>%
  select(FROM,ACTIVITY,SHARE,DISPERSION) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(DISPERSION)) %>%
  unique()


#fb[, `:=` (ACTIVITY = .N , SHARE = sum(SHARE_COUNT), DISPERSION = SHARE/ACTIVITY), FROM][,.(FROM,ACTIVITY,SHARE,DISPERSION)][ACTIVITY >= 100][order(-DISPERSION)] %>% unique()

# letters by influencer

dta %>% 
  group_by(FROM) %>%
  mutate(LETTERS = sum(SHARE_COUNT),
         ACTIVITY = n(),
         EFFORT = LETTERS/ACTIVITY) %>%
  select(FROM,ACTIVITY,LETTERS,EFFORT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFORT)) %>%
  unique()



#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM]

#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS/ACTIVITY), FROM][,.(FROM,ACTIVITY,LETTERS, EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()


# posts by REACH

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 

# posts by LIKE

dta[,.(LIKE_COUNT,FROM,FULL_TEXT, URL)][order(-LIKE_COUNT)] 

# posts by INTERACTIONS

dta[,.(INTERACTIONS,FROM,FULL_TEXT, URL)][order(-INTERACTIONS)]  

# posts by COMMENT

dta[,.(COMMENT_COUNT,FROM,FULL_TEXT, URL)][order(-COMMENT_COUNT)]  

# posts by SHARE

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 
```

#### Slova u prosječnom  naslovu i tesktu (deskripiva)

```{r echo=F, eval=F, message=F , warning= FALSE}

# how many letters in a title
dt[,
       .(Avg = mean(nchar(TITLE), na.rm = T),
         STD = sd(nchar(TITLE), na.rm = T),
         min = min(nchar(TITLE), na.rm = T),
         max = max(nchar(TITLE), na.rm = T)),
      SOURCE_TYPE][order(-Avg),]
# how many letters in a text
dt[,
       .(Avg = mean(nchar(FULL_TEXT)),
         STD = sd(nchar(FULL_TEXT)),
         min = min(nchar(FULL_TEXT)),
         max = max(nchar(FULL_TEXT))),
      SOURCE_TYPE][order(-Avg),]


```

#### Tokenizirani tekst prije i nakon čišćenja (broj riječi)

```{r echo=F, eval=T, message=F , warning= FALSE}
# dim before tokenize
dim(dta)

# tokenize
dta %>% 
  unnest_tokens(word, FULL_TEXT) -> n_token

# dim after tokenize
dim(n_token)

# check
# fb_token %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#     sample_n(.,100)

# remove stop words, numbers, single letters
n_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
  filter(!is.na(word)) -> n_tokenTidy

# check
# fb_tokenTidy  %>% 
#   select(FROM, word, MENTION_SNIPPET ) %>%
#   sample_n(.,100)

# dim after clean
dim(n_tokenTidy)

```


#### Najčešće riječi (>150)

```{r echo=F, eval=T, message=F , warning= FALSE, fig.height=12}
## Most common words
n_tokenTidy[,.N,by = word][order(-N),] %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))


an_tokenTidy <- n_tokenTidy %>%
  group_by(word, FROM) %>%
  summarise(N = n()) %>%
  arrange(desc(N)) %>%
  filter(N > 150) 

  
ggplot(an_tokenTidy, aes(reorder(word, N), N, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ FROM, scales = "free_y") +
  coord_flip() +
  theme_economist()



  
## Vizualize most common words
# fb_tokenTidy[,.N,by = word][N>10000][order(-N),][,word := reorder(word,N)] %>%
#   ggplot(aes(word, N)) +
#   geom_col() +
#   xlab(NULL) +
#   coord_flip() +
#   theme_economist()
# 
# ## Vizualize most common words over time
# fb_tokenTidy[,DAY:=floor_date(DATE,"day")][,N:=.N,by=DAY][,gn:=sum(N)][
#   word %in% c("ukrajini", "pomoć", "humanitarnu", "udruga", "prava"),] %>%
#   ggplot(., aes(DAY,  N / gn)) + 
#    geom_point() +
#    ggtitle("Učestalost korištenja riječi") +
#    ylab("% ukupnih riječi") +
#    geom_smooth() +
#    facet_wrap(~ word, scales = "free_y") +
#    scale_y_continuous(labels = scales::percent_format())+
#    theme_economist()

```


```{r echo=F, eval=F, message=F , warning= FALSE}
## WordCloud(vulgaris)
fb_tokenTidy %>%
  anti_join(CroSentilex_Gold,by="word") %>% 
  count(word) %>% 
  arrange(desc(n)) %>%
  top_n(100) %>%
  with(wordcloud(word, n, max.words = 80)) 
```


#### Sentiment

```{r echo=F, eval=F, message=F , warning= FALSE}
## Sentiment over time
vizualiziraj_sentiment <- function(dataset, frq = "week") {
dataset %>%
  inner_join( Crosentilex_sve, by = "word") %>%
  filter(!is.na(word)) %>%
  select(word, brija, DATE, sentiment) %>% 
  unique() %>%
  spread(. , brija, sentiment) %>%
  mutate(sentiment = POZ - NEG) %>%
  select(word, DATE, sentiment) %>% 
  group_by(word) %>% 
  mutate(count = n()) %>%
  arrange(desc(count)) %>%
  mutate( score = sentiment*count) %>%
  ungroup() %>%
  group_by(DATE) %>%
  arrange(desc(DATE)) -> sm
 
sm %>%
  select(DATE, score) %>%
  group_by(DATE = floor_date(DATE, frq)) %>%
  summarise(Dnevni_sent = sum(score, na.rm = TRUE)) %>%
  ggplot(., aes(DATE, Dnevni_sent)) +
  geom_bar(stat = "identity") + 
  ggtitle(paste0("Sentiment over time;freqency:", frq)) +
  ylab("SentimentScore") +
  theme_economist()-> gg_sentiment_kroz_vrijeme_qv
gg_sentiment_kroz_vrijeme_qv
}
vizualiziraj_sentiment(fb_tokenTidy,"week")
```

```{r echo=F, eval=T, message=F , warning= FALSE, fig.width=12}
## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
                                 sentiment == 1 ~ "NEGATIVE",
                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  theme_economist() -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(n_tokenTidy,30)
```

##### svi
```{r echo=F, eval=T, message=F , warning= FALSE}
## ComparisonCloud
n_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

##### vecernji
```{r echo=F, eval=T, message=F , warning= FALSE}

n_tokenTidy %>%
  filter(FROM == "vecernji.hr") %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

##### index
```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  filter(FROM == "index.hr") %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

##### tportal
```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  filter(FROM == "tportal.hr") %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

##### 24h
```{r echo=F, eval=T, message=F , warning= FALSE}
n_tokenTidy %>%
  filter(FROM == "24sata.hr") %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```


#### Najpozitivniji i najnegativni portali

```{r echo=F, eval=T, message=F , warning= FALSE}
## Negative profiles
wCount <- n_tokenTidy %>% 
  group_by(FROM) %>%
  summarise(word = n())
CroSentilex_Gold_neg <- CroSentilex_Gold %>% filter(sentiment == 1)
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
n_tokenTidy %>% 
  semi_join(CroSentilex_Gold_neg, by= "word") %>%
  group_by(FROM) %>% 
  summarise(negWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(negativnostIndex = (negWords/word)*100) %>%
  arrange(desc(negativnostIndex)) %>%
  select(FROM, negativnostIndex)  %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=F, eval=T, message=F , warning= FALSE}
## Najpozitivniji portali
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
n_tokenTidy %>% 
  semi_join(CroSentilex_Gold_poz, by= "word") %>%
  group_by(FROM) %>% 
  summarise(pozWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(pozitivnostIndex = (pozWords/word)*100) %>%
  arrange(desc(pozitivnostIndex)) %>%
  select(FROM, pozitivnostIndex)  %>%
  datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
```


#### Analysis of most liked posts

```{r echo=F, eval=F, message=F , warning= FALSE, fig.width=12}
# Select postes with +5k likes
dta[order(-LIKE_COUNT)][LIKE_COUNT >= 500] -> fb_TopLike
# check
dim(fb_TopLike)
# influencer activity
fb_TopLike[,.N,FROM][order(-N)]
# influencer by like
fb_TopLike[,.(FROM, LIKE_COUNT)][,LIKES := sum(LIKE_COUNT),FROM][,.(FROM,LIKES)][order(-LIKES)] %>% unique()

# tokenize
fb_TopLike %>% 
  unnest_tokens(word, FULL_TEXT) -> fb_token_TopLike

# dim after tokenize
dim(fb_token_TopLike)

# remove stop words, numbers, single letters
fb_token_TopLike %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> fb_tokenTidy_TopLike
# remove NA
fb_tokenTidy_TopLike %>%
  filter(!is.na(word)) -> fb_tokenTidy_TopLike


## most common words
fb_tokenTidy_TopLike[,.N,by = word][order(-N),]
## word cloud
# fb_tokenTidy_TopLike %>%
#   anti_join(CroSentilex_Gold,by="word") %>% 
#   count(word) %>% 
#   arrange(desc(n)) %>%
#   top_n(100) %>%
#   with(wordcloud(word, n, max.words = 120)) 

```

#### Term importance (najbitnije riječi po portalima)

```{r frekvencija, eval = F, echo = F, message=F, warning=F, fig.height=10, fig.width=12}
## Udio riječi po domenama
domenaWords <- n_tokenTidy %>%
  filter(FROM %in% c("vecernji.hr", "jutarnji.hr", "telegram.hr", "index.hr", "24sata.hr")) %>% 
  count(FROM, word, sort = T)
  
ukupnoWords <- domenaWords %>%
  group_by(FROM) %>%
  summarise(totWords = sum(n))
domenaWords <- left_join(domenaWords, ukupnoWords)
# domenaWords %>% head(15)
# domenaWords %>% 
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
  bind_tf_idf(word, FROM, n)
#idf %>% head(10)
# idf %>% 
#   select(-totWords) %>%
#   arrange(desc(tf_idf))
idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  mutate(FROM = factor(FROM)) %>%
  group_by(FROM) %>% 
  top_n(20) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() +
  theme_economist()
```


#### Phrases

```{r ,eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}
fb_bigram <- dta %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
#fb_bigram %>% head(10)
# fb_bigram %>%
#   count(bigram, sort = T) %>%
#   head(25) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>% 
  count(FROM,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  head(45) 
```


##### Skupno
```{r ,eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}


# Najvažniji bigrami po domenama
 bigram_tf_idf <- bigrams_united %>% 
#  filter (!is.na(bigram)) %>%
  count(FROM, bigram) %>%
  bind_tf_idf(bigram, FROM, n) %>%
  arrange(desc(tf_idf))

bigram_tf_idf %>%
  arrange(desc(tf_idf)) %>%
  filter(tf_idf > 0.09) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
#  group_by(FROM) %>% 
#  top_n(20) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
#  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()

```

#### Domene

```{r ,eval = T, echo = F, message=F, warning=F, fig.height=15, fig.width=15}

bigram_tf_idf %>%
  filter(FROM %in% c("vecenji.hr", "jutarnji.hr", "24sata.hr", "telegram.hr")) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(FROM) %>% 
  top_n(20) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()

```

#### PHRASES CORRELATION

```{r echo=F, eval = F,message=F, warning=F, fig.height=12, fig.width=12}

fb_tokenTidy %>% 
#  filter(datum > "2020-02-20") %>%
  group_by(word) %>%
  filter(n() > 200) %>%
  filter(!is.na(word)) %>%
  pairwise_cor(word,DATE, sort = T) -> corsWords
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
  filter(item1 %in% c("kupnja", "akcija", "poklon")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() + 
  theme_economist()

```


### TEMATIC ANALYSIS (moguće napraviti i po portalima)


```{r echo=F, eval=T, message=F , warning= FALSE, fig.height=12, fig.width=12}
n_tokenTidy %>%
  count(FROM, word, sort = TRUE) %>%
  cast_dtm(FROM, word,n) -> dtm
fb_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
fb_LDA_tidy <- tidy(fb_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- fb_LDA_tidy %>%
  drop_na(.) %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + 
  theme_economist()
```


```{r echo=F, eval=F, message=F , warning= FALSE}
bitokens <- dta %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)


bitokens %>% head(10)

bitokens %>%
  count(bigram, sort = T) %>%
  head(15)


bitokens_sep <- bitokens %>%
  separate(bigram, c("word1","word2"), sep = " ")


bitokens_sep_tidy <- bitokens_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) %>%
  filter(!is.na(word1)) %>%
  filter(!is.na(word2))




bitokens_sep_counts <- bitokens_sep_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- bitokens_sep_counts %>%
  unite(bigram, word1, word2, sep = " ")
#bigrams_united


bigrams_united %>% 
  count(FROM,bigram,sort = T) -> topicBigram
# Najvažniji bigrami po domenama
 bigram_tf_idf <- bigrams_united %>%
  count(FROM, bigram) %>%
  bind_tf_idf(bigram, FROM, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  filter(FROM %in% c("elladvornik", "imerovic_sandi_budivelik", "elaajerkovic" )) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(FROM) %>% 
  top_n(7) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()
```

