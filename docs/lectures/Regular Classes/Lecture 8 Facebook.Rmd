---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 8: Facebook"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=FALSE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
#library(reportMD)
```


# OUTLINE
<br>
<br>
<br>
- DATA IMPORT
<br>
<br>
- DESCRIPTIVE EXPLORATION
<br>
<br>
- TEXT TOKENIZATION AND CLEAN
<br>
<br>
- ANALYSIS


# DATA IMPORT

#### ARTICLES

```{r, echo=T, include=TRUE,eval = T, message=FALSE, warning=FALSE}
# read in data
# path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
# raw <- list.files(path = path , pattern="xlsx")
# raw_path <- paste0(path, "/", raw)
# all_raw <- map_df(raw_path, read_excel)

dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
```


```{r, echo=T, include=TRUE, eval = T, message=FALSE, warning=FALSE}
# some basic data wrangle
# all <- as.data.table(all_raw)
# all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
# posts <- all[!duplicated(all),]
# rm(all,all_raw)
# select facebook
# fb <- posts[SOURCE_TYPE == "facebook",]
# fb <- as.data.table(fb)
dta <- dt %>% 
  filter(SOURCE_TYPE == "facebook")
```

## Basic descriptives

```{r echo=FALSE, message=FALSE , warning= FALSE}

# date range
range(dta$DATE)
# number of articles
nrow(dta)

# articles over time
daily_counts <- dta %>%
  group_by(DATE) %>%
  summarise(count = n())

# descriptives 
summ <- daily_counts %>% 
  summarize(min = min(count), max = max(count), 
            mean = mean(count), q1= quantile(count, probs = 0.25), 
            median = median(count), q3= quantile(count, probs = 0.75),
            sd = sd(count)) %>%
  mutate_if(is.numeric, round, digits=2) 

summ

# create plot
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
  geom_line() +
  labs(x = "Date", y = "Number of Articles")
 

```

## Bigest profiles

```{r echo=T, message=FALSE , warning= FALSE}
# Portals by activity
activity <- dta %>%
  group_by(FROM) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Portals by reach
reach <- dta %>%
  group_by(FROM) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Portals by likes
like <- dta %>%
  group_by(FROM) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Portals by comments
comment <- dta %>%
  group_by(FROM) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Portals by shares
shares <- dta %>%
  group_by(FROM) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```

## Authors

```{r echo=T, message=FALSE , warning= FALSE}
# Authors by activity
authors <- dta %>%
  group_by(AUTHOR) %>%
  summarise(count = n()) %>%
  mutate(percent = round(count / sum(count) * 100,2)) %>% 
  arrange(desc(count))

datatable(authors, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Authors by reach
reach <- dta %>%
  group_by(AUTHOR) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach))

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Authors by likes
like <- dta %>%
  group_by(AUTHOR) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like))

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Authors by comments
comment <- dta %>%
  group_by(AUTHOR) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment))

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE}
# Authors by shares
shares <- dta %>%
  group_by(AUTHOR) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares))

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))
```


## Articles

```{r echo=T, message=FALSE , warning= FALSE, eval=FALSE}
# Articles by activity
articles <-  dta %>%
  group_by(TITLE) %>%
  summarise(count = n()) %>% 
  arrange(desc(count)) %>%
  slice(1:1000)

datatable(articles, options = list(scrollX = TRUE, scrollY = "500px"))

```

```{r echo=T, message=FALSE , warning= FALSE, eval=T}
# Articles by reach
reach <- dta %>%
  group_by(TITLE) %>%
  summarise(reach = sum(REACH)) %>%
  arrange(desc(reach)) %>%
  slice(1:1000)

datatable(reach, options = list(scrollX = TRUE, scrollY = "500px"))
```


```{r echo=T, message=FALSE , warning= FALSE, eval=T}
# Articles by likes
like <- dta %>%
  group_by(TITLE) %>%
  summarise(like = sum(LIKE_COUNT, na.rm = T)) %>%
  mutate(percent = round(like / sum(like) * 100,2)) %>% 
  arrange(desc(like)) %>%
  slice(1:1000)

datatable(like, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE, eval=T}
# Articles by comments
comment <- dta %>%
  group_by(TITLE) %>%
  summarise(comment = sum(COMMENT_COUNT, na.rm = T)) %>%
  mutate(percent = round(comment / sum(comment) * 100,2)) %>% 
  arrange(desc(comment)) %>%
  slice(1:1000)

datatable(comment, options = list(scrollX = TRUE, scrollY = "500px"))
```

```{r echo=T, message=FALSE , warning= FALSE, eval=T}
# Arcicles by shares
shares <- dta %>%
  group_by(TITLE) %>%
  summarise(shares = sum(SHARE_COUNT, na.rm = T)) %>%
  mutate(percent = round(shares / sum(shares) * 100,2)) %>% 
  arrange(desc(shares)) %>%
  slice(1:1000)

datatable(shares, options = list(scrollX = TRUE, scrollY = "500px"))

```


#### LEXICON 

```{r leksikoni, echo = F, message=F, warning=F}
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8")  %>%
                   rename(word = "V1", sentiment = "V2" ) %>%
                   mutate(brija = "NEG")
 
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
                                   header = FALSE,
                                   sep = " ",
                                   stringsAsFactors = FALSE,
                                   fileEncoding = "UTF-8") %>%
                    rename(word = "V1", sentiment = "V2" ) %>%
                    mutate(brija = "POZ")
 
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data 
head(sample_n(Crosentilex_sve,1000),15)

 
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
                                 header = FALSE,
                                 sep = " ",
                                 stringsAsFactors = FALSE) %>%
                    rename(word = "V1", sentiment = "V2" ) 
 Encoding(CroSentilex_Gold$word) <- "UTF-8"
 CroSentilex_Gold[1,1] <- "dati"
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
 CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
 CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data 
head(sample_n(CroSentilex_Gold,100),15)
 
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
  word = c(
    "jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
    "e","prvi", "dva","dvije","drugi",
    "tri","treći","pet","kod",
    "ove","ova",  "ovo","bez", "kod",
    "evo","oko",  "om", "ek",
    "mil","tko","šest", "sedam",
    "osam",   "čim", "zbog",
    "prema", "dok","zato", "koji", 
    "im", "čak","među", "tek",
    "koliko", "tko","kod","poput", 
    "baš", "dakle", "osim", "svih", 
    "svoju", "odnosno", "gdje",
    "kojoj", "ovi", "toga",
     "ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
  ),
  lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
  bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
```

# DESCRIPTIVE EXPLORATION

```{r tokeni, eval = T}

# influencers by ACTIVITY
dta[,.N,FROM][order(-N)]

# influencers by FOLLOWERS
dta[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique() 

# influencers by REACH
dta[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique() 

# influencers by REACH II

dta %>% 
  group_by(FROM) %>%
  mutate(ACTIVITY = n(),
         REACH = sum(REACH),
         EFFECT = REACH/ACTIVITY) %>%
  select(FROM,ACTIVITY,REACH,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()
         
#  fb %>% 
#  group_by(FROM) %>%
#  mutate(ACTIVITY = n(),
#         REACH = sum(REACH),
#         EFFECT = REACH/ACTIVITY) %>%
#  filter(ACTIVITY>100) %>%
#  arrange(desc(EFFECT)) %>%
#  unique() -> fb
#fb <- as.data.table(fb)
# influencers by LIKE
fb[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique() 

# influencers by LIKE II

dta %>% 
  group_by(FROM) %>%
  mutate(LIKE = sum(LIKE_COUNT),
         ACTIVITY = n(),
         EFFECT = LIKE/ACTIVITY) %>%
  select(FROM,ACTIVITY,LIKE,EFFECT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFECT)) %>%
  unique()



# influencers by INTERACTIONS
dta[,INTERACTIONS := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique() 


# influencers by COMMENT
dta[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique() 


# influencers by COMMENT II

dta %>% 
  group_by(FROM) %>%
  mutate(COMMENT = sum(COMMENT_COUNT),
         ACTIVITY = n(),
         ENGAGE = COMMENT/ACTIVITY) %>%
  select(FROM,ACTIVITY,COMMENT,ENGAGE) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(ENGAGE)) %>%
  unique()

#fb[, `:=` (ACTIVITY = .N , COMMENT = sum(COMMENT_COUNT),ENGAGE = COMMENT/ACTIVITY), FROM][,.(FROM,ACTIVITY,COMMENT, ENGAGE)][ACTIVITY >= 100][order(-COMMENT)] %>% unique()

# influencers by SHARE
dta[,SHARE := sum(SHARE_COUNT), FROM][,.(SHARE,FROM)][order(-SHARE)] %>% unique() 


# influencers by SHARE II

dta %>% 
  group_by(FROM) %>%
  mutate(SHARE = sum(SHARE_COUNT),
         ACTIVITY = n(),
         DISPERSION = SHARE/ACTIVITY) %>%
  select(FROM,ACTIVITY,SHARE,DISPERSION) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(DISPERSION)) %>%
  unique()


#fb[, `:=` (ACTIVITY = .N , SHARE = sum(SHARE_COUNT), DISPERSION = SHARE/ACTIVITY), FROM][,.(FROM,ACTIVITY,SHARE,DISPERSION)][ACTIVITY >= 100][order(-DISPERSION)] %>% unique()

# letters by influencer

dta %>% 
  group_by(FROM) %>%
  mutate(LETTERS = sum(SHARE_COUNT),
         ACTIVITY = n(),
         EFFORT = LETTERS/ACTIVITY) %>%
  select(FROM,ACTIVITY,LETTERS,EFFORT) %>%
  filter(ACTIVITY>100) %>%
  arrange(desc(EFFORT)) %>%
  unique()



#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM]

#fb[, `:=` (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS/ACTIVITY), FROM][,.(FROM,ACTIVITY,LETTERS, EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()


# posts by REACH

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 

# posts by LIKE

dta[,.(LIKE_COUNT,FROM,FULL_TEXT, URL)][order(-LIKE_COUNT)] 

# posts by INTERACTIONS

dta[,.(INTERACTIONS,FROM,FULL_TEXT, URL)][order(-INTERACTIONS)]  

# posts by COMMENT

dta[,.(COMMENT_COUNT,FROM,FULL_TEXT, URL)][order(-COMMENT_COUNT)]  

# posts by SHARE

dta[,.(SHARE_COUNT,FROM,FULL_TEXT, URL)][order(-SHARE_COUNT)] 


# how many letters in a title
dta[,
       .(Avg = mean(nchar(TITLE), na.rm = T),
         STD = sd(nchar(TITLE), na.rm = T),
         min = min(nchar(TITLE), na.rm = T),
         max = max(nchar(TITLE), na.rm = T)),
      SOURCE_TYPE][order(-Avg),]
# how many letters in a text
dta[,
       .(Avg = mean(nchar(FULL_TEXT)),
         STD = sd(nchar(FULL_TEXT)),
         min = min(nchar(FULL_TEXT)),
         max = max(nchar(FULL_TEXT))),
      SOURCE_TYPE][order(-Avg),]


```


```{r echo = F, eval=FALSE, include = F}


nog <- fb[,.N,FROM][order(-N)]
nogomet <- c("Index Sport","FC Barcelona Balkan fans", "gol.hr", "Sportnet.hr","Sportske novosti","Nogometne novosti", "PSK - Sports & Casino", "Sportski.net", "Hajdučke vijesti", "Germania Sport Kladionica", "Fudbalske Gluposti - FG", "GNK Dinamo Zagreb", "NFL Balkan", "Uživo sa Poljuda", "nogometplus.net", "Zona Dinamo", "NK Istra 1961", "HNK Hajduk Split", "Hrvatski nogometni savez", "Nogometni transferi", "Nogometne Ikone","NK Rijeka","Sport atraktivno","NK Slaven Belupo","Svijet Sporta","HNK Gorica","NK Lokomotiva Zagreb", "Reprezentacija Bosne i Hercegovine")

nog <- fb[FROM %in% nogomet,]

write.csv2(nog, "D:/LUKA/Freelance/Mediatoolkit/nogometFB.csv")


ap <- posts[SOURCE_TYPE %in% c("instagram", "youtube")]


ap[SOURCE_TYPE == "youtube",.N,FROM][order(-N)] %>% 
  filter(grepl("Doct",FROM))

igyt <- c("elladvornik","hana","elaajerkovic", "ADRI", "Zakon Braće","Nugato", "Marko Vuletić","BloodMaster")

igyt <- ap[FROM %in% igyt,]






write.csv2(igyt, "D:/LUKA/Freelance/Mediatoolkit/influenceIGYT.csv")


posts %>% 
  filter(grepl("Marija Selak", FULL_TEXT))




forum <- c("Zoran Milanović, predsjednik Republike Hrvatske vol. IV", "Andrej Plenković - predsjednik HDZ-a i predsjednik Vlade RH IV"

ap <- forum[TITLE == "Andrej Plenković - predsjednik HDZ-a i predsjednik Vlade RH IV",]

forumZM <- write_csv2(zm, "D:/LUKA/Freelance/Mediatoolkit/forumZM.csv")
forumAP <- write_csv2(ap, "D:/LUKA/Freelance/Mediatoolkit/forumAP.csv")

           
```



# TEXT TOKENIZATION AND CLEAN


#### TOKENIZE

```{r echo = T, eval=T, include =T }
# dim before tokenize
dim(dta)

# tokenize
dta %>% 
  unnest_tokens(word, FULL_TEXT) -> fb_token

# dim after tokenize
dim(fb_token)

# check
fb_token %>% 
  select(FROM, word, MENTION_SNIPPET ) %>%
    sample_n(.,100)
```

#### CLEAN

```{r}
# remove stop words, numbers, single letters
fb_token %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> fb_tokenTidy
# remove NA
fb_tokenTidy %>%
  filter(!is.na(word)) -> fb_tokenTidy

# check
fb_tokenTidy  %>% 
  select(FROM, word, MENTION_SNIPPET ) %>%
  sample_n(.,100)

```


# ANALYSE


#### BASIC FREQUENCIES

```{r, fig.height=12, message=F, warning=F}
## Most common words
fb_tokenTidy[,.N,by = word][order(-N),]

## Vizualize most common words
fb_tokenTidy[,.N,by = word][N>7000][order(-N),][,word := reorder(word,N)] %>%
  ggplot(aes(word, N)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  theme_economist()

## Vizualize most common words over time
fb_tokenTidy[,DAY:=floor_date(DATE,"day")][,N:=.N,by=DAY][,gn:=sum(N)][
  word %in% c("zagreb", "život", "poklon", "grad", "cijena"),] %>%
  ggplot(., aes(DAY,  N / gn)) + 
   geom_point() +
   ggtitle("Učestalost korištenja riječi") +
   ylab("% ukupnih riječi") +
   geom_smooth() +
   facet_wrap(~ word, scales = "free_y") +
   scale_y_continuous(labels = scales::percent_format())+
   theme_economist()

```

Simple WordCloud:

```{r WCloud, message=F, warning=F}
## WordCloud(vulgaris)
fb_tokenTidy %>%
  anti_join(CroSentilex_Gold,by="word") %>% 
  count(word) %>% 
  arrange(desc(n)) %>%
  top_n(100) %>%
  with(wordcloud(word, n, max.words = 80)) 
```

#### SENTIMENT

```{r sentimentTempus, message=F, warning=F}
## Sentiment over time
vizualiziraj_sentiment <- function(dataset, frq = "week") {
dataset %>%
  inner_join( Crosentilex_sve, by = "word") %>%
  filter(!is.na(word)) %>%
  select(word, brija, DATE, sentiment) %>% 
  unique() %>%
  spread(. , brija, sentiment) %>%
  mutate(sentiment = POZ - NEG) %>%
  select(word, DATE, sentiment) %>% 
  group_by(word) %>% 
  mutate(count = n()) %>%
  arrange(desc(count)) %>%
  mutate( score = sentiment*count) %>%
  ungroup() %>%
  group_by(DATE) %>%
  arrange(desc(DATE)) -> sm
 
sm %>%
  select(DATE, score) %>%
  group_by(DATE = floor_date(DATE, frq)) %>%
  summarise(Dnevni_sent = sum(score, na.rm = TRUE)) %>%
  ggplot(., aes(DATE, Dnevni_sent)) +
  geom_bar(stat = "identity") + 
  ggtitle(paste0("Sentiment over time;freqency:", frq)) +
  ylab("SentimentScore") +
  theme_economist()-> gg_sentiment_kroz_vrijeme_qv
gg_sentiment_kroz_vrijeme_qv
}
vizualiziraj_sentiment(fb_tokenTidy,"week")
```


```{r doprinoSentimentu, message=F, warning=F, fig.width=10}
## Sentiment 
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
  inner_join(CroSentilex_Gold, by = "word") %>% 
  count(word, sentiment,sort = TRUE) %>% 
  group_by(sentiment) %>%
  top_n(no) %>%
  ungroup() %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
                                 sentiment == 1 ~ "NEGATIVE",
                                 sentiment == 2 ~ "POSITIVE")) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  ggtitle( "Sentiment") +
  labs( x = "Riječ", y = "Number of words") +
  facet_wrap(~ sentiment, scales = "free_y") +
  coord_flip() +
  theme_economist() -> gg_doprinos_sentimentu
  
 gg_doprinos_sentimentu
 
}
doprinos_sentimentu(fb_tokenTidy,15)
```

```{r WCloutSent, warning=F, message=F}
## ComparisonCloud
fb_tokenTidy %>%
  inner_join(CroSentilex_Gold,by="word") %>% 
  count(word, sentiment) %>% 
  top_n(200) %>%
  mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
                                 sentiment == 1 ~ "-",
                                 sentiment == 2 ~ "+")) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
                   max.words = 120)

```

```{r negDomen, warning=F, message=F}
## Negative profiles
wCount <- fb_tokenTidy %>% 
  group_by(FROM) %>%
  summarise(word = n())
CroSentilex_Gold_neg <- CroSentilex_Gold %>% filter(sentiment == 1)
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
fb_tokenTidy %>% 
  semi_join(CroSentilex_Gold_neg, by= "word") %>%
  group_by(FROM) %>% 
  summarise(negWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(negativnostIndex = (negWords/word)*100) %>%
  arrange(desc(negativnostIndex)) %>%
  select(FROM, negativnostIndex)
```

```{r pozDomen, warning=F, message=F}
## Najpozitivniji portali
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
fb_tokenTidy %>% 
  semi_join(CroSentilex_Gold_poz, by= "word") %>%
  group_by(FROM) %>% 
  summarise(pozWords = n()) %>%
  left_join(wCount, by = "FROM") %>%
  mutate(pozitivnostIndex = (pozWords/word)*100) %>%
  arrange(desc(pozitivnostIndex)) %>%
  select(FROM, pozitivnostIndex)
```


# ANALYSIS OF MOST LIKED POSTS

```{r}
# Select postes with +5k likes
dta[order(-LIKE_COUNT)][LIKE_COUNT >= 1000] -> fb_TopLike
# check
dim(fb_TopLike)
# influencer activity
fb_TopLike[,.N,FROM][order(-N)]
# influencer by like
fb_TopLike[,.(FROM, LIKE_COUNT)][,LIKES := sum(LIKE_COUNT),FROM][,.(FROM,LIKES)][order(-LIKES)] %>% unique()

# tokenize
fb_TopLike %>% 
  unnest_tokens(word, FULL_TEXT) -> fb_token_TopLike

# dim after tokenize
dim(fb_token_TopLike)

# remove stop words, numbers, single letters
fb_token_TopLike %>% 
  anti_join(stop_corpus, by = "word") %>%
  mutate(word = gsub("\\d+", NA, word)) %>%
  mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> fb_tokenTidy_TopLike
# remove NA
fb_tokenTidy_TopLike %>%
  filter(!is.na(word)) -> fb_tokenTidy_TopLike

```

```{r warning=F, message=FALSE}
## most common words
fb_tokenTidy_TopLike[,.N,by = word][order(-N),]
## word cloud
fb_tokenTidy_TopLike %>%
  anti_join(CroSentilex_Gold,by="word") %>% 
  count(word) %>% 
  arrange(desc(n)) %>%
  top_n(100) %>%
  with(wordcloud(word, n, max.words = 80)) 

```


#### TERM IMPORTANCE


```{r frekvencija, message=F, warning=F, fig.height=10, fig.width=10}
## Udio riječi po domenama
domenaWords <- fb_tokenTidy_TopLike %>%
  filter(FROM %in% c("24sata", "Hajdučke vijesti", "Ivan Vilibor Sinčić", "Index.hr" )) %>% 
  count(FROM, word, sort = T)
  
ukupnoWords <- domenaWords %>%
  group_by(FROM) %>%
  summarise(totWords = sum(n))
domenaWords <- left_join(domenaWords, ukupnoWords)
# domenaWords %>% head(15)
# domenaWords %>% 
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
  bind_tf_idf(word, FROM, n)
idf %>% head(10)
# idf %>% 
#   select(-totWords) %>%
#   arrange(desc(tf_idf))
idf %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  mutate(FROM = factor(FROM)) %>%
  group_by(FROM) %>% 
  top_n(11) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() +
  theme_economist()
```


#### PHRASES

```{r nGRAMI, message=F, warning=F, fig.height=15, fig.width=10}
fb_bigram <- fb_TopLike %>%
  unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
fb_bigram %>% head(10)
fb_bigram %>%
  count(bigram, sort = T) %>%
  head(15) 
fb_bigram_sep <- fb_bigram %>%
  separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
  filter(!word1 %in% stop_corpus$word) %>%
  filter(!word2 %in% stop_corpus$word) %>%
  mutate(word1 = gsub("\\d+", NA, word1)) %>%
  mutate(word2 = gsub("\\d+", NA, word2)) %>%
  mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
  mutate(word2 = gsub("^[a-zA-Z]$", NA, word2)) 
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>% 
  count(word1, word2, sort = TRUE)

bigrams_united <- fb_bigram_tidy %>%
  unite(bigram, word1, word2, sep = " ") %>%
  filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>% 
  count(FROM,bigram,sort = T) -> topicBigram

bigrams_united %>%
  count(bigram, sort = T) %>%
  head(35) 


# Najvažniji bigrami po domenama
 bigram_tf_idf <- bigrams_united %>% 
#  filter (!is.na(bigram)) %>%
  count(FROM, bigram) %>%
  bind_tf_idf(bigram, FROM, n) %>%
  arrange(desc(tf_idf))
bigram_tf_idf %>%
  filter(FROM %in% c("Ivan Vilibor Sinčić", "Zoran Šprajc", "Velimir Bujanec", "Kristijan Iličić" )) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(FROM) %>% 
  top_n(10) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()

```


```{r, message=F, warning=F, fig.height=15, fig.width=10}
bigram_tf_idf %>%
  filter(FROM %in% c("Nikola Grmoja", "Mirko CroCop Filipovic", "Andrej Plenković", "Marko Perković Thompson" )) %>%
  arrange(desc(tf_idf)) %>%
  mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>% 
  group_by(FROM) %>% 
  top_n(5) %>% 
  ungroup() %>%
  ggplot(aes(bigram, tf_idf, fill = FROM)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~FROM, ncol = 2, scales = "free") +
  coord_flip() + 
  theme_economist()
```


#### PHRASES CORRELATION

```{r eval = F,message=F, warning=F, fig.height=10, fig.width=10}

fb_tokenTidy %>% 
#  filter(datum > "2020-02-20") %>%
  group_by(word) %>%
  filter(n() > 20) %>%
  filter(!is.na(word)) %>%
  pairwise_cor(word,DATE, sort = T) -> corsWords
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
  filter(item1 %in% c("kupnja", "akcija", "sex", "poklon")) %>%
  group_by(item1) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(item2 = reorder(item2, correlation)) %>%
  ggplot(aes(item2, correlation)) +
  geom_bar(stat = "identity") +
  facet_wrap(~ item1, scales = "free") +
  coord_flip() + 
  theme_economist()

```


### TEMATIC ANALYSIS


```{r TEME, message=F, warning=F, fig.height=10, fig.width=10}
fb_tokenTidy_TopLike %>%
  count(FROM, word, sort = TRUE) %>%
  cast_dtm(FROM, word,n) -> dtm
fb_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
fb_LDA_tidy <- tidy(fb_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- fb_LDA_tidy %>%
  drop_na(.) %>%
  group_by(topic) %>%
  top_n(15, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + 
  theme_economist()
```



```{r TEMEbigram, eval=T, message=F, warning=F,fig.height=10, fig.width=10}
# Bigrami 
topicBigram %>%
  cast_dtm(FROM, bigram,n) -> dtmB
insta_LDA <- LDA(dtmB, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
  drop_na(.) %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered() + 
  theme_economist()
```




















