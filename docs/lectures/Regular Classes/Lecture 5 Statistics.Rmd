---
title: "Learning Social Media Analytics"
# subtitle: "<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>"
subtitle: "Lecture 5: Statistics"
author: "Luka Sikic, PhD <br> Faculty of Croatian Studies | [LSMA](https://lusiki.github.io/Learning-Social-Media-Analytics/)"
output:
  html_document:
    code_folding: show
    theme: flatly
    highlight: haddock
    toc: yes
    toc_depth: 4
    toc_float: yes
    keep_md: yes
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = T, message = F, warning = F)
```


```{r libs, include=TRUE, echo=TRUE,message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
```


# OUTLINE

1. DESCRIPTIVE STATISTICS

2. COMPARING MEANS

3. CATEGORICAL ANALYSIS

4. ANOVA

5. LINEAR REGRESSION



# DESCRIPTIVE STATISTICS


```{r, echo=FALSE, eval = T}
# FULL SAMPLE
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel) %>% data.table()
#all_raw <- all_raw  %>%  mutate(DATE = as.Date(DATE,"%Y-%m-%d" ))
```

```{r}
# sneek peak
glimpse(all_raw[sample(nrow(all_raw)),1:8])
# size of the data
dim(all_raw)
# time rage
range(all_raw$DATE)
```





```{r}
# how much activity
all_raw[SOURCE_TYPE == "web", .N]
# how many authors
all_raw[SOURCE_TYPE == "web", length(unique(all_raw$AUTHOR))]
# how many domains?
all_raw[SOURCE_TYPE == "web", length(unique(all_raw$FROM))]
# how many domains?
all_raw[SOURCE_TYPE == "web",
        .(Number = length(unique(FROM)))]
# how many CRO domains?
all_raw[SOURCE_TYPE == "web" & grepl(".hr", unique(all_raw$FROM)),
        .(Number = length(unique(FROM)))]

```

```{r, echo = FALSE}
web <- all_raw[SOURCE_TYPE == "web",]
```



```{r}
# some domains?
web[,.(UniqueDomains = unique(FROM))] %>% sample_n(5)
# some CRO domains?
web[str_detect(web$FROM, "hr"),.(UniqueDomains = unique(FROM))] %>% sample_n(5)
```



```{r}
# check all domains
web[str_detect(web$FROM, ".hr"),.N,
        FROM][order(-N),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))
```



```{r}
web[str_detect(web$FROM, ".hr"),.N,FROM][N >= 50,][order(-N),] %>%
  ggplot(., aes(N)) +
  geom_histogram(bins = 10, color = "#000000", fill = "#0099F8") + 
  geom_vline(aes(xintercept = mean(N)), color = "#000000", size = 1.25) +
  geom_vline(aes(xintercept = mean(N) + sd(N)), color = "#000000", size = 1, linetype = "dashed") +
  geom_vline(aes(xintercept = mean(N) - sd(N)), color = "#000000", size = 1, linetype = "dashed") +
  labs(
    title = "Histogram of portal activity in Croatia",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "Number of articles",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  ) -> gg1
```




```{r , echo=FALSE}
gg1
```




```{r}
# just another view
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][, avg := (N / All)*100][order(-N),] 
```




```{r}

# check overall descriptives (number of articles)
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]

# check ranking by No. of articles
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-N),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )


# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
    .(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(COMMENT),
                                                        stdev = sd(COMMENT),
                                                        total = sum(COMMENT))]

# check ranking by No. of comments
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-COMMENT),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))


# check overall descriptives (number of likes)
web[str_detect(web$FROM, ".hr"),
    .(LIKE = sum(LIKE_COUNT, na.rm = T)),FROM][,.(mean = mean(LIKE),
                                                        stdev = sd(LIKE),
                                                        total = sum(LIKE))]
# check ranking by No. of likes
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-LIKE),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))


# check overall descriptives (reach)
web[str_detect(web$FROM, ".hr"),
    .(REACH = sum(REACH, na.rm = T)),FROM][,.(mean = mean(REACH),
                                                        stdev = sd(REACH),
                                                        total = sum(REACH))]
# check ranking by reach
web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM][order(-REACH),] %>%
  datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))





```

# COMPARING MEANS

Lets compare everage activity of first 100th portals by different criteria(comments,likes,reach).

We first have to subset these rankings:

```{r}
# select data to object
webRanking <- web[str_detect(web$FROM, ".hr"),
          .(.N,REACH = sum(REACH, na.rm = T),
          VIRALITY = sum(VIRALITY, na.rm = T),
          LIKE = sum(LIKE_COUNT, na.rm = T),
          COMMENT = sum(COMMENT_COUNT, na.rm = T)), 
          FROM]
# filter 100 biggest by criteria
portals100 <-  webRanking[head(order(-N),100),.(FROM,N)]
reach100 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)]
like100 <- webRanking[head(order(-LIKE),100),.(FROM,N, LIKE)]
comment100 <- webRanking[head(order(-COMMENT),100),.(FROM,N, COMMENT)]

```

Now lets use one sided t-test to check if 100 biggest potals publish average number of articles (482;see before).

```{r}
# library from the lsr boook
library(lsr)
# do the test
oneSampleTTest(portals100$N, mu=482 )

```


Our data is not intependent so we use paired test:

```{r}
# first check reach vs. like
 t.test( 
   x = reach100$N, # variable 1 is the number of articles for 100 biggest portals by reach
   y = like100$N, # variable 2 is the number of articles for 100 biggest portals by like
   paired = TRUE # paired test
 )

# second check reach vs. comment

t.test( 
   x = reach100$N, 
   y = comment100$N, 
   paired = TRUE 
 )

# third check comment vs. like

t.test( 
   x = comment100$N,
   y = like100$N, 
   paired = TRUE 
 )

# fourth check comment vs. reach

t.test( 
   x = comment100$N, 
   y = reach100$N,
   paired = TRUE # paired test
 )


```


Runings  independent t tests requires some indexing:

```{r}
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[order(-REACH),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[order(-LIKE),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[order(-COMMENT),.(FROM,N, COMMENT)] %>% slice(101:201)
```

Now we can compare if the activity (mean published articles) of 1th and 2nd hundred biggest portals is the same:

```{r}
# first we need to check the date to chose the right test 
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)

# then run the test
t.test( 
   x = portals100$N, 
   y = portals200$N,
   var.equal = FALSE, # Welch test
   paired = FALSE # independent test
 )

# second check for the reach
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)

# then run the test
t.test( 
   x = reach100$N, 
   y = reach200$N,
   var.equal = TRUE, # Student test
   paired = FALSE # independent test
 )
```

To see how good these tests are, one could also check the distributional properties (normality) with histogram, QQ plot and/or Shapiro-Wilk test.


# CATEGORICAL ANALYSIS

Here we have two (famous) tests: **Goodnes of Fit** and **Independence of association**.


### GOODNES OF FIT (GOF)

To run the GOF test we will use the full dataset (not just web) to see weather all of the networks are equally important. Lets explore the data once again:

```{r}
# check the activity per network
table(all_raw$SOURCE_TYPE)
# run the GOF test
goodnessOfFitTest(as.factor(all_raw$SOURCE_TYPE))
```

Different kind of probability specification is possible. For example, we want to test if the probablility of the web activity in the whole social media space equals 40%. To complete the example lets first do some data tweaking:

```{r}
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
# check the size of web vs. other media
table(all_raw$Network)
```


Now, lets run the test:


```{r}
# specify the probablities
probs = c(other = 0.60, web = 0.40)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# another way to run the test
#chisq.test(all_raw$Network, p = c(other = 0.60, web = 0.40))
```





### CHISQ TEST OF INDEPENDENCE

For this test we need to have at least two factorial variables. Lets make some new variables:

```{r}
# package
library(anytime)
library(lubridate)
# convert chr date to date format
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Night", "Morning", "Afternoon", "Evening")
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
                         breaks = breaks,
                         labels = labels,
                         include.lowest=TRUE)][
                           ,INTERDAYTIME := as.factor(INTERDAYTIME)][
                             ,SOURCE_TYPE := as.factor(SOURCE_TYPE)
                             ]
```

Now, we should check how these variables look like:

```{r}

# check the result
table(all_raw$INTERDAYTIME)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)

# we are interested in cross-tabulated View
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)

```

Finally, lets run the test:

```{r}
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
```



# ANOVA

We are interested in differences of web activity response with respect to the intraday period. Lets first select the data of intrest and check relevant descriptives.

```{r}
# first take relevant data from web
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, REACH)]
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),
         COMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
         REACH = mean(REACH, na.rm = TRUE)),
      INTERDAYTIME]
```


Now we can run the anova test(s):

```{r}
# test for likes
summary(aov( formula = LIKE_COUNT ~ INTERDAYTIME, data = anova))
# test for comments
summary(aov( formula = COMMENT_COUNT ~ INTERDAYTIME, data = anova))
# test for reach
summary(aov( formula = REACH ~ INTERDAYTIME, data = anova))
```



### Checking assumptions

# LINEAR REGRESSION

The ususal pipeline consits of following elements:

- explore data (looking at your data, vizualization, summary statistics)
- fit the model
- check quality


```{r}
# select relevant data
reg <- all_raw[SOURCE_TYPE == "web",.(REACH,INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, TITLE, .N)][,TITLE := nchar(TITLE)]
# check descriptives
reg[,.(meanREACH = mean(REACH, na.rm = TRUE),
       meanLIKE = mean(LIKE_COUNT, na.rm = TRUE),
       meanCOMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
       No = mean(N),
       meanTIT = mean(TITLE, na.rm = TRUE)
       )]
```
Lets also make some vizual inspections

```{r}
ggplot(reg, aes(x = log(REACH))) +
  geom_histogram(bins = 50, color = "#000000", fill = "#0099F8") +
  labs(
    title = "Distribution of REACH metric",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "REACH (log scale)",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  )

```


```{r}
ggplot(reg, aes(x = log(LIKE_COUNT))) +
  geom_histogram(bins = 50, color = "#000000", fill = "#0099F8") +
  labs(
    title = "Distribution of LIKE_COUNT metric",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "LIKE (log scale)",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  )
```


```{r}
ggplot(reg, aes(x = log(COMMENT_COUNT))) +
  geom_histogram(bins = 50, color = "#000000", fill = "#0099F8") +
  labs(
    title = "Distribution of COMMENT_COUNT metric",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "COMMENT (log scale)",
    y = "Count"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  )
```




```{r}
ggplot(reg, aes(x = INTERDAYTIME, y = log(REACH))) +
  geom_boxplot() +
  labs(x = "INTERDAYTIME", y = "REACH") +
  labs(
    title = "Distribution of REACH by time of the day",
    subtitle = "Made by LSMA",
    caption = "Source: Mediatoolkit dataset",
    x = "INTERDAY",
    y = "Reach (log scale)"
  ) +
  theme_classic() +
  theme(
    plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
    plot.subtitle = element_text(size = 10, face = "bold"),
    plot.caption = element_text(face = "italic")
  )
```




Also, lets see how the relationship looks

```{r}
ggplot(reg, aes(x = REACH, y = LIKE_COUNT)) +
  geom_point() +
  labs(x = "REACH", y = "LIKE_COUNT") + 
  geom_smooth(methot = "lm", se = FALSE)

ggplot(reg, aes(x = REACH, y = COMMENT_COUNT)) +
  geom_point() +
  labs(x = "REACH", y = "COMMENT_COUNT") + 
  geom_smooth(methot = "lm", se = FALSE)

ggplot(reg, aes(x = REACH, y = LIKE_COUNT)) +
  geom_point() +
  labs(x = "REACH", y = "LIKE_COUNT") + 
  geom_smooth(methot = "lm", se = FALSE)

```



Now we can fit a first regression model:


```{r}
# run the model1
model1 <- lm(REACH ~ COMMENT_COUNT, data = reg)
# output content
summary(model1)
# run the model2
model2 <- lm(REACH ~ LIKE_COUNT, data = reg)
# output content
summary(model2)
# run the model3
model3 <- lm(REACH ~ TITLE, data = reg)
# output content
summary(model3)
# run the model4
model4 <- lm(REACH ~ INTERDAYTIME, data = reg)
# output content
summary(model4)
```


Combine all variables together in a multiple regression model:

```{r}
# run the model5
model5 <- lm(REACH ~ LIKE_COUNT + COMMENT_COUNT + TITLE + INTERDAYTIME, 
                    data = reg)
# output content
summary(model5)
```


The procedure doesnt end with the model. We also need to choose the best model and asses its quality. The main criteria in regression model is R2.

```{r}
library(moderndive)
# Get fitted/values & residuals, compute R^2 using residuals for model1
get_regression_points(model1) %>%
  summarize(r_squared = 1 - var(residual) / var(REACH))
  
# Get fitted/values & residuals, compute R^2 using residuals for model2
get_regression_points(model2) %>%
  summarize(r_squared = 1 - var(residual) / var(REACH))

# Get fitted/values & residuals, compute R^2 using residuals for model3
get_regression_points(model3) %>%
  summarize(r_squared = 1 - var(residual) / var(REACH))

# Get fitted/values & residuals, compute R^2 using residuals for model4
get_regression_points(model4) %>%
  summarize(r_squared = 1 - var(residual) / var(REACH))

# Get fitted/values & residuals, compute R^2 using residuals for model5
get_regression_points(model5) %>%
  summarize(r_squared = 1 - var(residual) / var(REACH))
  
```









# Thank you for your attention!





















