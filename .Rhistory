YDT %>%
select(title, subscriberCount) %>%
arrange(desc(subscriberCount))
YDT %>%
select(title, videoCount) %>%
arrange(desc(videoCount))
YDT %>%
select(title, subscriberCount) %>%
arrange(desc(subscriberCount))
YDT %>% filter(title == "2CELLOS")
View(YDT)
write.csv2(YDT,"./data/YouTube/YDT.csv")
YDT <- rbindlist(lapply(raw_path, function(x)readLines(x, header=T, fill = T)))
proba <- lapply(raw_path, function(x)readLines(x))
proba <- rbindlist(lapply(raw_path, function(x)readLines(x)))
lapply(raw_path, function(x)readLines(x))
YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T, sep = "")))
# clean data
YDT <- YDT %>%
mutate(viewCount = as.numeric(viewCount),
subscriberCount = as.numeric(subscriberCount),
videoCount = as.numeric(videoCount),
publishedAt = anytime(publishedAt))
View(YDT)
YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T, sep = " ")))
View(YDT)
View(proba)
View(proba)
rm(proba)
proba <- rbindlist(lapply(raw_path, function(x)read_tsv(x)))
View(proba)
#YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T)))
YDT <- rbindlist(lapply(raw_path, function(x)read_tsv(x)))
str(YDT)
names(YDT)
head(YDT)
YDT %>%
select(title, subscriberCount) %>%
arrange(desc(subscriberCount))
#YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T)))
YDT <- rbindlist(lapply(raw_path, function(x)read_tsv(x))) %>% distinct()
YDT %>%
select(title, subscriberCount) %>%
arrange(desc(subscriberCount))
YDT %>%
select(title, viewCount) %>%
arrange(desc(viewCount))
YDT %>%
select(title, subscriberCount) %>%
arrange(desc(subscriberCount)) %>%
distinct(title)
#YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T)))
YDT <- rbindlist(lapply(raw_path, function(x)read_tsv(x))) %>% distinct(title)
YDT %>%
select(title, viewCount) %>%
arrange(desc(viewCount))
#YDT <- rbindlist(lapply(raw_path, function(x)read.table(x, header=T, fill = T)))
YDT <- rbindlist(lapply(raw_path, function(x)read_tsv(x))) %>% distinct(title,.keep_all = T)
YDT %>%
select(title, viewCount) %>%
arrange(desc(viewCount))
YDT %>%
select(title, videoCount) %>%
arrange(desc(videoCount))
View(YDT)
YDT %>% filter(title == "2CELLOS")
YDT %>%
select(title, videoCount) %>%
arrange(desc(publishedAt))
YDT %>%
select(title, publishedAt) %>%
arrange(desc(publishedAt))
YDT %>%
select(title, publishedAt) %>%
arrange(publishedAt)
YDT %>%
select(title, videoCount) %>%
arrange(desc(videoCount))
path <- "./data/YouTube/ChannelDta/"
raw <- list.files(path = path , pattern="podkastInkubator")
raw
raw_path <- paste0(path, "/", raw)
raw_path
PI <- read_tsv("./data/YouTube/ChannelDta//podkastInkubator.tab")
PI <- read_tsv("./data/YouTube/podkastInkubator.tab")
View(PI)
str(PI)
head(PI)
names(PI)
PI %>%
select(videoTitle, durationSec, viewCount) %>%
arrange(desc(viewCount))
PI %>%
select(videoTitle, durationSec, likeCount) %>%
arrange(desc(likeCount))
PI %>%
select(videoTitle, durationSec, dislikeCount) %>%
arrange(desc(dislikeCount))
PI %>%
select(videoTitle, durationSec, commentCount) %>%
arrange(desc(commentCount))
PI %>%
select(videoTitle, durationSec) %>%
arrange(desc(durationSec))
View(PI)
PI %>%
select(videoTitle, videoId, durationSec, commentCount) %>%
arrange(desc(commentCount))
# read in videos (Podcast incubator) from YOUTUBE DATA TOOLS
COMMENTSbasic <- read_tsv("./data/YouTube/comments_basicinfo.tab")
str(COMMENTSbasic)
View(COMMENTSbasic)
names(COMMENTSbasic)
head(COMMENTSbasic)
head(COMMENTSbasic,20)
str(COMMENTSauthors)
# authors
COMMENTSauthors <- read_tsv("./data/YouTube/comments_authors.tab")
str(COMMENTSauthors)
names(COMMENTSauthors)
head(COMMENTSauthors,20)
head(COMMENTSauthors,20)
# comments
COMMENTS <- read_tsv("./data/YouTube/comments_comments.tab")
str(COMMENTS)
View(COMMENTS)
names(COMMENTS)
head(COMMENTS,20)
names(COMMENTS)
COMMENTS %>%
select(authorName,text,likeCount) %>%
arrange(desc(likeCount))
names(COMMENTS)
# tokenize
COMMENTS %>%
unnest_tokens(word, text) -> COMMENTS_token
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8")  %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8") %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data
head(sample_n(Crosentilex_sve,1000),15)
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
Encoding(CroSentilex_Gold$word) <- "UTF-8"
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data
head(sample_n(CroSentilex_Gold,100),15)
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
word = c(
"jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez", "kod",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga",
"ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8")  %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8") %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data
head(sample_n(Crosentilex_sve,1000),15)
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
Encoding(CroSentilex_Gold$word) <- "UTF-8"
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data
head(sample_n(CroSentilex_Gold,100),15)
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
word = c(
"jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez", "kod",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga",
"ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
# remove stop words, numbers, single letters
COMMENTS_token %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> COMMENTS_tokenTidy
# remove NA
COMMENTS_tokenTidy %>%
filter(!is.na(word)) -> COMMENTS_tokenTidy
## Most common words
COMMENTS_tokenTidy[,.N,by = word][order(-N),]
COMMENTS_tokenTidy[,.N,by = word]
COMMENTS_tokenTidy
COMMENTS_tokenTidy <- as.data.table(COMMENTS_tokenTidy)
## Most common words
COMMENTS_tokenTidy[,.N,by = word][order(-N),]
## Vizualize most common words
fb_tokenTidy[,.N,by = word][N>50][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
## Vizualize most common words
COMMENTS_tokenTidy[,.N,by = word][N>50][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
## Vizualize most common words
COMMENTS_tokenTidy[,.N,by = word][N>60][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
```{r fig.height=12}
## Vizualize most common words
COMMENTS_tokenTidy[,.N,by = word][N>60][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
## ComparisonCloud
COMMENTS_tokenTidy %>%
inner_join(CroSentilex_Gold,by="word") %>%
count(word, sentiment) %>%
top_n(200) %>%
mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
sentiment == 1 ~ "-",
sentiment == 2 ~ "+")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
max.words = 120)
## ComparisonCloud
COMMENTS_tokenTidy %>%
inner_join(CroSentilex_Gold,by="word") %>%
count(word, sentiment) %>%
top_n(200) %>%
mutate(sentiment = case_when(sentiment == 0 ~ "+/-",
sentiment == 1 ~ "-",
sentiment == 2 ~ "+")) %>%
acast(word ~ sentiment, value.var = "n", fill = 0) %>%
comparison.cloud(colors = c("firebrick3", "deepskyblue3","darkslategray"),
max.words = 120)
COMMENTS_bigram <- COMMENTS %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
COMMENTS_bigram <- COMMENTS %>%
unnest_tokens(bigram, text, token = "ngrams", n = 2)
COMMENTS_bigram %>% head(10)
COMMENTS_bigram %>%
count(bigram, sort = T) %>%
head(15)
COMMENTS_bigram_sep <- COMMENTS_bigram %>%
separate(bigram, c("word1","word2"), sep = " ")
COMMENTS_bigram_tidy <- COMMENTS_bigram_sep %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
mutate(word1 = gsub("\\d+", NA, word1)) %>%
mutate(word2 = gsub("\\d+", NA, word2)) %>%
mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
mutate(word2 = gsub("^[a-zA-Z]$", NA, word2))
COMMENTS_bigram_tidy_bigram_counts <- COMMENTS_bigram_tidy %>%
count(word1, word2, sort = TRUE)
bigrams_united <- COMMENTS_bigram_tidy %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>%
count(FROM,bigram,sort = T) -> topicBigram
#bigrams_united
bigrams_united %>%
count(authorName,bigram,sort = T) -> topicBigram
bigrams_united %>%
count(bigram, sort = T) %>%
head(35)
head(COMMENTSauthors,20)
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(FROM, bigram) %>%
bind_tf_idf(bigram, FROM, n) %>%
arrange(desc(tf_idf))
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(FROM, bigram) %>%
bind_tf_idf(bigram, authorName, n) %>%
arrange(desc(tf_idf))
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(authorName, bigram) %>%
bind_tf_idf(bigram, authorName, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "Vedski Sturmovik" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "Vedski Sturmovik" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "Vedski Sturmovik" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~authorName, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "Vedski Sturmovik" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~authorName, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
```{r nGRAMI, message=F, warning=F, fig.height=15, fig.width=15}
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "Vedski Sturmovik" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~authorName, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva", "MLINKO HR" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~authorName, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(authorName %in% c("Zeljko F", "Jasna X", "Post i molitva")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(authorName) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = authorName)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~authorName, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
COMMENTS_tokenTidy %>%
count(authorName, word, sort = TRUE) %>%
cast_dtm(authorName, word,n) -> dtm
LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
terms <- LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
LDA_tidy <- tidy(fb_LDA, matrix = "beta")
LDA_tidy <- tidy(LDA, matrix = "beta")
terms <- LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
LDA <- LDA(dtm, k = 2,  control = list(seed = 1234))
LDA_tidy <- tidy(LDA, matrix = "beta")
terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
LDA_tidy <- tidy(LDA, matrix = "beta")
terms <- LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
