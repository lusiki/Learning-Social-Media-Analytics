# influencers by FOLLOWERS
fb[FOLLOWERS_COUNT == max(FOLLOWERS_COUNT), by=FROM]
# influencers by FOLLOWERS
fb[max(FOLLOWERS_COUNT),, by=FROM]
# influencers by FOLLOWERS
fb[max(FOLLOWERS_COUNT), by=FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][order(-FOLLOWERS)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][order(-FOLLOWERS),c(FOLLOWERS,FROM)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique()
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique() %>% printMD( big.mark=",")
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique()
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(FOLLOWERS,FROM)][order(-REACH),])# influencers by LIKE
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(FOLLOWERS,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(REACH,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique()
# youtube <- posts[SOURCE_TYPE == "youtube",]
#instagram <- posts[SOURCE_TYPE == "instagram",]
#rm(posts)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique()
fb[,.N,FROM][order(-N)]
# influencers by LIKE
fb[,LIKE := sum(LIKE), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique()
# influencers by LIKE
fb[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique()
# influencers by INTERACTIONS
fb[,LIKE := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique()
# influencers by COMMENT
fb[,INTERACTIONS := sum(COMMENT_COUNT), FROM][,.(COMMENT_COUNT,FROM)][order(-COMMENT_COUNT)] %>% unique()
# influencers by INTERACTIONS
fb[,INTERACTIONS := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique()
# influencers by COMMENT
fb[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique()
# influencers by SHARE
fb[,SHARE := sum(SHARE_COUNT), FROM][,.(SHARE,FROM)][order(-SHARE)] %>% unique()
# dim before tokenize
dim(fb)
# how many letters in a title
fb[,
.(Avg = mean(nchar(TITLE), na.rm = T),
STD = sd(nchar(TITLE), na.rm = T),
min = min(nchar(TITLE), na.rm = T),
max = max(nchar(TITLE), na.rm = T)),
SOURCE_TYPE][order(-Avg),]
# how many letters in a text
fb[,
.(Avg = mean(nchar(FULL_TEXT)),
STD = sd(nchar(FULL_TEXT)),
min = min(nchar(FULL_TEXT)),
max = max(nchar(FULL_TEXT))),
SOURCE_TYPE][order(-Avg),]
# youtube <- posts[SOURCE_TYPE == "youtube",]
#instagram <- posts[SOURCE_TYPE == "instagram",]
#rm(posts)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[,.(N := .N,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[,.(ACTIVITY := .N ,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[,(ACTIVITY := .N ,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][,.(FROM,ACTIVITY,LETTERS)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][,.(FROM,ACTIVITY,LETTERS)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-ACTIVITY)] %>% unique()
fb[FROM == Budicool,]
fb[FROM == "Budicool",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "Općina Lipovljani",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "Tinka Kalajzic",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "RCKTU Zabok",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[FROM == "Zoran Šprajc",]
# influencers by COMMENT
fb[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique()
fb[FROM == "Andrea Andrassy",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = avg(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
hist(fb,activity)
hist(fb,ACTIVITY)
hist(fb$ACTIVITY)
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[FROM == "Naklada Ljevak",]
fb[FROM == "Naklada Ljevak",][order(-DATETIME)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()
knitr::opts_chunk$set(echo = T, message = F, warning = F)
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel) %>% data.table()
knitr::opts_chunk$set(echo = T, message = F, warning = F)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
# select facebook
fb <- posts[SOURCE_TYPE == "facebook",]
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8")  %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8") %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data
head(sample_n(Crosentilex_sve,1000),15)
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
Encoding(CroSentilex_Gold$word) <- "UTF-8"
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data
head(sample_n(CroSentilex_Gold,100),15)
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
word = c(
"jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez", "kod",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga",
"ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
# dim before tokenize
dim(fb)
# tokenize
fb %>%
unnest_tokens(word, FULL_TEXT) -> fb_token
# dim after tokenize
dim(fb_token)
# check
fb_token %>%
select(FROM, word, MENTION_SNIPPET ) %>%
sample_n(.,10)
# remove stop words, numbers, single letters
fb_token %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> fb_tokenTidy
# remove NA
fb_tokenTidy %>%
filter(!is.na(word)) -> fb_tokenTidy
# check
fb_tokenTidy  %>%
select(FROM, word, MENTION_SNIPPET ) %>%
sample_n(.,15)
# Select postes with +5k likes
fb[order(-LIKE_COUNT)][LIKE_COUNT >= 1000] -> fb_TopLike
# check
dim(fb_TopLike)
# influencer activity
fb_TopLike[,.N,FROM][order(-N)]
# influencer by like
fb_TopLike[,.(FROM, LIKE_COUNT)][,LIKES := sum(LIKE_COUNT),FROM][,.(FROM,LIKES)][order(-LIKES)] %>% unique()
# tokenize
fb_TopLike %>%
unnest_tokens(word, FULL_TEXT) -> fb_token_TopLike
# dim after tokenize
dim(fb_token_TopLike)
# remove stop words, numbers, single letters
fb_token_TopLike %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> fb_tokenTidy_TopLike
# remove NA
fb_tokenTidy_TopLike %>%
filter(!is.na(word)) -> fb_tokenTidy_TopLike
## most common words
fb_tokenTidy_TopLike[,.N,by = word][order(-N),]
## word cloud
fb_tokenTidy_TopLike %>%
anti_join(CroSentilex_Gold,by="word") %>%
count(word) %>%
arrange(desc(n)) %>%
top_n(100) %>%
with(wordcloud(word, n, max.words = 80))
## Udio riječi po domenama
domenaWords <- fb_tokenTidy_TopLike %>%
filter(FROM %in% c("24sata", "Hajdučke vijesti", "Ivan Vilibor Sinčić", "Index.hr" )) %>%
count(FROM, word, sort = T)
ukupnoWords <- domenaWords %>%
group_by(FROM) %>%
summarise(totWords = sum(n))
domenaWords <- left_join(domenaWords, ukupnoWords)
# domenaWords %>% head(15)
# domenaWords %>%
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
bind_tf_idf(word, FROM, n)
# domenaWords %>% head(15)
# domenaWords %>%
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
bind_tf_idf(word, FROM, n)
idf %>% head(10)
# idf %>%
#   select(-totWords) %>%
#   arrange(desc(tf_idf))
idf %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(FROM = factor(FROM)) %>%
group_by(FROM) %>%
top_n(11) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
fb_bigram <- fb_TopLike %>%
unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
fb_bigram <- fb_TopLike %>%
unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
fb_bigram %>% head(10)
fb_bigram <- fb_TopLike %>%
unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
fb_bigram %>% head(10)
fb_bigram %>%
count(bigram, sort = T) %>%
head(15)
fb_bigram %>%
count(bigram, sort = T) %>%
head(15)
fb_bigram_sep <- fb_bigram %>%
separate(bigram, c("word1","word2"), sep = " ")
fb_bigram %>%
count(bigram, sort = T) %>%
head(15)
fb_bigram_sep <- fb_bigram %>%
separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
mutate(word1 = gsub("\\d+", NA, word1)) %>%
mutate(word2 = gsub("\\d+", NA, word2)) %>%
mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
mutate(word2 = gsub("^[a-zA-Z]$", NA, word2))
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>%
count(word1, word2, sort = TRUE)
bigrams_united <- fb_bigram_tidy %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>%
count(FROM,bigram,sort = T) -> topicBigram
#bigrams_united
bigrams_united %>%
count(FROM,bigram,sort = T) -> topicBigram
bigrams_united %>%
count(bigram, sort = T) %>%
head(25)
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(FROM, bigram) %>%
bind_tf_idf(bigram, FROM, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
filter(FROM %in% c("Ivan Vilibor Sinčić", "Zoran Šprajc", "Velimir Bujanec", "Kristijan Iličić" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(10) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("Nikola Grmoja", "Mirko CroCop Filipovic", "Andrej Plenković", "Marko Perković Thompson" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
fb_tokenTidy %>%
count(FROM, word, sort = TRUE) %>%
cast_dtm(FROM, word,n) -> dtm
fb_tokenTidy_TopLike %>%
count(FROM, word, sort = TRUE)
fb_tokenTidy_TopLike %>%
count(FROM, word, sort = TRUE) %>%
cast_dtm(FROM, word,n) -> dtm
insta_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
insta_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
fb_LDA <- LDA(dtm, k = 4,  control = list(seed = 1234))
fb_LDA_tidy <- tidy(fb_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(15, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
# Bigrami
topicBigram %>%
cast_dtm(FROM, bigram,n) -> dtmB
insta_LDA <- LDA(dtmB, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#newsCOVID_terms
insta_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
rm(list=ls())
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
View(all_raw)
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
# select facebook
fb <- posts[SOURCE_TYPE == "facebook",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique()
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique()
# influencers by REACH II
fb[, `:=` (ACTIVITY = .N ,REACH = sum(REACH),EFFECT = REACH/ACTIVITY), FROM][,.(FROM,ACTIVITY,REACH, EFFECT)][ACTIVITY >= 100][order(-EFFECT)] %>% unique()
# influencers by LIKE
fb[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique()
# influencers by LIKE II
fb[, `:=` (ACTIVITY = .N ,LIKE = sum(LIKE_COUNT),EFFICIENCY = LIKE/ACTIVITY), FROM][,.(FROM,ACTIVITY,LIKE, EFFICIENCY)][ACTIVITY >= 100][order(-EFFICIENCY)] %>% unique()
# influencers by LIKE II
fb[,ACTIVITY := .N,FROM]
# influencers by LIKE II
fb <- fb[,ACTIVITY := .N,FROM]
fb
View(fb)
# influencers by LIKE II
fb <- fb[,.(ACTIVITY := .N),FROM]
# influencers by LIKE II
fb <- fb[,.(ACTIVITY = .N),FROM]
# select facebook
fb <- posts[SOURCE_TYPE == "facebook",]
# influencers by LIKE II
fb[,.(ACTIVITY = .N),FROM]
# influencers by LIKE II
fb[,`:=` (ACTIVITY = .N),FROM]
fb
fb[, `:=` (LIKE = sum(LIKE_COUNT),EFFICIENCY = LIKE/ACTIVITY), FROM][,.(FROM,ACTIVITY,LIKE, EFFICIENCY)][ACTIVITY >= 100][order(-EFFICIENCY)] %>% unique()
fb[, `:=` (LIKE = sum(LIKE_COUNT) ,EFFICIENCY = LIKE/ACTIVITY), FROM]
fb[,`:=` (LIKE = sum(LIKE_COUNT) , FFICIENCY = LIKE/ACTIVITY), by= FROM]
fb[, `:=` (LIKE = sum(LIKE_COUNT) , FFICIENCY = LIKE/ACTIVITY), by = FROM]
fb[, LIKE := sum(LIKE_COUNT), by = FROM]
fb[, FFICIENCY = LIKE/ACTIVITY, by = FROM]
fb[, FFICIENCY = LIKE / ACTIVITY, by = FROM]
fb[, FFICIENCY := LIKE / ACTIVITY, by = FROM]
fb[, FFICIENCY := LIKE / ACTIVITY, by = FROM][,.(FROM,ACTIVITY,LIKE, EFFICIENCY)][ACTIVITY >= 100][order(-EFFICIENCY)] %>% unique()
fb[, EFICIENCY := LIKE / ACTIVITY, by = FROM][,.(FROM,ACTIVITY,LIKE, EFFICIENCY)][ACTIVITY >= 100][order(-EFFICIENCY)] %>% unique()
