#   count(bigram, sort = T) %>%
#   head(25)
fb_bigram_sep <- fb_bigram %>%
separate(bigram, c("word1","word2"), sep = " ")
fb_bigram_tidy <- fb_bigram_sep %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
mutate(word1 = gsub("\\d+", NA, word1)) %>%
mutate(word2 = gsub("\\d+", NA, word2)) %>%
mutate(word1 = gsub("^[a-zA-Z]$", NA, word1)) %>%
mutate(word2 = gsub("^[a-zA-Z]$", NA, word2))
fb_bigram_tidy_bigram_counts <- fb_bigram_tidy %>%
count(word1, word2, sort = TRUE)
bigrams_united <- fb_bigram_tidy %>%
unite(bigram, word1, word2, sep = " ") %>%
filter(., !grepl("NA",bigram))
#bigrams_united
bigrams_united %>%
count(FROM,bigram,sort = T) -> topicBigram
bigrams_united %>%
count(bigram, sort = T) %>%
filter(n>10) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(FROM, bigram) %>%
bind_tf_idf(bigram, FROM, n) %>%
arrange(desc(tf_idf))
# Najvažniji bigrami po domenama
bigram_tf_idf <- bigrams_united %>%
#  filter (!is.na(bigram)) %>%
count(FROM, bigram) %>%
bind_tf_idf(bigram, FROM, n) %>%
arrange(desc(tf_idf))
bigram_tf_idf %>%
arrange(desc(tf_idf)) %>%
filter(tf_idf > 0.09) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
#  group_by(FROM) %>%
#  top_n(20) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
#  facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
arrange(desc(tf_idf)) %>%
filter(tf_idf > 0.09)
bigram_tf_idf %>%
filter(FROM %in% c("vecenji.hr", "jutarnji.hr", "24sata.hr", "telegram.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(10)
bigram_tf_idf %>%
filter(FROM %in% c("vecenji.hr", "jutarnji.hr", "24sata.hr", "telegram.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("vecenji.hr", "jutarnji.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("jutarnji.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("vecenji.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("vecenji.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
bigram_tf_idf %>%
filter(FROM %in% c("telegram.hr")) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(5) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
n_tokenTidy %>%
#  filter(datum > "2020-02-20") %>%
group_by(word) %>%
filter(n() > 20) %>%
filter(!is.na(word)) %>%
pairwise_cor(word,DATE, sort = T) -> corsWords
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
filter(item1 %in% c("bolest", "rizik", "strah")) %>%
group_by(item1) %>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip() +
theme_economist()
industry_list
industry_list %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
ggplot(data = dta, aes(x = PREMIUM.ČLANAK)) +
ggplot(data = dta, aes(x = PREMIUM.ČLANAK)) +
dta
ggplot(data = dta, aes(x = PREMIUM.ČLANAK)) +
l
ggplot(data = dta, aes(x = HRV.IL.STRANI.BREND)) +
geom_histogram(binwidth = 0.5, fill = "blue", alpha = 0.7) +
ggtitle("Porijeklo") +
xlab("Industrija") +
ylab("Frequency") +
scale_x_continuous(breaks = seq(from = min(dta$HRV.IL.STRANI.BREND), to =  max(dta$HRV.IL.STRANI.BREND), by = 1))
ggplot(data = dta, aes(x = PREMIUM.ČLANAK )) +
ggplot(data = dta, aes(x = `PREMIUM.ČLANAK` )) +
dta$PREMIUM.ČLANAK
dta %>%
group_by(NAZIV.BRENDA) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
ggplot(data = dta, aes(x = dta$PREMIUM.ČLANAK )) +
ggplot(data = dta, aes(x = dta$PREMIUM.ČLANAK)) +
autorstvo %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
dta %>%
group_by(AUTORSTVO) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
left_join(autorstvo, by = c("AUTORSTVO" = "number")) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
dta %>%
group_by(AUTORSTVO) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
left_join(autorstvo, by = c("AUTORSTVO" = "number")) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
autorstvo %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
dta %>%
group_by(AUTORSTVO) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
left_join(autorstvo, by = c("AUTORSTVO" = "number")) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
rubrika <- data.frame(
number = c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18),
text = c(
"Native",
"Partneri",
"Promo",
"Vijesti",
"Vanjska politika",
"Unutarnja politika",
"Magazin / mozaik / zabava",
"Financije / novac / biznis / poduzetnički savjetnik",
"Dom i dizajn",
"Lifestyle / estrada / celebrity / scena / showbiz",
"Auto-moto",
"Sport",
"Tehnologija / znanost (tech-sci)",
"Putovanja",
"Prehrana / gastro / dobra hrana",
"Lokalna rubrika (Zagreb, Dalmacija)",
"Zdravlje, zdrav život",
"Openspace (Telegram.hr)"
)
)
dta %>%
group_by(RUBRIKA) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
left_join(rubrika, by = c("RUBRIKA" = "number")) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
rubr <- dta %>%
group_by(RUBRIKA) %>%
summarise(BrojObjava = n()) %>%
arrange(desc(BrojObjava)) %>%
left_join(rubrika, by = c("RUBRIKA" = "number"))
rubr
ggplot(data = dta, aes(x = RUBRIKA)) +
geom_histogram(binwidth = 1, fill = "blue", alpha = 0.7) +
ggtitle("Rubrika") +
xlab("Rubrika") +
ylab("Frequency") +
scale_x_continuous(breaks = seq(from = min(dta$RUBRIKA), to =  max(dta$RUBRIKA), by = 1))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dt <- dt %>% filter(SOURCE_TYPE == "web") %>%
distinct(URL,.keep_all = TRUE)
generalno <- c("crkva", "biskup", "Kaptol", "časna sestra", "svećenik", "župnik", "vjernik", "kardinal", "papa", "sveti otac", "redovnik", "redovnica","kršćanstvo", "vjera", "Gospa", "Bog", "Isus", "katolički", "misa", "pričest", "krizma", "grijeh", "vjeroučitelj", "vjeronauk", "blagoslov","svjedočanstvo", "relikvija", "stigma", "duhovna zvanja", "don", "fra", "velečasni","zaređenje", "krunica", "vjeronauk", "ukazanje")
katoliq <- dt %>%
distinct(URL,.keep_all = TRUE) %>%
slice(1:10000)
batch_size <- 1000
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
contains_string <- function(text) {
return(any(str_detect(text, fixed(generalno, ignore_case = TRUE))))
}
# Process each batch
results <- purrr::map(batches, function(batch) {
batch %>%
rowwise() %>%
mutate(matches = contains_string(generalno))
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
View(df_result)
df_result %>% filter(matches == TRUE)
# Process each batch
results <- purrr::map(batches, function(batch) {
batch %>%
filter(contains_string(generalno))
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
df_result %>% filter(matches == TRUE)
# Define a function to check if any string from string_vector is contained in a text
contains_string <- function(text) {
return(any(str_detect(text, fixed(generalno, ignore_case = TRUE))))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 100
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, function(batch) {
batch_matches <- sapply(batch$FULL_TEXT, contains_string)
return(data.frame(text_column = batch$FULL_TEXT, matches = batch_matches))
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
View(df_result)
# Define a function to check if any string from string_vector is contained in a text
# Define a function to return matched words from string_vector in a text
matched_words <- function(text) {
matched <- str_subset(generalno, fixed(text, ignore_case = TRUE))
return(ifelse(length(matched) > 0, paste(matched, collapse = ", "), NA))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 100
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
results <- purrr::map(batches, function(batch) {
batch_matched_words <- sapply(batch$FULL_TEXT, matched_words)
return(data.frame(text_column = batch$FULL_TEXT, matched_words = batch_matched_words))
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
df_result
# Define a function to check if any string from string_vector is contained in a text
# Define a function to return matched words from string_vector in a text
matched_words <- function(text) {
matched <- str_subset(generalno, fixed(text, ignore_case = TRUE))
return(ifelse(length(matched) > 0, paste(matched, collapse = ", "), NA))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 100
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
results <- purrr::map(batches, function(batch) {
batch_matched_words <- sapply(batch$FULL_TEXT, matched_words)
return(data.frame(text_column = batch$FULL_TEXT, matched_words = batch_matched_words))
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
View(df_result)
# Define a function to check if any string from string_vector is contained in a text
# Define a function to return matched words from string_vector in a text
# Define a function to return matched words from string_vector in a text
matched_words_and_bool <- function(text) {
matched <- str_subset(generalno, fixed(text, ignore_case = TRUE))
matched_str <- ifelse(length(matched) > 0, paste(matched, collapse = ", "), NA)
return(list(matches = length(matched) > 0, matched_words = matched_str))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 100
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, function(batch) {
batch_results <- lapply(batch$FULL_TEXT, matched_words_and_bool)
batch_df <- data.frame(
text_column = batch$FULL_TEXT,
matches = sapply(batch_results, `[[`, "matches"),
matched_words = sapply(batch_results, `[[`, "matched_words"),
stringsAsFactors = FALSE
)
return(batch_df)
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
df_result <- df_result %>% filter (matches == T)
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 1000
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Define a function to check if any string from string_vector is contained in a text
# Define a function to return matched words from string_vector in a text
# Define a function to return matched words from string_vector in a text
matched_words_and_bool <- function(text) {
matched <- str_subset(generalno, fixed(text, ignore_case = TRUE))
matched_str <- ifelse(length(matched) > 0, paste(matched, collapse = ", "), NA)
return(list(matches = length(matched) > 0, matched_words = matched_str))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 1000
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, function(batch) {
batch_results <- lapply(batch$FULL_TEXT, matched_words_and_bool)
batch_df <- data.frame(
text_column = batch$FULL_TEXT,
matches = sapply(batch_results, `[[`, "matches"),
matched_words = sapply(batch_results, `[[`, "matched_words"),
stringsAsFactors = FALSE
)
return(batch_df)
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
df_result <- df_result %>% filter (matches == T)
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 1000
batches <- split(dt, ceiling(seq_along(dt$FULL_TEXT)/batch_size))
dta <- katoliq %>%
# filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(FULL_TEXT,fixed("crkva",ignore_case = TRUE)))
dta <- katoliq %>%
# filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(FULL_TEXT,fixed("crkva",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
# filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(FULL_TEXT,fixed("crkva",ignore_case = TRUE)))
# Define a function to check if any string from string_vector is contained in a text
# Define a function to return matched words from string_vector in a text
# Define a function to return matched words from string_vector in a text
matched_words_and_bool <- function(text) {
matched <- str_subset(generalno, fixed(text, ignore_case = TRUE))
matched_str <- ifelse(length(matched) > 0, paste(matched, collapse = ", "), NA)
return(list(matches = length(matched) > 0, matched_words = matched_str))
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 100)
batch_size <- 1000
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, function(batch) {
batch_results <- lapply(batch$FULL_TEXT, matched_words_and_bool)
batch_df <- data.frame(
text_column = batch$FULL_TEXT,
matches = sapply(batch_results, `[[`, "matches"),
matched_words = sapply(batch_results, `[[`, "matched_words"),
stringsAsFactors = FALSE
)
return(batch_df)
})
# Recombine the results into a single data frame
df_result <- bind_rows(results)
df_result <- df_result %>% filter (matches == T)
process_batch <- function(batch) {
result <- batch %>%
filter(if_any(everything(), ~ grepl(paste(generalno, collapse = "|"), .)))
return(result)
}
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 1000)
batch_size <- 1000
batches <- split(katoliq, ceiling(seq_along(katoliq$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, process_batch)
# Recombine the results into a single data frame
filtered_df <- bind_rows(results)
View(filtered_df)
# Split the data frame into batches
# Assume batch_size is the number of rows per batch (e.g., 1000)
batch_size <- 1000
batches <- split(dt, ceiling(seq_along(dt$FULL_TEXT)/batch_size))
# Process each batch
results <- purrr::map(batches, process_batch)
# Recombine the results into a single data frame
filtered_df <- bind_rows(results)
knitr::opts_chunk$set(echo = TRUE)
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1)
variables <- read.xlsx("C:/Users/Lukas/OneDrive/Desktop/Native.xlsx", sheetIndex = 1)
variables <- variables[,-c(23,24,25)] %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
dta <- merge(original, variables, by = "V1", all.x = TRUE)
dta$DATE <- as.Date(dta$DATE)
# articles over time
daily_counts <- dta %>%
group_by(DATE) %>%
summarise(count = n())
# descriptives
summ <- daily_counts %>%
summarize(min = min(count), max = max(count),
mean = mean(count), q1= quantile(count, probs = 0.25),
median = median(count), q3= quantile(count, probs = 0.75),
sd = sd(count)) %>%
mutate_if(is.numeric, round, digits=2)
summ
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1)
variables <- read.xlsx("C:/Users/Lukas/OneDrive/Desktop/Native.xlsx", sheetIndex = 1)
variables <- variables[,-c(23,24,25)] %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1)
variables <- read.xlsx("C:/Users/Lukas/OneDrive/Desktop/Native.xlsx", sheetIndex = 1)
