pairedSamplesTTest(
formula = ~ reach100$N + like100$N # one-sided formula listing the two variables
)
reach100$N
like100$N
reach100$N
pairedSamplesTTest(
formula = ~ reach100$N + like100$N # one-sided formula listing the two variables
)
pairedSamplesTTest(
formula =  reach100$N ~ like100$N # one-sided formula listing the two variables
)
t.test(
x = reach100$N, # variable 1 is the "test2" scores
y = clike100$N, # variable 2 is the "test1" scores
paired = TRUE # paired test
)
t.test(
x = reach100$N, # variable 1 is the "test2" scores
y = like100$N, # variable 2 is the "test1" scores
paired = TRUE # paired test
)
webRanking[order(-N),.(FROM,N)]
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)][slice(100:200),]
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
l
# filter 100 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
View(portals200)
View(portals100)
View(portals200)
# filter 100 biggest by criteria
portals100 <-  webRanking[head(order(-N),100),.(FROM,N)]
View(portals100)
View(portals200)
t.test(
x = portals100$N,
y = portals200$N,
paired = FALSE # paired test
)
t.test(
x = portals100$N,
y = portals200$N,
var.equal = TRUE, # Student test
paired = FALSE # paired test
)
t.test(
x = reach100$N,
y = reach200$N,
var.equal = FALSE, # Welch test
paired = FALSE # paired test
)
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[head(order(-LIKE),100),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[head(order(-COMMENT),100),.(FROM,N, COMMENT)] %>% slice(101:201)
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
mean(reach100$N, na.rm = TRUE)
mean(reach200$N, na.rm = TRUE)
sd(reach100$N, na.rm = TRUE)
sd(reach200$N, na.rm = TRUE)
l
l
l
l
l
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[order(-REACH),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[order(-LIKE),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[order(-COMMENT),.(FROM,N, COMMENT)] %>% slice(101:201)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
# then run the test
t.test(
x = reach100$N,
y = reach200$N,
var.equal = TRUE, # Student test
paired = FALSE # independent test
)
# then run the test
t.test(
x = portals100$N,
y = portals200$N,
var.equal = FALSE, # Welch test
paired = FALSE # independent test
)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
# then run the test
t.test(
x = reach100$N,
y = reach200$N,
var.equal = TRUE, # Student test
paired = FALSE # independent test
)
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)
# then run the test
t.test(
x = portals100$N,
y = portals200$N,
var.equal = FALSE, # Welch test
paired = FALSE # independent test
)
names(web)
unique(all_raw$SOURCE_TYPE)
tabulate(all_raw$SOURCE_TYPE)
table(all_raw$SOURCE_TYPE)
# run the GOF test
goodnessOfFitTest( all_raw$SOURCE_TYPE )
# run the GOF test
goodnessOfFitTest(as.factor(all_raw$SOURCE_TYPE))
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", ]
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", .SOURCE_TYPE]
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", .(SOURCE_TYPE)]
all_raw[SOURCE_TYPE == "web" .(SOURCE_TYPE)]
all_raw[SOURCE_TYPE == "web" ,]
all_raw[SOURCE_TYPE == "web&facebook" ,]
all_raw[, Network := ifelse(SOURCE_TYPE != "web", "other")]
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")]
all_raw
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
table(all_raw$Network)
# specify the probablities
probs = c(other = 0.6, web = 0.4)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.55, web = 0.45)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.65, web = 0.35)
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.60, web = 0.40)
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
names(all_raw)
uniqe(all_raw$LOCATIONS)
uniqeu(all_raw$LOCATIONS)
unique(all_raw$LOCATIONS)
View(all_raw)
names(all_raw)
all_raw$DATE
all_raw$TIME
all_raw$DATE
all_raw$TIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%m-%d %H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%d-%m%H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%d-%m %H%M%S")]
all_raw$DTIME
View(all_raw$DATE)
paste(all_raw$DATE,all_raw$TIME)
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y%m%d %H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%m-%d %H:%M:%S")]
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
library(anytime)
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
all_raw$DTIME
str(all_raw)
mydateseq<-seq(as.POSIXct("2016-01-01"), by="2 hour", length.out = 20)
mydateseq
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
library(lubridate)
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
all_raw$DTIME
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Night", "Morning", "Afternoon", "Evening")
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)]
all_raw$INTERDAYTIME
table(all_raw$INTERDAYTIME)
# check the activity per network
table(all_raw$SOURCE_TYPE)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)
# we are interested in cross-tabulation
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)
include.lowest=TRUE)]
# check the result
table(all_raw$INTERDAYTIME)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)
# we are interested in cross-tabulated View
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor()]
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)]
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)][
,SOURCE_TYPE := as.factor(SOURCE_TYPE)
]
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
# another way to run the test
chisq.test(all_raw$Network, p = probs)
all_raw$Network
# another way to run the test
chisq.test(all_raw$Network, p = c(other = 0.60, web = 0.40))
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# another way to do the test
chisq.test(as.factor(all_raw$SOURCE_TYPE))
# check the activity per network
table(all_raw$SOURCE_TYPE)
# another way to do the test
chisq.test(as.factor(table(all_raw$SOURCE_TYPE)))
all_raw$SOURCE_TYPE
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT)]
anova
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT, REACH)]
anova
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, REACH)]
anova
anova
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT),COMMENT = mean(COMMENT_COUNT), REACH = mean(REACH)),INTERDAYTIME]
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),COMMENT = mean(COMMENT_COUNT), REACH = mean(REACH)),INTERDAYTIME]
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),
COMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
REACH = mean(REACH, na.rm = TRUE)),
INTERDAYTIME]
anova
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),
COMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
REACH = mean(REACH, na.rm = TRUE)),
INTERDAYTIME]
aov( formula = LIKE ~ INTERDAYTIME, data = anova )
aov( formula = LIKE_COUNT ~ INTERDAYTIME, data = anova )
summary(aov( formula = LIKE_COUNT ~ INTERDAYTIME, data = anova))
anova
# test for reach
summary(aov( formula = REACH_COUNT ~ INTERDAYTIME, data = anova))
# test for reach
summary(aov( formula = REACH ~ INTERDAYTIME, data = anova))
knitr::opts_chunk$set(echo = T, message = F, warning = F)
cor(REACH,LIKE_COUNT)
cor(reg$REACH,reg$LIKE_COUNT)
model_score_2 <- lm(REACH ~ COMMENT_COUNT, data = reg)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
# FULL SAMPLE
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel) %>% data.table()
#all_raw <- all_raw  %>%  mutate(DATE = as.Date(DATE,"%Y-%m-%d" ))
library(anytime)
library(lubridate)
# convert chr date to date format
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Night", "Morning", "Afternoon", "Evening")
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)][
,SOURCE_TYPE := as.factor(SOURCE_TYPE)
]
# run the model
model_score_2 <- lm(REACH ~ COMMENT_COUNT, data = reg)
# select relevant data
reg <- all_raw[SOURCE_TYPE == "web",.(REACH,INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, TITLE, .N)]
# run the model
model_score_2 <- lm(REACH ~ COMMENT_COUNT, data = reg)
# output content
summary(model_score_2)
# run the model2
model2 <- lm(REACH ~ LIKE_COUNT, data = reg)
# output content
summary(model2)
model3 <- lm(REACH ~ INTRADAYTIME, data = reg)
# output content
summary(model3)
# run the model3
model3 <- lm(REACH ~ INTERDAYTIME, data = reg)
# output content
summary(model3)
# select relevant data
reg <- all_raw[SOURCE_TYPE == "web",.(REACH,INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, TITLE, .N)][,TITLE := nchar(TITLE)]
# check descriptives
reg[,.(meanREACH = mean(REACH, na.rm = TRUE),
meanLIKE = mean(LIKE_COUNT, na.rm = TRUE),
meanCOMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
No = mean(N),
meanTIT = mean(TITLE), na.rm = TRUE)
)]
# check descriptives
reg[,.(meanREACH = mean(REACH, na.rm = TRUE),
meanLIKE = mean(LIKE_COUNT, na.rm = TRUE),
meanCOMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
No = mean(N),
meanTIT = mean(TITLE, na.rm = TRUE)
)]
# run the model3
model3 <- lm(REACH ~ TITLE, data = reg)
# output content
summary(model3)
ggplot(reg, aes(x = INTERDAYTIME, y = REACH)) +
geom_boxplot() +
labs(x = "INTERDAYTIME", y = "REACH")
ggplot(reg, aes(x = INTERDAYTIME, y = log(REACH)) +
ggplot(reg, aes(x = INTERDAYTIME, y = log(REACH))) +
geom_boxplot() +
labs(x = "INTERDAYTIME", y = "REACH")
ggplot(reg, aes(x = INTERDAYTIME, y = log(REACH))) +
geom_boxplot() +
labs(x = "INTERDAYTIME", y = "REACH")
ggplot(reg, aes(x = INTERDAYTIME, y = log(REACH))) +
geom_boxplot() +
labs(x = "INTERDAYTIME", y = "REACH") +
labs(
title = "Distribution of REACH by time of the day",
subtitle = "Made by LSMA",
caption = "Source: Mediatoolkit dataset",
x = "INTERDAY",
y = "Reach (log scale)"
) +
theme_classic() +
theme(
plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
plot.subtitle = element_text(size = 10, face = "bold"),
plot.caption = element_text(face = "italic")
)
# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model1) %>%
summarize(r_squared = 1 - var(residual) / var(log(REACH)))
install.packages("moderndive")
library(moderndive)
# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model1) %>%
summarize(r_squared = 1 - var(residual) / var(log(REACH)))
run the model1
model1 <- lm(REACH ~ COMMENT_COUNT, data = reg)
# output content
summary(model1)
# run the model2
model2 <- lm(REACH ~ LIKE_COUNT, data = reg)
# output content
summary(model2)
# run the model3
model3 <- lm(REACH ~ TITLE, data = reg)
# output content
summary(model3)
# run the model4
model4 <- lm(REACH ~ INTERDAYTIME, data = reg)
# output content
summary(model4)
# run the model5
model5 <- lm(log(REACH) ~ log(LIKE_COUNT) + log(COMMENT_COUNT) + TITLE + INTERDAYTIME,
data = reg)
# run the model5
model5 <- lm(log(REACH) ~ log(LIKE_COUNT) + log(COMMENT_COUNT) + TITLE + INTERDAYTIME,
data = reg)
# run the model5
model5 <- lm(REACH ~ LIKE_COUNT + COMMENT_COUNT + TITLE + INTERDAYTIME,
data = reg)
# Get fitted/values & residuals, compute R^2 using residuals
get_regression_points(model1) %>%
summarize(r_squared = 1 - var(residual) / var(log(REACH)))
# Get fitted/values & residuals, compute R^2 using residuals for model1
get_regression_points(model1) %>%
summarize(r_squared = 1 - var(residual) / var(REACH))
# Get fitted/values & residuals, compute R^2 using residuals for model1
get_regression_points(model2) %>%
summarize(r_squared = 1 - var(residual) / var(REACH))
knitr::opts_chunk$set(echo = T, message = F, warning = F)
library(dplyr)     # for data manipulation
library(ggplot2)   # for awesome graphics
# Modeling process packages
library(rsample)   # for resampling procedures
library(caret)     # for resampling and model training
library(h2o)       # for resampling and model training
# h2o set-up
h2o.no_progress()  # turn off h2o progress bars
h2o.init()         # launch h2o
ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)
ames.h2o <- as.h2o(ames)
# Job attrition data
churn <- rsample::attrition %>%
mutate_if(is.ordered, .funs = factor, ordered = FALSE)
attrition
rsample::attrition
# Using base R
set.seed(123)  # for reproducibility
index_1 <- sample(1:nrow(ames), round(nrow(ames) * 0.7))
train_1 <- ames[index_1, ]
test_1  <- ames[-index_1, ]
index_2 <- createDataPartition(ames$Sale_Price, p = 0.7,
list = FALSE)
train_2 <- ames[index_2, ]
test_2  <- ames[-index_2, ]
# Using rsample package
set.seed(123)  # for reproducibility
split_1  <- initial_split(ames, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)
# Using h2o package
split_2 <- h2o.splitFrame(ames.h2o, ratios = 0.7,
seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
set.seed(123)
split <- initial_split(ames, prop = 0.7,
strata = "Sale_Price")
ames_train  <- training(split)
ames_test   <- testing(split)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
# Example using h2o
h2o.cv <- h2o.glm(
x = x,
y = y,
training_frame = ames.h2o,
nfolds = 10  # perform 10-fold CV
)
bootstraps(ames, times = 10)
sum(is.na(AmesHousing::ames_raw))
mesHousing::ames_raw %>%
is.na() %>%
reshape2::melt() %>%
ggplot(aes(Var2, Var1, fill=value)) +
geom_raster() +
coord_flip() +
scale_y_continuous(NULL, expand = c(0, 0)) +
scale_fill_grey(name = "",
labels = c("Present",
"Missing")) +
xlab("Observation") +
theme(axis.text.y  = element_text(size = 4))
AmesHousing::ames_raw %>%
is.na() %>%
reshape2::melt() %>%
ggplot(aes(Var2, Var1, fill=value)) +
geom_raster() +
coord_flip() +
scale_y_continuous(NULL, expand = c(0, 0)) +
scale_fill_grey(name = "",
labels = c("Present",
"Missing")) +
xlab("Observation") +
theme(axis.text.y  = element_text(size = 4))
caret::nearZeroVar(ames_train, saveMetrics = TRUE) %>%
tibble::rownames_to_column() %>%
filter(nzv)
