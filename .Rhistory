library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
knitr::opts_chunk$set(echo = T, message = F, warning = F)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
install.packages("reportMD")
yt <- read.csv2("C:/Users/Lukas/OneDrive/Desktop/yt.csv")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
knitr::opts_chunk$set(echo = TRUE)
# date range
range(dta$DATE)
# date range
range(dt$DATE)
# read data
dta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# select web
dta <- dta %>%
filter(SOURCE_TYPE == "web")
# Portals by activity
activity <- dta %>%
group_by(FROM) %>%
summarise(count = n()) %>%
mutate(percent = round(count / sum(count) * 100,2)) %>%
arrange(desc(count))
datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))
prigorski <- dt %>%
filter(FROM == "prigorski.hr" )
prigorski <- dt %>%
filter(FROM == "prigorski.hr" ) %>%
distinct()
write.csv2(prigorski, file = "D:/LUKA/Freelance/Mediatoolkit/prigorski.csv", row.names = T)
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,"sadržaj nastao") |
str_detect(AUTHOR,fixed("sponzor",ignore_case = TRUE)))
# Authors by activity
authors <- dta %>%
group_by(AUTHOR) %>%
summarise(count = n()) %>%
mutate(percent = round(count / sum(count) * 100,2)) %>%
arrange(desc(count))
datatable(authors, options = list(scrollX = TRUE, scrollY = "500px"))
View(authors)
View(dta)
write.xlsx(authors, file = "D:/LUKA/Freelance/Mediatoolkit/native.csv", row.names = T)
library(readxl)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
write.xlsx(authors, file = "D:/LUKA/Freelance/Mediatoolkit/native.csv", row.names = T)
library(xlsx)
install.packages("xlsx")
library(xlsx)
write.xlsx(authors, file = "D:/LUKA/Freelance/Mediatoolkit/native.csv", row.names = T)
write.xlsx(authors, file = "D:/LUKA/Freelance/Mediatoolkit/native.xlsx", row.names = T)
write.xlsx(dta, file = "D:/LUKA/Freelance/Mediatoolkit/native.xlsx", row.names = T)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,"sadržaj nastao") |
str_detect(AUTHOR,fixed("sponzor",ignore_case = TRUE)))
write.xlsx(dta, file = "D:/LUKA/Freelance/Mediatoolkit/native.xlsx", row.names = T)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
# some basic data wrangle
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
# select media
forum <- posts[SOURCE_TYPE == "forum",]
posts <- as.data.table(posts)
range(posts$DATE)
forum[,.N,TITLE][order(-N)]
forum[,.N,TITLE][order(-N)][]
posts %>%
group_by(SOURCE_TYPE) %>%
mutate(UkBrojObjava = n()) %>%
group_by(DATE, INTERDAYTIME, SOURCE_TYPE) %>%
mutate(PerDay = n()) %>%
ungroup() %>%
group_by(INTERDAYTIME,SOURCE_TYPE) %>%
summarise(AVG = mean(PerDay)) %>%
filter(SOURCE_TYPE != "comment") %>%
arrange(desc(AVG)) -> activePerDay
# convert chr date to date format
posts[,DTIME := anytime(paste(posts$DATE,posts$TIME))]
# convert chr date to date format
posts[,DTIME := anytime(paste(posts$DATE,posts$TIME))]
# make lubridate
posts[,DTIME := ymd_hms(paste(posts$DATE,posts$TIME))]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Vecer", "Jutro", "Popodne", "Prevecer")
# make a new variable
posts[, INTERDAYTIME := cut(x=hour(posts$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)][
,SOURCE_TYPE := as.factor(SOURCE_TYPE)
]
forum[TITLE == "Potraga za razlogom ili teorije zavjere vol. 9",][]
ggplot(activePerDay, aes(x = INTERDAYTIME, y = AVG, fill = INTERDAYTIME, width=0.75)) +
labs(y = "Broj objava ", x = "", title = "Aktivnost na online društvenim medijima U RH",
subtitle = "Dnevni prosjek za cjelokupni medijski prostor u razdoblju 6 mj",
caption = "Izvor: Mediatoolkit | Izradio: Lukos") +
geom_bar(stat = "identity") +
facet_wrap(~SOURCE_TYPE, scales = "free") +
coord_flip() +
guides(fill=FALSE) +
theme_bw() + theme( strip.background  = element_blank(),
panel.grid.major = element_blank(),
panel.border = element_blank(),
axis.ticks = element_blank(),
panel.grid.minor.y = element_blank(),
panel.grid.major.y = element_blank(),
panel.grid.minor.x = element_blank(),
panel.grid.major.x = element_blank(),
text=element_text(size=9,  family="Roboto")) +
theme(legend.position="bottom") +
scale_fill_brewer(palette="Set2") +
scale_fill_grey(start=0,end =0.8)
# read data
dta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
dta <- dta[!duplicated(dta),]
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dt <- dt[!duplicated(dt),]
# select twitter
tw <- dta %>%
filter(SOURCE_TYPE == "web")
# select forum
forum <- dta %>%
filter(SOURCE_TYPE == "forum")
# select twitter
tw <- dta %>%
filter(SOURCE_TYPE == "twitter")
# PER INFLUENCER
tw <- tw %>%
mutate(PROFILE = gsub("^.*\\.com/([^/]+).*", "\\1", URL))
tw <- as.data.table(tw)
# most active profiles
unique(tw[,.N,PROFILE][order(-N)])
# most active profiles
unique(tw[,.N,PROFILE][order(-N)]) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active profiles
unique(tw[,.N,PROFILE][order(-N)]) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most popular
tw %>%
group_by(PROFILE) %>%
summarise(FOLLOW = mean(FOLLOWERS_COUNT)) %>%
arrange(desc(FOLLOW)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most influential
tw %>%
group_by(PROFILE) %>%
summarise(REACH = sum(REACH)) %>%
arrange(desc(REACH)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most influential II
tw %>%
group_by(PROFILE) %>%
summarise(INTERACTIONS = sum(INTERACTIONS)) %>%
arrange(desc(INTERACTIONS)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most appreciated
tw %>%
group_by(PROFILE) %>%
summarise(FAVORITE = sum(FAVORITE_COUNT)) %>%
arrange(desc(FAVORITE)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most appreciated
tw %>%
group_by(PROFILE) %>%
summarise(RETWEET = sum(RETWEET_COUNT)) %>%
arrange(desc(RETWEET)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most popular
tw %>%
select(PROFILE, FULL_TEXT, FOLLOWERS_COUNT,URL) %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# select twitter
tw <- dt %>%
filter(SOURCE_TYPE == "twitter")
# most popular
tw %>%
select(PROFILE, FULL_TEXT, FOLLOWERS_COUNT,URL) %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# PER INFLUENCER
tw <- tw %>%
mutate(PROFILE = gsub("^.*\\.com/([^/]+).*", "\\1", URL))
tw <- as.data.table(tw)
# most popular
tw %>%
select(PROFILE, FULL_TEXT, FOLLOWERS_COUNT,URL) %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# select relevant CRO profiles
unique(tw[,.N,FROM][order(-N)]) %>%
filter(N > 5) %>%
pull(FROM) -> CRO_TW
tw[FROM %in% CRO_TW,] -> CTW
CTW %>%
mutate(PROFILE = gsub("^.*\\.com/([^/]+).*", "\\1", URL)) -> CTW
# most popular
CTW %>%
group_by(PROFILE) %>%
summarise(FOLLOW = mean(FOLLOWERS_COUNT)) %>%
arrange(desc(FOLLOW))  %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most influential
CTW %>%
group_by(PROFILE) %>%
summarise(REACH = sum(REACH)) %>%
arrange(desc(REACH)) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
CRO_TW
# most appreciated
CTW %>%
group_by(PROFILE) %>%
summarise(RETWEET = sum(RETWEET_COUNT)) %>%
arrange(desc(RETWEET))  %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8")  %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8") %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data
head(sample_n(Crosentilex_sve,1000),15)
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
Encoding(CroSentilex_Gold$word) <- "UTF-8"
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data
head(sample_n(CroSentilex_Gold,100),15)
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
word = c(
"jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez", "kod",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga",
"ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
# check stopwords data
head(sample_n(stop_corpus,100),15)
# read in data
forum <- as.data.table(forum)
forum[,.N, TITLE][order(-N)] %>% head(50)
forum[,.N, TITLE][order(-N)] %>%
slice(1:1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
forum[TITLE == "Zoran Milanović, predsjednik Republike Hrvatske vol. IV",] %>%
unnest_tokens(word,FULL_TEXT) -> ZM_token
# select forum
forum <- dt %>%
filter(SOURCE_TYPE == "forum")
# read in data
forum <- as.data.table(forum)
forum[TITLE == "Zoran Milanović, predsjednik Republike Hrvatske vol. IV",] %>%
unnest_tokens(word,FULL_TEXT) -> ZM_token
# remove stop words, numbers, single letters
ZM_token %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> ZM_tokenTidy
# remove NA
ZM_tokenTidy %>%
filter(!is.na(word)) -> ZM_tokenTidy
ZM_tokenTidy[,.N,by = word][order(-N),]
## Vizualize most common words
ZM_tokenTidy[,.N,by = word][N>500][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dta <- dt %>%
filter(str_detect(AUTHOR,fixed("sponzor",ignore_case = TRUE)))
dta <- dt %>%
filter(str_detect(AUTHOR,fixed("tnative",ignore_case = TRUE)))
dta <- dt %>%
filter(str_detect(AUTHOR,fixed("24ContentHaus",ignore_case = TRUE)))
dta <- dt %>%
filter(str_detect(AUTHOR,fixed("Haus",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("Haus",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("Content",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,"sadržaj donosi",ignore_case = TRUE))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,"sadržaj donosi",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("sadržaj donosi",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("nativ",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("24Content",ignore_case = TRUE)))24Content
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("24Content",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(FULL_TEXT,fixed("24Content",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("sadržaj donosi",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("sadržaj nastao ",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("prilog je napravljen u produkciji",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("prilog je napravljen",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("tnativ",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("nativ",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web")  %>%
filter(str_detect(AUTHOR,fixed("Native tim",ignore_case = TRUE)))
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("Native tim",ignore_case = TRUE)))
View(dta)
dta <- dt %>%
filter(SOURCE_TYPE =="web") %>%
filter(str_detect(FULL_TEXT,fixed("Sponzorirani sadržaj",ignore_case = TRUE)))
View(dta)
