parseArticle <- function(webpage) {
title <- html_nodes(webpage, xpath = '//h1[@class="article__title"]') %>%
html_text() %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
date <- html_nodes(webpage, xpath = '//*[@class="article__header_date"]') %>%
html_text() %>%
str_replace_all(pattern = "\\\r\\\n| u", replacement = "") %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
noComment <- html_nodes(webpage, xpath = '//*[@class="article__comments_number"]') %>%
html_text() %>%
trimws() %>%
str_extract("\\d+") %>%
as.numeric(.) %>%
ifelse(length(.) == 0, NA, .)
views <- html_nodes(webpage, xpath = '//*[@class="article__header_views"]') %>%
html_text() %>%
trimws() %>%
str_extract("\\d+") %>%
as.numeric(.) %>%
ifelse(length(.) == 0, NA, .)
articleLabel <- html_nodes(webpage, xpath = '//*[@class="article__label"]') %>%
html_text() %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
author <- html_nodes(webpage, xpath = '//*[@class="article__author--link"]') %>%
html_text() %>%
trimws()
if (length(autor) == 0) {autor <- NA}
articletext <- html_nodes(webpage, xpath = '//*[@class="article__body--main_content"]/p') %>%
html_text() %>%
str_flatten(., "\n") %>%
ifelse(length(.) == 0, NA, .)
keywords <- html_nodes(webpage, xpath = '//*[@class="article__tag_name"]') %>%
html_text() %>%
trimws() %>%
str_flatten(., ";") %>%
ifelse(length(.) == 0, NA, .)
articles <- cbind.data.frame(title, date, noComment, views, articleLabel,
articleLabel, author, articletext, keywords, stringsAsFactors = FALSE)
return(articles)
}
# 4. apply the function
data <- parseArticle(page1)
# 3. write a function to take parts of the article
parseArticle <- function(webpage) {
title <- html_nodes(webpage, xpath = '//h1[@class="article__title"]') %>%
html_text() %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
date <- html_nodes(webpage, xpath = '//*[@class="article__header_date"]') %>%
html_text() %>%
str_replace_all(pattern = "\\\r\\\n| u", replacement = "") %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
noComment <- html_nodes(webpage, xpath = '//*[@class="article__comments_number"]') %>%
html_text() %>%
trimws() %>%
str_extract("\\d+") %>%
as.numeric(.) %>%
ifelse(length(.) == 0, NA, .)
views <- html_nodes(webpage, xpath = '//*[@class="article__header_views"]') %>%
html_text() %>%
trimws() %>%
str_extract("\\d+") %>%
as.numeric(.) %>%
ifelse(length(.) == 0, NA, .)
articleLabel <- html_nodes(webpage, xpath = '//*[@class="article__label"]') %>%
html_text() %>%
trimws() %>%
ifelse(length(.) == 0, NA, .)
author <- html_nodes(webpage, xpath = '//*[@class="article__author--link"]') %>%
html_text() %>%
trimws()
if (length(author) == 0) {author <- NA}
articletext <- html_nodes(webpage, xpath = '//*[@class="article__body--main_content"]/p') %>%
html_text() %>%
str_flatten(., "\n") %>%
ifelse(length(.) == 0, NA, .)
keywords <- html_nodes(webpage, xpath = '//*[@class="article__tag_name"]') %>%
html_text() %>%
trimws() %>%
str_flatten(., ";") %>%
ifelse(length(.) == 0, NA, .)
articles <- cbind.data.frame(title, date, noComment, views, articleLabel,
articleLabel, author, articletext, keywords, stringsAsFactors = FALSE)
return(articles)
}
# 4. apply the function
data <- parseArticle(page1)
# 5. check the data
str(data)
data$title
data$numberOfComments
data$views
nchar(data$article)
nchar(data$articletext)
# assign urls of the articles
url3 <- "https://www.vecernji.hr/vijesti/glavni-tajnik-un-a-nuklearni-sukob-ponovno-izgleda-moguc-1570823"
url4 <- "https://www.vecernji.hr/vijesti/sto-je-clanak-5-nato-a-aktiviran-je-samo-jednom-a-ne-moze-se-primijeniti-na-ukrajinu-1570810"
url5 <- "https://www.vecernji.hr/vijesti/kotromanovic-ako-je-bila-rijec-o-naoruzanom-dronu-napad-je-to-na-clanicu-nato-a-1570731"
# bind all articles together
urls <- c(url1,url2,url3,url4,url5)
urls
clanciK <- list()
clanciLoop <- list()
for (i in urls) {
p <- html_session(url[i])
Sys.sleep(sample(1:5, 1))
clanakj[[j]] <- parseArticle(p)
Sys.sleep(1L)
clanciLoop[[i]] <- do.call(rbind, clanakj)
}
p <- html_session(urls[i])
for (i in urls) {
p <- html_session(urls[i])
Sys.sleep(sample(1:5, 1))
clanakj[[j]] <- parseArticle(p)
Sys.sleep(1L)
clanciLoop[[i]] <- do.call(rbind, clanakj)
}
clanakj <- list()
clanciLoop <- list()
for (i in urls) {
p <- html_session(urls[i])
Sys.sleep(sample(1:5, 1))
clanakj[[j]] <- parseArticle(p)
Sys.sleep(1L)
clanciLoop[[i]] <- do.call(rbind, clanakj)
}
p <- list()
clanakj <- list()
clanciLoop <- list()
for (i in urls) {
p <- html_session(urls[i])
clanakj[[j]] <- parseArticle(p)
Sys.sleep(1L)
clanciLoop[[i]] <- do.call(rbind, clanakj)
}
for (i in urls) {
p <- html_session(urls[i])
clanakj <- parseArticle(p)
clanciLoop[[i]] <- do.call(rbind, clanakj)
}
urls
p <- lapply(html_session,urls)
p <- lapply(urls,html_session)
pages <- lapply(urls,html_session)
multipleArticles <- lapply(pages, parseArticle)
dataArticles <- do.call(rbind, multipleArticles)
# check the data
str(dataArticles)
dataArticles$title
dataArticles$views
nchar(dataArticles$articletext)
glimpse(dataArticles)
# check the data
dim(dataArticles)
source("./Creds/api.R)"
source("./Creds/api.R")
source("../Creds/api.R")
source("./Creds/api.R")
source(here::here("Creds/api.R")
token <- "token"
token <- token
token
groups <- "182718"
keywords <- "6521533"
from_time <- as.character(as.numeric(as.POSIXlt("2022-01-01", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-01-02", format="%Y-%m-%d")))
groups <- "182718"
keywords <- "6521533"
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-01", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
count <- 30
probs <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
source(here::here("Creds/api.R")
source(here::here("Creds/api.R"))
source(here::here("Creds/api.R"))
probs <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
API_zahtjev <- httr::GET(probs)
jS_tekst <- httr::content(API_zahtjev, as = "text", type = "aplication/json", encoding = "UTF-8")
DF_za_analizu <- jsonlite::fromJSON(jS_tekst, flatten = TRUE)
View(DF_za_analizu)
dta <- data.frame(DF_za_analizu$data)
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-3", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
API_request
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-3", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# parse the request into jason object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
jS_text
# number of articles to retrieve
count <- 10
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into jason object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
jS_text
jS_text[1]
dataframeObject <- jsonlite::fromJSON(jS_text, flatten = TRUE)
#
str(js_text)
#
str(jS_text)
str(dataframeObject)
summary(dataframeObject)
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
lapply(dataList, head)
summary(dataList)
# make a data.frame from list
dataList$data
# make a data.frame from list
names(dataList$data)
# make a data.frame from list
names(dataList$data)
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-3", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
dim(data)
glimpse(data)
# check if there are articles from Večernji list
data %>%
group_by(response.from) %>%
count
# check if there are articles from Večernji list
data %>%
group_by(response.from) %>%
count %>%
arrange(desc(n)) %>%
head()
data %>%
filter(response.from == "vecernji.hr") %>%
select(response.title,
esponse.insert_time,
response.author,
response.full_mention,
response.url,
comment = response.comment_count,
response.mention)
data %>%
filter(response.from == "vecernji.hr") %>%
select(response.title,
response.insert_time,
response.author,
response.full_mention,
response.url,
comment = response.comment_count,
response.mention)
data %>%
filter(response.from == "vecernji.hr") %>%
select(title = response.title,
time = response.insert_time,
author = response.author,
mention = response.full_mention,
url = response.url,
comment = response.comment_count,
mention = response.mention) %>%
arrange(desc(comment))
data %>%
filter(response.from == "vecernji.hr") %>%
select(title = response.title,
time = response.insert_time,
author = response.author,
mention = response.full_mention,
url = response.url,
comment = response.comment_count,
text = response.mention) %>%
arrange(desc(comment))
data %>%
filter(response.from == "vecernji.hr") %>%
select(title = response.title,
time = response.insert_time,
author = response.author,
url = response.url,
comment = response.comment_count,
text = response.mention) %>%
arrange(desc(comment)) %>%
head
data %>%
slice(1) %>%
select(title, text, url)
data %>%
slice(1) %>%
select(title = response.title,
text = response.mention,
url = response.url)
data %>%
slice(1) %>%
select(url = response.url)
data %>%
slice(1) %>%
select(text = response.mention)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(rvest)
library(tidyverse)
library(httr)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-3", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-15", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# size of the data
dim(data)
# variables and variable types
glimpse(data)
# check if there are articles from Večernji list
data %>%
group_by(response.from) %>%
count %>%
arrange(desc(n)) %>%
head()
data %>%
filter(response.from == "vecernji.hr") %>%
select(title = response.title,
time = response.insert_time,
author = response.author,
url = response.url,
comment = response.comment_count,
text = response.mention) %>%
arrange(desc(comment)) %>%
head
data %>%
slice(1) %>%
select(title = response.title)
data %>%
slice(1) %>%
select(text = response.mention)
data %>%
slice(1) %>%
select(url = response.url)
