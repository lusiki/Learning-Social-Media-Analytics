<<<<<<< HEAD
variables <- read.xlsx("C:/Users/lukas/Dropbox/Članci/Native rad/Native.xlsx", sheetIndex = 1)
=======
words_vector <- str_c("\\b(", str_c(imena$root, collapse = "|"), ")\\b")
words_vector
dt[, FULL_TEXT := tolower(FULL_TEXT)]
# Vectorized function to check for matches
check_matches <- function(text, words_vector) {
any(stri_detect_fixed(text, words_vector, negate = FALSE))
}
batch_size <- 1000
# Calculate the number of batches
num_batches <- ceiling(nrow(dt) / batch_size)
# Loop through each batch
for (i in 1:num_batches) {
start_time <- Sys.time()
# Calculate the start and end row indices for the current batch
start_idx <- (i - 1) * batch_size + 1
end_idx <- min(i * batch_size, nrow(dt))
# Print the current batch number and row indices
cat(sprintf("Processing batch %d (rows %d to %d)...\n", i, start_idx, end_idx))
# Subset the data table for the current batch and apply the operations
dt[start_idx:end_idx, `:=` (
has_match = sapply(FULL_TEXT, check_matches, words_vector),
matched_words = sapply(FULL_TEXT, function(x) paste(words_vector[stri_detect_fixed(x, words_vector)], collapse=", "))
)]
batch_data <- dt[start_idx:end_idx]
end_time <- Sys.time()
duration <- end_time - start_time
# Print the duration for the current batch
cat(sprintf("Batch %d processed in %f seconds.\n", i, duration))
# ... [rest of your loop code for saving etc.] ...
}
imena <- dt[has_match==T,]
# Vectorized function to check for matches
check_matches <- function(text, words_vector) {
any(stri_detect_regex(text, words_vector, negate = FALSE))
}
batch_size <- 1000
# Calculate the number of batches
num_batches <- ceiling(nrow(dt) / batch_size)
# Loop through each batch
for (i in 1:num_batches) {
start_time <- Sys.time()
# Calculate the start and end row indices for the current batch
start_idx <- (i - 1) * batch_size + 1
end_idx <- min(i * batch_size, nrow(dt))
# Print the current batch number and row indices
cat(sprintf("Processing batch %d (rows %d to %d)...\n", i, start_idx, end_idx))
# Subset the data table for the current batch and apply the operations
dt[start_idx:end_idx, `:=` (
has_match = sapply(FULL_TEXT, check_matches, words_vector),
matched_words = sapply(FULL_TEXT, function(x) paste(unlist(str_extract_all(x, words_vector)), collapse=", "))
)]
batch_data <- dt[start_idx:end_idx]
end_time <- Sys.time()
duration <- end_time - start_time
# Print the duration for the current batch
cat(sprintf("Batch %d processed in %f seconds.\n", i, duration))
# ... [rest of your loop code for saving etc.] ...
}
imena <- dt[has_match==T,]
View(imena)
generalno <- c("crkva", "biskup", "Kaptol", "časna sestra", "svećenik", "župnik", "vjernik", "kardinal", "papa", "sveti otac", "redovnik", "redovnica","kršćanstvo", "vjera", "Gospa", "Isus", "katolički", "misa", "pričest", "krizma", "grijeh", "vjeroučitelj", "vjeronauk", "blagoslov","svjedočanstvo", "relikvija", "stigma", "duhovnost", "velečasni","zaređenje", "krunica", "vjeronauk", "ukazanje") %>% tolower()
generalno
a
genralno_root <- sapply(generalno, write_tokens)
genralno_root
genralno_root <- sapply(strsplit(genralno_root, "\t"), `[`, 2)
genralno_root
generalno <- enframe(genralno_root, name = "name", value = "root")
generalno
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(writexl)
library(data.table)
library(stringi)
dt <- read_excel("D:/LUKA/Academic/HKS/Projekti/Dezinformacije/Data/generalno.xlsx")
library(DBI) ## učitano
con <- dbConnect(RSQLite::SQLite(), path = ":memory:")
library(RSQLite)
install.packages("RSQLite")
library(RSQLite)
library(DBI) ## učitano
con <- dbConnect(RSQLite::SQLite(), path = ":memory:")
copy_to(
dest = con,
df = nycflights13::flights,
name = "flights",
temporary = FALSE,
indexes = list(
c("year", "month", "day"),
"carrier",
"tailnum",
"dest"
)
)
library(dplyr)
copy_to(
dest = con,
df = nycflights13::flights,
name = "flights",
temporary = FALSE,
indexes = list(
c("year", "month", "day"),
"carrier",
"tailnum",
"dest"
)
)
flights_db <- tbl(con, "flights")
flights_db
dbGetQuery(con, "SELECT * FROM flights WHERE dep_delay > 240.0 LIMIT 5")
q_con <-
dbConnect(
bigrquery::bigquery(),
project = "publicdata",
dataset = "samples",
billing = billing_id
)
billing_id <- Sys.getenv("GCE_DEFAULT_PROJECT_ID") ## zamijenite sa vašim ID
q_con <-
dbConnect(
bigrquery::bigquery(),
project = "publicdata",
dataset = "samples",
billing = billing_id
)
dbListTables(bq_con)
dbListTables(q_con)
natality <- tbl(bq_con, "natality")
natality <- tbl(q_con, "natality")
dbListTables(q_con)
View(natality)
bw <-
natality %>%
filter(!is.na(state)) %>% ## makni outlier-e
group_by(year) %>%
summarise(weight_pounds = mean(weight_pounds, na.rm=T)) %>%
collect()
View(bw)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1) %>% mutate(V1 = as.numeric(V1))
variables <- read.xlsx("D:/LUKA/Academic/Native rad/Native.xlsx", sheetIndex = 1)
>>>>>>> 0e746d60b77243cb41bd3b12ea7f4d02cb4fa0f0
variables <- variables[,-c(23,24,25)]# %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
dta <- merge(original, variables, by = "V1", all.x = TRUE)
dta$DATE <- as.Date(dta$DATE)
stemmed <- readRDS("C:/Users/Lukas/Dropbox/Mediatoolkit/native_token_stemm.rds")
stemmed <- readRDS("C:/Users/lukas/Dropbox/Članci/Native rad/native_token_stemm.rds")
stemmed <- stemmed %>%
rename("wordp" = "word") %>%
rename("word" = "transformed_column")
<<<<<<< HEAD
rm(dt)
View(variables)
View(dta)
View(original)
View(dta)
# articles over time
daily_counts <- dta %>%
group_by(DATE) %>%
summarise(count = n())
# descriptives
summ <- daily_counts %>%
summarize(min = min(count), max = max(count),
mean = mean(count), q1= quantile(count, probs = 0.25),
median = median(count), q3= quantile(count, probs = 0.75),
sd = sd(count)) %>%
mutate_if(is.numeric, round, digits=2)
summ
# create plot of articles over time
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
geom_line() +
labs(x = "Date", y = "Number of Articles")
>>>>>>> 0e746d60b77243cb41bd3b12ea7f4d02cb4fa0f0
options(htmltools.dir.version = FALSE)
library(tidyverse)
library(readxl)
=======
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
>>>>>>> 0e746d60b77243cb41bd3b12ea7f4d02cb4fa0f0
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
<<<<<<< HEAD
library(RMySQL)
conn <- dbConnect(RMySQL::MySQL(), dbname = "determ_all", host = "127.0.0.1",
user = "Lux", password = "Theanswer0207", local_infile = TRUE)
query <- "SELECT * FROM media_space_2022"
data <- dbGetQuery(conn, query)
conn <- dbConnect(RMySQL::MySQL(), dbname = "determ_all", host = "127.0.0.1",
user = "Lux", password = "Theanswer0207", local_infile = TRUE)
query <- "SELECT * FROM media_space_2023"
data <- dbGetQuery(conn, query)
range(data$DATE)
unique(data$SOURCE_TYPE)
length(unique(data$AUTHOR))
# unique titles
length(unique(data$TITLE))
# unique sources
length(unique(data$FROM))
# unique urls
length(unique(data$URL))
# GENERAL OVERVIEW
data %>%
group_by(SOURCE_TYPE) %>%
count() %>%
arrange(desc(n)) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
slice(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
data %>%
group_by(SOURCE_TYPE) %>%
count() %>%
arrange(desc(n)) %>%
mutate_if(is.numeric, format, big.mark = ".")
# GENERAL OVERVIEW
data %>%
group_by(SOURCE_TYPE) %>%
count() %>%
arrange(desc(n)) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
kable(., "html") %>%
kable_styling("striped", full_width = F)
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(8) %>%
slice(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# read data in (one day worth of data)
one_day_sample <- read_excel(here::here("data/Mediatoolkit/sample.xlsx"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(8) %>%
slice(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(8) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(20) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TITLES
data %>%
select(SOURCE_TYPE, TITLE, URL) %>%
sample_n(12) %>%
slice(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TITLES
data %>%
select(SOURCE_TYPE, TITLE, URL) %>%
sample_n(12) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TITLES
data %>%
select(SOURCE_TYPE, TITLE, URL) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "facebook") %>%
select(AUTHOR,
REACH,
FOLLOWERS_COUNT,
LIKE_COUNT,
COMMENT_COUNT,
TOTAL_REACTIONS_COUNT,
INFLUENCE_SCORE,
TITLE,
#         MENTION_SNIPPET,
DATE,
TIME) -> fb
fb %>%
group_by(AUTHOR) %>%
summarize(FOLLOWERS_COUNT = max(FOLLOWERS_COUNT)) %>%
ungroup() %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
slice(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(AUTHOR) %>%
summarize(FOLLOWERS_COUNT = max(FOLLOWERS_COUNT)) %>%
ungroup() %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(AUTHOR) %>%
summarize(FOLLOWERS_COUNT = max(FOLLOWERS_COUNT)) %>%
ungroup() %>%
arrange(desc(FOLLOWERS_COUNT)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
fb %>%
group_by(AUTHOR) %>%
summarise( N = n()) %>%
ungroup() %>%
arrange(desc(N)) %>%
mutate(N = format(N, big.mark = ",")) %>%
head(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
fb %>%
group_by(AUTHOR) %>%
summarise( N = n()) %>%
ungroup() %>%
arrange(desc(N)) %>%
mutate(N = format(N, big.mark = ",")) %>%
head(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# authors with impact
fb %>%
group_by(AUTHOR) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
arrange(desc(LIKES)) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
head(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(TITLE) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
arrange(desc(LIKES)) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
head(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(TITLE) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
arrange(desc(LIKES)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# important activity and author
fb %>%
group_by(AUTHOR) %>%
mutate(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
select(AUTHOR, TITLE, LIKES, COMMENT, REACH,REACTIONS ) %>%
arrange(desc(LIKES)) %>%
slice(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(TITLE) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
select(AUTHOR, TITLE, LIKES, COMMENT, REACH,REACTIONS ) %>%
arrange(desc(LIKES)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
fb %>%
group_by(TITLE) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
select(TITLE, LIKES, COMMENT, REACH,REACTIONS ) %>%
arrange(desc(LIKES)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# important activity and author
fb %>%
group_by(AUTHOR) %>%
mutate(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
REACTIONS = sum(TOTAL_REACTIONS_COUNT)) %>%
select(AUTHOR, TITLE, LIKES, COMMENT, REACH,REACTIONS ) %>%
arrange(desc(LIKES)) %>%
slice(100) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "web") %>%
select(FROM_SITE,
AUTHOR,
REACH,
VIRALITY,
LIKE_COUNT,
COMMENT_COUNT,
INFLUENCE_SCORE,
TITLE,
#       MENTION_SNIPPET,
DATE,
TIME) -> web
# most active authors
web %>%
group_by(FROM_SITE) %>%
summarise(N = n()) %>%
arrange(desc(N))  %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
slice(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
web %>%
group_by(FROM_SITE) %>%
summarise(N = n()) %>%
arrange(desc(N))  %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
web %>%
group_by(FROM_SITE) %>%
summarise(N = n()) %>%
arrange(desc(N))  %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
web %>%
group_by(FROM_SITE) %>%
summarise(LIKES = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT),
REACH = sum(REACH),
VIRALITY = sum(VIRALITY)) %>%
arrange(desc(LIKES)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",")  %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "twitter") %>%
select(AUTHOR,
REACH,
FOLLOWERS_COUNT,
FAVORITE_COUNT,
RETWEET_COUNT,
TWEET_TYPE,
TWEET_SOURCE_NAME,
TITLE,
#      MENTION_SNIPPET,
DATE,
TIME) -> twitter
# most important authors
twitter %>%
group_by(AUTHOR) %>%
summarise(FOLLOWERS = max(FOLLOWERS_COUNT)) %>%
arrange(desc(FOLLOWERS))  %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
twitter %>%
group_by(AUTHOR) %>%
summarise(N = n()) %>%
arrange(desc(N))  %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
twitter %>%
group_by(AUTHOR) %>%
summarise(FAVORITE = sum(FAVORITE_COUNT),
RETWEET = sum(RETWEET_COUNT)) %>%
arrange(desc(RETWEET))  %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# originality
twitter %>%
group_by(TWEET_TYPE) %>% # interesting to check per author
summarise(N = n()) %>%
arrange(desc(N)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# technology
twitter %>%
group_by(TWEET_SOURCE_NAME) %>%
summarise(N = n()) %>%
arrange(desc(N)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# most active authors
youtube %>%
group_by(AUTHOR) %>%
count() %>%
arrange(desc(n)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "youtube") %>%
select(AUTHOR,
REACH,
LIKE_COUNT,
VIEW_COUNT,
COMMENT_COUNT,
TITLE,
#        MENTION_SNIPPET,
URL,
DATE,
TIME) -> youtube
# most active authors
youtube %>%
group_by(AUTHOR) %>%
count() %>%
arrange(desc(n)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# activity per author
youtube %>%
group_by(AUTHOR) %>%
summarise(REACH = sum(REACH),
LIKE = sum(LIKE_COUNT),
VIEW = sum(VIEW_COUNT),
COMMENTS= sum(COMMENT_COUNT)) %>%
arrange(desc(VIEW)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# breaks down
youtube %>%
group_by(AUTHOR, URL,REACH,LIKE_COUNT, VIEW_COUNT, COMMENT_COUNT) %>%
arrange(desc(VIEW_COUNT)) %>%
head(1000) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "forum") %>%
select(FROM_SITE,
TITLE,
#      MENTION_SNIPPET,
DATE,
TIME) -> forum
# not a very good coverage
forum %>%
group_by(FROM_SITE) %>%
count() %>%
arrange(desc(n)) %>%
head(1000) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# choose variables to investigate
data %>%
filter(SOURCE_TYPE == "instagram") %>%
select(FROM_SITE,
TITLE,
AUTHOR,
#      MENTION_SNIPPET,
REACH,
LIKE_COUNT,
COMMENT_COUNT,
SHARE_COUNT,
VIEW_COUNT,
DATE,
TIME) -> instagram
# influencer impact
instagram %>%
group_by(FROM_SITE) %>%
summarise(REACH = sum(REACH),
COMMENT = sum(COMMENT_COUNT),
LIKE = sum(LIKE_COUNT),
VIEW = sum(VIEW_COUNT)) %>%
mutate_if(is.numeric, format, big.mark = ",") %>%
arrange(desc(LIKE)) %>%
head(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# influencer activity
instagram %>%
group_by(FROM_SITE) %>%
count() %>%
arrange(desc(n)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ".")%>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "200px"))
# GENERAL OVERVIEW
data %>%
group_by(SOURCE_TYPE) %>%
count() %>%
arrange(desc(n)) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
kable(., "html") %>%
kable_styling("striped", full_width = F)
# SAMPLE SOME TITLES
data %>%
select(SOURCE_TYPE, TITLE, URL) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "200px"))
# SAMPLE SOME TXT
one_day_sample %>%
select(SOURCE_TYPE, MENTION_SNIPPET, URL) %>%
unique(.) %>%
sample_n(100) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "350px"))
# influencer activity
instagram %>%
group_by(FROM_SITE) %>%
count() %>%
arrange(desc(n)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
kadable(., options = list(scrollX = TRUE, scrollY = "200px"))
# influencer activity
instagram %>%
group_by(FROM_SITE) %>%
count() %>%
arrange(desc(n)) %>%
head(100) %>%
mutate_if(is.numeric, format, big.mark = ".") %>%
kable(., options = list(scrollX = TRUE, scrollY = "200px"))
# technology
twitter %>%
group_by(TWEET_SOURCE_NAME) %>%
summarise(N = n()) %>%
arrange(desc(N)) %>%
mutate_if(is.numeric, format, big.mark = ",")  %>%
kable(., options = list(scrollX = TRUE, scrollY = "200px"))
=======
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
# Read In
original <- read.xlsx("D:/LUKA/Freelance/Mediatoolkit/native1.xlsx", sheetIndex = 1) %>% mutate(V1 = as.numeric(V1))
variables <- read.xlsx("D:/LUKA/Academic/Native rad/Native.xlsx", sheetIndex = 1)
original <- read.xlsx("C:/Users/lukas/Desktop/native1.xlsx", sheetIndex = 1) %>% mutate(V1 = as.numeric(V1))
variables <- variables[,-c(23,24,25)]# %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
dta <- merge(original, variables, by = "V1", all.x = TRUE)
dta$DATE <- as.Date(dta$DATE)
stemmed <- readRDS("C:/Users/lukas/Desktop/native_token_stemm.rds")
stemmed <- readRDS("C:/Users/Lukas/Dropbox/Mediatoolikit/native_token_stemm.rds")
stemmed <- readRDS("C:/Users/Lukas/Dropbox/Mediatoolkit/native_token_stemm.rds")
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dt <- dt %>% filter(SOURCE_TYPE == "web")
n_distinct(dt$FROM)
result <- dt[, .(Count = .N), by = .(FROM)][, Percentage := (Count / sum(Count)) * 100]
View(result)
result %>% arrange(Count)
result %>% arrange(Count) %>% View()
result %>% arrange(desc(Count)) %>% View()
result %>% filter(str_detect(FROM,"bitno")
)
result %>% filter(str_detect(FROM,"fino") )
result %>% filter(str_detect(FROM,"net") )
result %>% filter(str_detect(FROM,"net") ) %>% View()
result %>% filter(str_detect(FROM,"bit") ) %>% View()
result %>% filter(str_detect(FROM,"bitno") ) %>% View()
knitr::opts_chunk$set(echo = TRUE)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
dt %>% filter(str_detect(FROM,"bitno") )
dt %>% filter(str_detect(FROM,"bitno") ) %>% View()
dt %>% filter(str_detect(URL,"bitno") ) %>% View()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
# fullDta <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_NOTXT.csv")
# fullDtaTxt <- fread("D:/LUKA/Freelance/Mediatoolkit/FULLDATA_TXT.csv")
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
View(dt)
original <- read.xlsx("C:/Users/lukas/Dropbox/Članci/Native rad/native1.xlsx", sheetIndex = 1) %>% mutate(V1 = as.numeric(V1))
variables <- read.xlsx("C:/Users/lukas/Dropbox/Članci/Native rad/Native.xlsx", sheetIndex = 1)
variables <- variables[,-c(23,24,25)]# %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
dta <- merge(original, variables, by = "V1", all.x = TRUE)
dta$DATE <- as.Date(dta$DATE)
stemmed <- readRDS("C:/Users/Lukas/Dropbox/Mediatoolkit/native_token_stemm.rds")
stemmed <- readRDS("C:/Users/lukas/Dropbox/Članci/Native rad/native_token_stemm.rds")
stemmed <- stemmed %>%
rename("wordp" = "word") %>%
rename("word" = "transformed_column")
View(original)
rm(dt)
# articles over time
daily_counts <- dta %>%
group_by(DATE) %>%
summarise(count = n())
# descriptives
summ <- daily_counts %>%
summarize(min = min(count), max = max(count),
mean = mean(count), q1= quantile(count, probs = 0.25),
median = median(count), q3= quantile(count, probs = 0.75),
sd = sd(count)) %>%
mutate_if(is.numeric, round, digits=2)
summ
# create plot of articles over time
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
geom_line() +
labs(x = "Date", y = "Number of Articles")
# Portals by activity
activity <- dta %>%
group_by(FROM) %>%
summarise(count = n()) %>%
mutate(percent = round(count / sum(count) * 100,2)) %>%
arrange(desc(count))
datatable(activity, options = list(scrollX = TRUE, scrollY = "500px"))
# read in lexicons
CroSentilex_n <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-negatives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8")  %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p  <- read.delim("C:/Users/Lukas/Dropbox/Mislav@Luka/crosentilex-positives.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE,
fileEncoding = "UTF-8") %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
# check lexicon data
#head(sample_n(Crosentilex_sve,1000),15)
CroSentilex_Gold  <- read.delim2("C:/Users/Lukas/Dropbox/Mislav@Luka/gs-sentiment-annotations.txt",
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
Encoding(CroSentilex_Gold$word) <- "UTF-8"
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
# check lexicon data
#head(sample_n(CroSentilex_Gold,100),15)
LilaHR  <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean.xlsx", sheet = "Sheet1") %>% select (-"...1")
LilaHR_long <- read_excel("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_clean_long.xlsx", sheet = "Sheet1") %>% select (-"...1")
# Print the long format data
#print(data_long)
#proba <- read.csv2("C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHRcsv.csv", encoding = "UTF-8")
#df <- separate_rows(LilaHR, HR, sep = ", ")
#
# zero_rows_count <- sum(apply(df[-1], 1, function(row) all(row == 0)))
# print(zero_rows_count)
#
# filtered_df <- df %>%
#   filter(!apply(.[,-1], 1, function(row) all(row == 0)))
#
# write.xlsx(filtered_df, "C:/Users/Lukas/Dropbox/Mislav@Luka/lilaHR_.xlsx" )
# create stop words
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
# check stopwords data
#head(sample_n(stopwords_cro,100),15)
# extend stop words
my_stop_words <- tibble(
word = c(
"jedan","mjera", "može", "možete", "mogu", "kad", "sada", "treba", "ima", "osoba",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez", "kod",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga",
"ubera", "vozača", "hrvatskoj", "usluge", "godine", "više", "taksi", "taxi", "taksija", "taksija", "kaže", "rekao", "19"," aee", "ae","bit.ly", "https", "one", "the"
),
lexicon = "lux"
)
# full set with diacritics
cro_sw_full_d <- tibble(word = c("a","ako","ali","baš","bez","bi","bih","bila","bili","bilo","bio","bismo","bit","biti","bolje","bude","čak","čega","čemu","često","četiri","čime","čini","će","ćemo","ćete","ću","da","dakle","dalje","dan","dana","dana","danas","dio","do","dobro","dok","dosta","dva","dvije","eto","evo","ga","gdje","god","godina","godine","gotovo","grada","i","iako","ići","ih","ili","im","ima","imaju","imali","imam","imao","imati","inače","ipak","isto","iz","iza","između","ja","jako","je","jedan","jedna","jednog","jednom","jednostavno","jednu","jer","joj","još","ju","ka","kad","kada","kaj","kako","kao","kaže","kod","koja","koje","kojeg","kojeg","kojem","koji","kojih","kojim","kojima","kojoj","kojom","koju","koliko","kraju","kroz","li","malo","manje","me","među","međutim","mene","meni","mi","milijuna","mislim","mjesto","mnogo","mogao","mogli","mogu","moj","mora","možda","može","možemo","možete","mu","na","način","nad","naime","nakon","nam","naravno","nas","ne","neće","nego","neka","neke","neki","nekog","nekoliko","neku","nema","nešto","netko","ni","nije","nikad","nisam","nisu","ništa","niti","no","njih","o","od","odmah","odnosno","oko","on","ona","onda","oni","onih","ono","opet","osim","ova","ovaj","ovdje","ove","ovim","ovo","ovog","ovom","ovu","pa","pak","par","po","pod","poput","posto","postoji","pred","preko","prema","pri","prije","protiv","prvi","puno","put","radi","reći","s","sa","sad","sada","sam","samo","sati","se","sebe","si","smo","ste","stoga","strane","su","svaki","sve","svi","svih","svoj","svoje","svoju","što","ta","tada","taj","tako","također","tamo","te","tek","teško","ti","tih","tijekom","time","tko","to","tog","toga","toj","toliko","tom","tome","treba","tu","u","uopće","upravo","uvijek","uz","vam","vas","već","vi","više","vrijeme","vrlo","za","zapravo","zar","zato","zbog","zna","znači"),
lexicon = "boras")
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
stop_corpus <- stop_corpus %>%
bind_rows(cro_sw_full_d)
# check stopwords data
#head(sample_n(stop_corpus,100),15)
# tokenize
dta %>%
unnest_tokens(word, FULL_TEXT) -> n_token
View(n_token)
# remove stop words, numbers, single letters
n_token %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) -> n_tokenTidy
# remove NA
n_tokenTidy %>%
filter(!is.na(word)) -> n_tokenTidy
stemmed %>%
group_by(word) %>%
summarise(count = n()) %>%
mutate(percent = round(count / sum(count) * 100,2)) %>%
arrange(desc(count)) %>%
filter(count > 50) %>%
datatable(., options = list(scrollX = TRUE, scrollY = "500px"))
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(readxl)
library(xlsx)
library(here)
library(kableExtra)
library(DT)
library(purrr)
library(data.table)
library(tidytext)
library(dplyr)
library(lubridate)
library(anytime)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(xlsx)
library(knitr)
library(kableExtra)
library(stopwords)
dt <- fread("D:/LUKA/Freelance/Mediatoolkit/MktFULLtxt.csv")
View(dt)
original <- read.xlsx("C:/Users/lukas/Dropbox/Članci/Native rad/native1.xlsx", sheetIndex = 1) %>% mutate(V1 = as.numeric(V1))
variables <- read.xlsx("C:/Users/lukas/Dropbox/Članci/Native rad/Native.xlsx", sheetIndex = 1)
variables <- variables[,-c(23,24,25)]# %>% drop_na()
original <- original %>% filter(V1 %in% variables$V1)
dta <- merge(original, variables, by = "V1", all.x = TRUE)
dta$DATE <- as.Date(dta$DATE)
stemmed <- readRDS("C:/Users/Lukas/Dropbox/Mediatoolkit/native_token_stemm.rds")
stemmed <- readRDS("C:/Users/lukas/Dropbox/Članci/Native rad/native_token_stemm.rds")
stemmed <- stemmed %>%
rename("wordp" = "word") %>%
rename("word" = "transformed_column")
rm(dt)
View(variables)
View(dta)
View(original)
View(dta)
# articles over time
daily_counts <- dta %>%
group_by(DATE) %>%
summarise(count = n())
# descriptives
summ <- daily_counts %>%
summarize(min = min(count), max = max(count),
mean = mean(count), q1= quantile(count, probs = 0.25),
median = median(count), q3= quantile(count, probs = 0.75),
sd = sd(count)) %>%
mutate_if(is.numeric, round, digits=2)
summ
# create plot of articles over time
ggplot(data = daily_counts, aes(x = DATE, y = count)) +
geom_line() +
labs(x = "Date", y = "Number of Articles")
>>>>>>> 0e746d60b77243cb41bd3b12ea7f4d02cb4fa0f0
