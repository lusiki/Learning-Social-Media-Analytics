facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
# Bigrami
topicBigram %>%
cast_dtm(FROM, bigram,n) -> dtmB
# Bigrami
topicBigram %>%
cast_dtm(FROM, bigram,n) -> dtmB
#bigrams_united
bigrams_united %>%
count(FROM,bigram,sort = T) -> topicBigram
# Bigrami
topicBigram %>%
cast_dtm(FROM, bigram,n) -> dtmB
insta_LDA <- LDA(dtmB, k = 4,  control = list(seed = 1234))
insta_LDA_tidy <- tidy(insta_LDA, matrix = "beta")
#newsCOVID_LDA_tidy
newsCOVID_terms <- newsCOVID_LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#newsCOVID_LDA_tidy
insta_terms <- insta_LDA_tidy %>%
drop_na(.) %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, -beta)
#newsCOVID_terms
newsCOVID_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
#newsCOVID_terms
insta_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(term, beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip() +
scale_x_reordered() +
theme_economist()
# time span
range(posts$DATETIME)
# descriptives
posts[,.N,SOURCE_TYPE][order(-N)]
# how many letters in a title
posts[,
.(Avg = mean(nchar(TITLE), na.rm = T),
STD = sd(nchar(TITLE), na.rm = T),
min = min(nchar(TITLE), na.rm = T),
max = max(nchar(TITLE), na.rm = T)),
SOURCE_TYPE][order(-Avg),]
# how many letters in a text
posts[,
.(Avg = mean(nchar(FULL_TEXT)),
STD = sd(nchar(FULL_TEXT)),
min = min(nchar(FULL_TEXT)),
max = max(nchar(FULL_TEXT))),
SOURCE_TYPE][order(-Avg),]
insta
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
insta[,.N,FROM][order(-N)]
head(insta)
names(insta)
View(insta)
View(insta)
insta[,LIKE := sum(REACH), FROM][order(-LIKE)]
insta[,LIKE := sum(REACH), FROM][,.(FROM,LIKE)][order(-LIKE)]
insta[,LIKE := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE)]
insta[,LIKE := sum(REACH), FROM,LIKE][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE)]
insta[,LIKE := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE)]
insta[,LIKE := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE),]
unique(insta[,LIKE := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE),])
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
insta[,.N,FROM][order(-N)]
# influencers by LIKE
unique(insta[,LIKE := sum(LIKE_COUNT), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE),])
# influencers by INTERACTIONS
unique(insta[,INTERACTIONS := sum(INTERACTIONS), FROM][FROM != "anonymous_user",.(FROM,INTERACTIONS)][order(-INTERACTIONS),])
# influencers by LIKE
unique(insta[,LIKE := sum(LIKE_COUNT), FROM][FROM != "anonymous_user",.(FROM,LIKE)][order(-LIKE),])
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
printMD(insta[,.N,FROM][order(-N)], big.mark=",")
remotes::install_github("humburg/reportmd")
library(reportmd)
library(reportmd)
require(reportmd)
require(reportMD)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
printMD(insta[,.N,FROM][order(-N)], big.mark=",")
# influencers by REACH
printMD(unique(insta[,REACH := sum(REACH), FROM][FROM != "anonymous_user",.(FROM,REACH)][order(-REACH),]), big.mark=",")
# influencers by INTERACTIONS
printMD(unique(insta[,INTERACTIONS := sum(INTERACTIONS), FROM][FROM != "anonymous_user",.(FROM,INTERACTIONS)][order(-INTERACTIONS),]), big.mark=",")
# influencers by COMMENT
printMD(unique(insta[,COMMENT := sum(COMMENT_COUNT), FROM][FROM != "anonymous_user",.(FROM,COMMENT)][order(-COMMENT),]), big.mark=",")
View(insta)
# tokenize
insta %>%
unnest_tokens(word, FULL_TEXT) -> insta
# dim before tokenize
dim(insta)
# check
insta %>%
sample_n(.,10)
# check
insta_tokenTidy %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))
# check
insta_tokenTidy %>%
sample_n(.,15) %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T))
## Most common words
insta_tokenTidy[,.N,by = word][order(-N),]
library(reportMD)
## Most common words
printMD(insta_tokenTidy[,.N,by = word][order(-N),], big.mark=",")
## Most common words
printMD(insta_tokenTidy[,.N,by = word][order(-N),][N > 500], big.mark=",")
## Vizualize most common words
insta_tokenTidy[,.N,by = word][N>2000][order(-N),][,word := reorder(word,N)] %>%
ggplot(aes(word, N)) +
geom_col() +
xlab(NULL) +
coord_flip() +
theme_economist()
## Vizualize most common words over time
insta_tokenTidy[,DAY:=floor_date(DATE,"day")][,N:=.N,by=DAY][,gn:=sum(N)][
word %in% c("akcija", "ponuda", "poklon", "gratis", "kože"),] %>%
ggplot(., aes(DAY,  N / gn)) +
geom_point() +
ggtitle("Učestalost korištenja riječi") +
ylab("% ukupnih riječi") +
geom_smooth() +
facet_wrap(~ word, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format())+
theme_economist()
insta[ ,.N,FROM][N>1500][order(-N)]
printMD(insta[ ,.N,FROM][N>1500][order(-N)], big.mark=",")
printMD(insta[,.N,FROM][order(-N)], big.mark=",")
#web <- posts[SOURCE_TYPE == "web",]
# fb <- posts[SOURCE_TYPE == "facebook",]
# youtube <- posts[SOURCE_TYPE == "youtube",]
insta <- posts[SOURCE_TYPE == "instagram",]
printMD(insta[,.N,FROM][order(-N)], big.mark=",")
insta_tokenTidy %>%
filter(word == "poklon" & FROM %in% c("elladvornik", "beautypharmacy_hr", "lucija_lugomer_official")) %>%
mutate(WEEK = floor_date(DATE, "week")) %>%
group_by(WEEK, FROM) %>%
summarise(n = n()) %>%
ungroup() %>%
arrange(desc(n)) %>%
ggplot(., aes(WEEK,  n)) +
geom_line() +
ggtitle("Članci o poklonima na najvažnijim IG profilima") +
ylab("Broj objava") +
geom_smooth() +
facet_wrap(~ FROM, scales = "free_y") +
theme_economist()
## Sentiment over time
vizualiziraj_sentiment <- function(dataset, frq = "week") {
dataset %>%
inner_join( Crosentilex_sve, by = "word") %>%
filter(!is.na(word)) %>%
select(word, brija, DATE, sentiment) %>%
unique() %>%
spread(. , brija, sentiment) %>%
mutate(sentiment = POZ - NEG) %>%
select(word, DATE, sentiment) %>%
group_by(word) %>%
mutate(count = n()) %>%
arrange(desc(count)) %>%
mutate( score = sentiment*count) %>%
ungroup() %>%
group_by(DATE) %>%
arrange(desc(DATE)) -> sm
sm %>%
select(DATE, score) %>%
group_by(DATE = floor_date(DATE, frq)) %>%
summarise(Dnevni_sent = sum(score, na.rm = TRUE)) %>%
ggplot(., aes(DATE, Dnevni_sent)) +
geom_bar(stat = "identity") +
ggtitle(paste0("Sentiment over time;freqency:", frq)) +
ylab("SentimentScore") +
theme_economist()-> gg_sentiment_kroz_vrijeme_qv
gg_sentiment_kroz_vrijeme_qv
}
vizualiziraj_sentiment(insta_tokenTidy,"week")
## Sentiment
doprinos_sentimentu <- function(dataset, no = n) {
dataset %>%
inner_join(CroSentilex_Gold, by = "word") %>%
count(word, sentiment,sort = TRUE) %>%
group_by(sentiment) %>%
top_n(no) %>%
ungroup() %>%
mutate(sentiment = case_when(sentiment == 0 ~ "NEUTRAL",
sentiment == 1 ~ "NEGATIVE",
sentiment == 2 ~ "POSITIVE")) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n, fill = sentiment)) +
geom_col(show.legend = FALSE) +
ggtitle( "Sentiment") +
labs( x = "Riječ", y = "Number of words") +
facet_wrap(~ sentiment, scales = "free_y") +
coord_flip() +
theme_economist() -> gg_doprinos_sentimentu
gg_doprinos_sentimentu
}
doprinos_sentimentu(insta_tokenTidy,15)
## Najnegativniji portali
wCount <- insta_tokenTidy %>%
group_by(FROM) %>%
summarise(word = n())
CroSentilex_Gold_neg <- CroSentilex_Gold %>% filter(sentiment == 1)
CroSentilex_Gold_poz <- CroSentilex_Gold %>% filter(sentiment == 2)
insta_tokenTidy %>%
semi_join(CroSentilex_Gold_neg, by= "word") %>%
group_by(FROM) %>%
summarise(negWords = n()) %>%
left_join(wCount, by = "FROM") %>%
mutate(negativnostIndex = (negWords/word)*100) %>%
arrange(desc(negativnostIndex))
insta_tokenTidy %>%
semi_join(CroSentilex_Gold_neg, by= "word") %>%
group_by(FROM) %>%
summarise(negWords = n()) %>%
left_join(wCount, by = "FROM") %>%
mutate(negativnostIndex = (negWords/word)*100) %>%
arrange(desc(negativnostIndex)) %>%
printMD(., big.mark=",")
insta_tokenTidy %>%
semi_join(CroSentilex_Gold_poz, by= "word") %>%
group_by(FROM) %>%
summarise(pozWords = n()) %>%
left_join(wCount, by = "FROM") %>%
mutate(pozitivnostIndex = (pozWords/word)*100) %>%
arrange(desc(pozitivnostIndex)) %>%
printMD(., big.mark=",")
## Udio riječi po domenama
domenaWords <- insta_tokenTidy %>%
filter(FROM %in% c("beautypharmacy_hr", "jutarnji.hr", "vecernji.list" )) %>%
count(FROM, word, sort = T)
ukupnoWords <- domenaWords %>%
group_by(FROM) %>%
summarise(totWords = sum(n))
domenaWords <- left_join(domenaWords, ukupnoWords)
# domenaWords %>% head(15)
# domenaWords %>%
# ggplot(., aes(n/totWords, fill = domena)) +
#   geom_histogram(show.legend = FALSE) +
#   xlim(NA, 0.0009) +
#   facet_wrap(~domena, ncol = 2, scales = "free_y")
## Najbitnije riječi po domenma
idf <- domenaWords %>%
bind_tf_idf(word, FROM, n)
idf %>% head(10)
# idf %>%
#   select(-totWords) %>%
#   arrange(desc(tf_idf))
idf %>%
arrange(desc(tf_idf)) %>%
mutate(word = factor(word, levels = rev(unique(word)))) %>%
mutate(FROM = factor(FROM)) %>%
group_by(FROM) %>%
top_n(15) %>%
ungroup() %>%
ggplot(aes(word, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
insta_bigram <- insta %>%
unnest_tokens(bigram, FULL_TEXT, token = "ngrams", n = 2)
insta_bigram %>% head(10)
insta_bigram %>%
count(bigram, sort = T) %>%
head(15)
bigram_tf_idf %>%
filter(FROM %in% c("elladvornik", "imerovic_sandi_budivelik", "elaajerkovic" )) %>%
arrange(desc(tf_idf)) %>%
mutate(bigram = factor(bigram, levels = rev(unique(bigram)))) %>%
group_by(FROM) %>%
top_n(7) %>%
ungroup() %>%
ggplot(aes(bigram, tf_idf, fill = FROM)) +
geom_col(show.legend = FALSE) +
labs(x = NULL, y = "tf-idf") +
facet_wrap(~FROM, ncol = 2, scales = "free") +
coord_flip() +
theme_economist()
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
filter(item1 %in% c("kupnja", "akcija", "sex", "poklon")) %>%
group_by(item1) %>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip() +
theme_economist()
#corsWords %>%
#  filter(item1 == "oporavak")
corsWords %>%
filter(item1 %in% c("kupnja", "akcija", "sex", "poklon")) %>%
group_by(item1) %>%
top_n(10) %>%
ungroup() %>%
mutate(item2 = reorder(item2, correlation)) %>%
ggplot(aes(item2, correlation)) +
geom_bar(stat = "identity") +
facet_wrap(~ item1, scales = "free") +
coord_flip() +
theme_economist()
View(insta)
# tokenize
insta %>%
unnest_tokens(word, FULL_TEXT) -> insta
View(insta)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
# read in data
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLtxtDATA"
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(data.table)
library(lubridate)
library(anytime)
library(tidytext)
library(grid)
library(wordcloud)
library(reshape2)
library(ggraph)
library(widyr)
library(topicmodels)
library(ggthemes)
library(reportMD)
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, read_excel)
knitr::opts_chunk$set(echo = T, message = F, warning = F)
all <- as.data.table(all_raw)
all <- all[,DATE := as.Date(DATE,"%Y-%m-%d")][,DATETIME := anytime(paste(DATE,TIME))]
posts <- all[!duplicated(all),]
rm(all,all_raw)
#web <- posts[SOURCE_TYPE == "web",]
fb <- posts[SOURCE_TYPE == "facebook",]
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
printMD(fb[,.N,FROM][order(-N)], big.mark=",")
View(fb)
fb[,.N,FROM]
fb[,.N,FROM][order(-N)]
printMD(fb[,.N,FROM][order(-N)], big.mark=",")
fb[,.N,FROM][order(-N)]
View(fb)
# influencers by FOLLOWERS
fb[FOLLOWERS_COUNT == max(FOLLOWERS_COUNT), by=FROM]
# influencers by FOLLOWERS
fb[max(FOLLOWERS_COUNT),, by=FROM]
# influencers by FOLLOWERS
fb[max(FOLLOWERS_COUNT), by=FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][order(-FOLLOWERS)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][order(-FOLLOWERS),c(FOLLOWERS,FROM)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)]
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique()
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique() %>% printMD( big.mark=",")
# influencers by FOLLOWERS
fb[,FOLLOWERS := max(FOLLOWERS_COUNT), FROM][,c("FOLLOWERS","FROM")][order(-FOLLOWERS)] %>% unique()
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(FOLLOWERS,FROM)][order(-REACH),])# influencers by LIKE
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(FOLLOWERS,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][[,.(REACH,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique()
# youtube <- posts[SOURCE_TYPE == "youtube",]
#instagram <- posts[SOURCE_TYPE == "instagram",]
#rm(posts)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
# influencers by REACH
fb[,REACH := sum(REACH), FROM][,.(REACH,FROM)][order(-REACH)] %>% unique()
fb[,.N,FROM][order(-N)]
# influencers by LIKE
fb[,LIKE := sum(LIKE), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique()
# influencers by LIKE
fb[,LIKE := sum(LIKE_COUNT), FROM][,.(LIKE,FROM)][order(-LIKE)] %>% unique()
# influencers by INTERACTIONS
fb[,LIKE := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique()
# influencers by COMMENT
fb[,INTERACTIONS := sum(COMMENT_COUNT), FROM][,.(COMMENT_COUNT,FROM)][order(-COMMENT_COUNT)] %>% unique()
# influencers by INTERACTIONS
fb[,INTERACTIONS := sum(INTERACTIONS), FROM][,.(INTERACTIONS,FROM)][order(-INTERACTIONS)] %>% unique()
# influencers by COMMENT
fb[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique()
# influencers by SHARE
fb[,SHARE := sum(SHARE_COUNT), FROM][,.(SHARE,FROM)][order(-SHARE)] %>% unique()
# dim before tokenize
dim(fb)
# how many letters in a title
fb[,
.(Avg = mean(nchar(TITLE), na.rm = T),
STD = sd(nchar(TITLE), na.rm = T),
min = min(nchar(TITLE), na.rm = T),
max = max(nchar(TITLE), na.rm = T)),
SOURCE_TYPE][order(-Avg),]
# how many letters in a text
fb[,
.(Avg = mean(nchar(FULL_TEXT)),
STD = sd(nchar(FULL_TEXT)),
min = min(nchar(FULL_TEXT)),
max = max(nchar(FULL_TEXT))),
SOURCE_TYPE][order(-Avg),]
# youtube <- posts[SOURCE_TYPE == "youtube",]
#instagram <- posts[SOURCE_TYPE == "instagram",]
#rm(posts)
# twitter <- posts[SOURCE_TYPE == "twitter",]
# reddit <- posts[SOURCE_TYPE == "reddit",]
# forum <-  posts[SOURCE_TYPE == "forum",]
# comment <-  posts[SOURCE_TYPE == "comment",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[,.(N := .N,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[,.(ACTIVITY := .N ,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[,(ACTIVITY := .N ,LETTERS := sum(nchar(FULL_TEXT))),FROM][]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][,.(FROM,ACTIVITY,LETTERS)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT))),FROM][,.(FROM,ACTIVITY,LETTERS)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-ACTIVITY)] %>% unique()
fb[FROM == Budicool,]
fb[FROM == "Budicool",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "Općina Lipovljani",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "Tinka Kalajzic",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[FROM == "RCKTU Zabok",]
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[FROM == "Zoran Šprajc",]
# influencers by COMMENT
fb[,COMMENT := sum(COMMENT_COUNT), FROM][,.(COMMENT,FROM)][order(-COMMENT)] %>% unique()
fb[FROM == "Andrea Andrassy",]
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = avg(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
hist(fb,activity)
hist(fb,ACTIVITY)
hist(fb$ACTIVITY)
# influencers by ACTIVITY
fb[,.N,FROM][order(-N)]
fb[FROM == "Naklada Ljevak",]
fb[FROM == "Naklada Ljevak",][order(-DATETIME)]
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = mean(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()
fb[, ":=" (ACTIVITY = .N ,LETTERS = sum(nchar(FULL_TEXT)), EFFORT = LETTERS / ACTIVITY),FROM][,.(FROM,ACTIVITY,LETTERS,EFFORT)][ACTIVITY >= 100][order(-EFFORT)] %>% unique()
