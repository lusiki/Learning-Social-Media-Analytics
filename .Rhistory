# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
dataDT = as.data.table(data)
library(data.table)
dataDT = as.data.table(data)
str(dataDT)
str(data)
glimpse(data)
library(tidyverse)
library(readxl)
library(here)
library(kableExtra)
library(DT)
library(rvest)
library(tidyverse)
library(httr)
glimpse(data)
??glimpse
?glimpse
class(data)
class(dataDT)
dataDT[response.type == "web"][]
dataDT[response.type == "web",
.(minShare = min(response.share_count),
maxShare = max(response.share_count),
avgShare = mean(response.share_count),
stdShare = sd(reresponse.share_count))][]
dataDT[response.type == "web",
.(minShare = min(response.share_count),
maxShare = max(response.share_count),
avgShare = mean(response.share_count),
stdShare = sd(response.share_count))][]
#
dataDT[response.type == "web",
.(minShare = mean(nchar(response.mention)),
maxShare = sd(nchar(response.mention)),
avgShare = min(nchar(response.mention)),
stdShare = max(nchar(response.mention)))][]
# do some descriptive statistics
dataDT[response.type == "web",
.(minShare = min(response.share_count),
maxShare = max(response.share_count),
avgShare = mean(response.share_count),
stdShare = sd(response.share_count))][]
#
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))][]
#
dataDT[response.type == "web",
.(Avg = mean(nchar(response.full_mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))][]
names(data)
View(data)
#
dataDT[response.type == "web",
.(Avg = mean(nchar(response.description)),
STD = sd(nchar(response.description)),
min = min(nchar(response.description)),
max = max(nchar(response.description)))][]
#
dataDT[response.type == "web",
.(Avg = mean(nchar(response.title)),
STD = sd(nchar(response.title)),
min = min(nchar(response.title)),
max = max(nchar(response.title)))][]
# how many letters in a text
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))][]
# how many letters in a title
dataDT[response.type == "web",
.(Avg = mean(nchar(response.title)),
STD = sd(nchar(response.title)),
min = min(nchar(response.title)),
max = max(nchar(response.title)))][]
# how many letters in a text
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))][]
# how many letters in a text
dataDT[,
.(Avg = mean(nchar(response.mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention))),
by = response.type][]
# how many letters in a text
dataDT[,
.(Avg = mean(nchar(response.title)),
STD = sd(nchar(response.title)),
min = min(nchar(response.title)),
max = max(nchar(response.title))),
by = response.type][]
# how many letters in a text
dataDT[,
.(Avg = mean(nchar(response.mention)),
STD = sd(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention))),
by = response.type][]
# FULL SAMPLE
path <- "D:/LUKA/Freelance/Mediatoolkit/FULLDATA"
raw <- list.files(path = path , pattern="xlsx")
raw_path <- paste0(path, "/", raw)
all_raw <- map_df(raw_path, fread)
all_raw <- map_df(raw_path, read_excel)
allDT <- as.data.table(all_raw)
# how many letters in a text by DT
speeDT <- function(){
allDT[,
.(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET))),
by = SOURCE_TYPE]
}
# how many letters in a text by tidy
speeTIDY <- function(){
all_raw %>%
group_by(SOURCE_TYPE) %>%
summarise(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET)))
}
microbenchmark(speeDT,speeTIDY)
microbenchmark::microbenchmark(speeDT,speeTIDY)
library(microbenchmark)
install.packages("microbenchmark")
microbenchmark(speeDT,speeTIDY)
library(microbenchmark)
microbenchmark(speeDT,speeTIDY)
microbenchmark(speeDT,speeTIDY, times = 5)
microbenchmark(speeDT,speeTIDY, times = 10)
microbenchmark(all_raw %>%
group_by(SOURCE_TYPE) %>%
summarise(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET))))
allDT[,
.(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET))),
by = SOURCE_TYPE]
microbenchmark(speeDT(),speeTIDY(), times = 10)
microbenchmark(allDT[,
.(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET))),
by = SOURCE_TYPE])
allDT[,
.(Avg = mean(nchar(MENTION_SNIPPET)),
STD = sd(nchar(MENTION_SNIPPET)),
min = min(nchar(MENTION_SNIPPET)),
max = max(nchar(MENTION_SNIPPET))),
by = SOURCE_TYPE]
microbenchmark(allDT[,
.(Avg = mean(nchar(MENTION_TITLE)),
min = min(nchar(MENTION_TITLE)),
max = max(nchar(MENTION_TITLE))),
by = SOURCE_TYPE])
names(allDT)
microbenchmark(allDT[,
.(Avg = mean(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE))),
by = SOURCE_TYPE])
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))]
microbenchmark(dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))])
library(tictoc)
install.packages("tictoc")
library(tictoc)
tic()
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))])
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))]
tic()
dataDT[response.type == "web",
.(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))]
toc()
tic()
# how many letters in a text by tidy
data %>%
group_by(response.type) %>%
summarise(Avg = mean(nchar(response.mention)),
min = min(nchar(response.mention)),
max = max(nchar(response.mention)))
toc()
# lets check average activity size across
allDT[,
.(Avg = mean(nchar(MENTION_TITLE)),
STD = sd(nchar(MENTION_TITLE)),
min = min(nchar(MENTION_TITLE)),
max = max(nchar(MENTION_TITLE))),
by = SOURCE_TYPE]
# lets check average activity size across
allDT[,
.(Avg = mean(nchar(TITLE)),
STD = sd(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE))),
by = SOURCE_TYPE]
# lets check average activity size across
allDT[,
.(Avg = mean(nchar(TITLE)),
STD = sd(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE))),
by = SOURCE_TYPE]
View(allDT)
allDT[SOURCE_TYPE == "web",]
# lets check average activity size across
allDT[,
.(Avg = mean(nchar(TITLE)),
STD = sd(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE))),
by = SOURCE_TYPE]
allDT[SOURCE_TYPE == "web",.(Avg = mean(nchar(TITLE)),
STD = sd(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE)))]
allDT[SOURCE_TYPE == "twitter",.(Avg = mean(nchar(TITLE)),
STD = sd(nchar(TITLE)),
min = min(nchar(TITLE)),
max = max(nchar(TITLE)))]
tools::package_dependencies("dplyr", recursive = TRUE)[[1]]
names(allDT)
allDT[SOURCE_TYPE == "web" & from == "večernji.hr",]
allDT[SOURCE_TYPE == "web" & FROM == "večernji.hr",]
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",]
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",head()]
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",] %>% head(10)
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",.(TITLE,URL)] %>% head(10)
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & 1:10,.(TITLE,URL)]
# check
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & order(COMMENT_COUNT) &1:10,
.(TITLE,URL)]
# check
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & order(COMMENT_COUNT) &1:10,
.(TITLE,URL,COMMENT_COUNT)]
# check
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & arrange(desc(COMMENT_COUNT)) &1:10,
.(TITLE,URL,COMMENT_COUNT)]
# check
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & order(desc(COMMENT_COUNT)) &1:10,
.(TITLE,URL,COMMENT_COUNT)]
# check 10 articles d+from vecernji.hr
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & order(-COMMENT_COUNT) ,
.(TITLE,URL,COMMENT_COUNT)]
# check 10 articles d+from vecernji.hr
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr" & order(COMMENT_COUNT) ,
.(TITLE,URL,COMMENT_COUNT)]
# check 10 articles d+from vecernji.hr
allDT[SOURCE_TYPE == "web" & FROM == "vecernji.hr",
.(TITLE,URL,COMMENT_COUNT)]
allDT[, DATE := as.Date(DATE,"%Y-%m-%d" )]
str(allDT)
str(allDT$DATE)
# show the results
allDT[1:15,.(DateColumn,TITLE,SOURCE_TYPE)][]
# change the date column into date format
allDT[, DateColumn := as.Date(DATE,"%Y-%m-%d" )] # modify by reference
# show the results
allDT[1:15,.(DateColumn,TITLE,SOURCE_TYPE)][]
# show the results
allDT[1:5,.(DateColumn,TITLE,SOURCE_TYPE)][]
View(allDT)
allDT[SOURCE_TYPE == "instagram",.(TITLE,FOLLOWERS_COUNT)]
allDT[SOURCE_TYPE == "instagram",.(AUTHOR,TITLE,FOLLOWERS_COUNT)]
allDT[SOURCE_TYPE == "instagram",.(AUTHOR,TITLE,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT >0)
allDT[SOURCE_TYPE == "instagram",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0)
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0)
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0) %>%
arrange(desc(FOLLOWERS_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0 & unique(AUTHOR)) %>%
arrange(desc(FOLLOWERS_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0) %>%
arrange(desc(FOLLOWERS_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0)
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
filter(COMMENT_COUNT  > 0) %>%
arrange(desc(COMMENT_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
filter(COMMENT_COUNT  > 0) %>%
distinct(.)
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
filter(COMMENT_COUNT  > 0) %>%
distinct(.) %>%
arrange(desc(COMMENT_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,FOLLOWERS_COUNT)] %>%
filter(FOLLOWERS_COUNT  > 0) %>%
distinct(.) %>%
arrange(desc(FOLLOWERS_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
filter(COMMENT_COUNT  > 0) %>%
distinct(.) %>%
arrange(desc(COMMENT_COUNT))
allDT[SOURCE_TYPE == "facebook",.(AUTHOR,COMMENT_COUNT)] %>%
filter(COMMENT_COUNT  > 0) %>%
distinct(.) %>%
arrange(desc(COMMENT_COUNT)) %>%
head(15)
allDT[,.mean(VIEW_COUNT), by = SOURCE_TYPE]
allDT[,mean(VIEW_COUNT), by = SOURCE_TYPE][]
allDT[,AwerageNoView = mean(VIEW_COUNT), by = SOURCE_TYPE][]
allDT[,.(AwerageNoView = mean(VIEW_COUNT)), by = SOURCE_TYPE][]
names(allDT)
allDT[,.(Awerage = .N), by = SOURCE_TYPE][]
allDT[,.(AwerageNoArticles = .N), by = SOURCE_TYPE][order(Average)]
allDT[,.(AwerageNoArticles = .N), by = SOURCE_TYPE][order(AwerageNoArticles)]
allDT[,.(AwerageNoArticles = .N), by = SOURCE_TYPE][order(-AwerageNoArticles)]
# check if there are articles from Večernji list
data %>%
group_by(response.from) %>%
count %>%
arrange(desc(n)) %>%
head()
# variables and variable types
glimpse(data)
a
# check if there are articles from Večernji list
data %>%
group_by(response.type) %>%
count %>%
arrange(desc(n)) %>%
head()
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
# this is a private infor
source(here::here("Creds/api.R"))
# identify from your Mediatoolkit App
groups <- "182718"
keywords <- "6521533"
# select time period
from_time <- as.character(as.numeric(as.POSIXlt("2022-03-13", format="%Y-%m-%d")))
to_time <- as.character(as.numeric(as.POSIXlt("2022-03-14", format="%Y-%m-%d")))
# number of articles to retrieve
count <- 3000
# connect all parts into request string
requestString <- paste0("https://api.mediatoolkit.com/organizations/126686/groups/",groups,
"/keywords/",keywords,
"/mentions?access_token=",token,
"&from_time=",from_time,
"&to_time=",to_time,
"&count=",count,
"&sort=time&type=all&offset=0&ids_only=false")
# check the request string
requestString
# make GET request to Mediatoolkit server API
API_request <- httr::GET(requestString)
# check the API request object
API_request
# parse the request into JSON object
jS_text <- httr::content(API_request, as = "text", type = "aplication/json", encoding = "UTF-8")
# make a list from JSON object
dataList <- jsonlite::fromJSON(jS_text, flatten = TRUE)
# make a data.frame from list
data <- data.frame(dataList$data)
tools::package_dependencies("data.table", recursive = TRUE)[[1]]
a <- 2+2; a
course.url <- "http://kingaa.github.io/R_Tutorial/"
download.file(paste0(course.url,"Intro1.R"),destfile="Intro1.R",mode="w")
download.file(paste0(course.url,"Intro2.R"),destfile="Intro2.R",mode="w")
download.file(paste0(course.url,"ChlorellaGrowth.csv"),destfile="ChlorellaGrowth.csv",mode="w")
X <- read.csv("ChlorellaGrowth.csv",comment.char='#')
X
