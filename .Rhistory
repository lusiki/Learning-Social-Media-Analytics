ggplot(., aes(N)) +
geom_histogram(bins = 10, color = "#000000", fill = "#0099F8")
web[str_detect(web$FROM, ".hr"),.N,FROM][N >= 50,][order(-N),] %>%
ggplot(., aes(N)) +
geom_histogram(bins = 10, color = "#000000", fill = "#0099F8") +
geom_vline(aes(xintercept = mean(N)), color = "#000000", size = 1.25) +
geom_vline(aes(xintercept = mean(N) + sd(N)), color = "#000000", size = 1, linetype = "dashed") +
geom_vline(aes(xintercept = mean(N) - sd(N)), color = "#000000", size = 1, linetype = "dashed") +
labs(
title = "Histogram of portal activity in Croatia",
subtitle = "Made by LSMA",
caption = "Source: Mediatoolkit dataset",
x = "Number of articles",
y = "Count"
) +
theme_classic() +
theme(
plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
plot.subtitle = element_text(size = 10, face = "bold"),
plot.caption = element_text(face = "italic")
)
web[str_detect(web$FROM, ".hr"),.N,FROM]
web[str_detect(web$FROM, ".hr"),.N,FROM][order(-N),]
web[str_detect(web$FROM, ".hr"),.N,FROM][.All = sum(N)]
web[str_detect(web$FROM, ".hr"),.N,FROM][, ":=" All = sum(N)]
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][order(-N),]
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][, avg := N / All][order(-N),]
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][, avg := (N / All)*100][order(-N),]
web[str_detect(web$FROM, ".hr"),.N,FROM][, All := sum(N)][, avg := (N / All)*100][order(-N),] %>%
ggplot(., aes(avg)) +
geom_histogram(bins = 10, color = "#000000", fill = "#0099F8") +
geom_vline(aes(xintercept = mean(avg)), color = "#000000", size = 1.25) +
geom_vline(aes(xintercept = mean(avg) + sd(avg)), color = "#000000", size = 1, linetype = "dashed") +
geom_vline(aes(xintercept = mean(avg) - sd(avg)), color = "#000000", size = 1, linetype = "dashed") +
labs(
title = "Histogram of portal activity in Croatia",
subtitle = "Made by LSMA",
caption = "Source: Mediatoolkit dataset",
x = "Number of articles",
y = "Count"
) +
theme_classic() +
theme(
plot.title = element_text(color = "#0099F8", size = 16, face = "bold"),
plot.subtitle = element_text(size = 10, face = "bold"),
plot.caption = element_text(face = "italic")
)
all raw[SOURCE_TYPE == "web" & grepl(".hr", unique(all_raw$FROM)),.N,FROM]
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
all_raw[SOURCE_TYPE == "web" & grepl(".hr", unique(all_raw$FROM)),.N,FROM][,.(mean = mean(N),
stdev = sd(N),
total = sum(N))]
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
web[str_detect(web$FROM, ".hr"),.N,FROM]
web[str_detect(web$FROM, ".hr"),.N,unique(FROM)]
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N)),FROM]
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
# check overall descriptives (number of articles)
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
web[str_detect(web$FROM, ".hr"),.N,FROM]
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH),
VIRALITY = sum(VIRALITY),
LIKE = sum(LIKE_COUNT),
COMMENT = sum(COMMENT_COUNT)),
FROM]
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM]
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-N),]
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-N),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
options(digits = 2)
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-N),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-LIKE),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-N),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-COMMENT),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
# check overall descriptives (sort by like)
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-LIKE),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
.(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(N),
stdev = sd(N)),
total = sum(N))]
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
.COMMENT = sum(COMMENT_COUNT, na.rm = T),FROM][,.(mean = mean(N),
stdev = sd(N)),
total = sum(N))]
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(N),
stdev = sd(N)),
total = sum(N))]
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(N),
stdev = sd(N),
total = sum(N))]
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
.(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(N),
stdev = sd(N),
total = sum(N))]
# check overall descriptives (number of comments)
web[str_detect(web$FROM, ".hr"),
.(COMMENT = sum(COMMENT_COUNT, na.rm = T)),FROM][,.(mean = mean(COMMENT),
stdev = sd(COMMENT),
total = sum(COMMENT))]
# check overall descriptives (number of likes)
web[str_detect(web$FROM, ".hr"),
.(LIKE = sum(LIKE_COUNT, na.rm = T)),FROM][,.(mean = mean(LIKE),
stdev = sd(LIKE),
total = sum(LIKE))]
knitr::opts_chunk$set(echo = T, message = F, warning = F)
# check overall descriptives (number of articles)
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
# check overall descriptives (number of likes)
web[str_detect(web$FROM, ".hr"),
.(LIKE = sum(LIKE_COUNT, na.rm = T)),FROM][,.(mean = mean(LIKE),
stdev = sd(LIKE),
total = sum(LIKE))]
# check ranking by No. of likes
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-LIKE),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
# check ranking by reach
web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM][order(-REACH),] %>%
datatable(., rownames = FALSE, options = list(pageLength = 5, scrollX=T) )
webRanking <- web[str_detect(web$FROM, ".hr"),
.(.N,REACH = sum(REACH, na.rm = T),
VIRALITY = sum(VIRALITY, na.rm = T),
LIKE = sum(LIKE_COUNT, na.rm = T),
COMMENT = sum(COMMENT_COUNT, na.rm = T)),
FROM]
reach100 <- webRanking[,top_n(REACH,100)]
reach100 <- webRanking[,top_n(webRanking$REACH,100)]
reach100 <- webRanking[top_n(webRanking$REACH,100),]
reach100 <- webRanking[order(-REACH),]
reach100
reach100 <- webRanking[head(order(-REACH),100),]
reach100
like100 <- webRanking[head(order(-LIKE),100),]
comment100 <- webRanking[head(order(-COMMENT),100),]
# filter 100 biggest by riteria
reach100 <- webRanking[head(order(-REACH),100),.(N, REACH)]
reach100
like100
# filter 100 biggest by riteria
reach100 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)]
reach100
# filter 100 biggest by riteria
reach100 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)]
like100 <- webRanking[head(order(-LIKE),100),.(FROM,N, LIKE)]
comment100 <- webRanking[head(order(-COMMENT),100),.(FROM,N, COMMENT)]
like100
# filter 100 biggest by riteria
portals100 <-  webRanking[head(order(-N),100),.(FROM,N)]
filter
portals100
# check overall descriptives (number of articles)
web[str_detect(web$FROM, ".hr"),.N,FROM][,.(mean = mean(N),stdev = sd(N), total = sum(N))]
library(lsr)
oneSampleTTest(portals100$N, mu=482 )
mean(portals100$N)
pairedSamplesTTest(
formula = ~ reach100$N + like100$N, # one-sided formula listing the two variables
)
pairedSamplesTTest(
formula = ~ reach100$N + like100$N # one-sided formula listing the two variables
)
reach100$N
like100$N
reach100$N
pairedSamplesTTest(
formula = ~ reach100$N + like100$N # one-sided formula listing the two variables
)
pairedSamplesTTest(
formula =  reach100$N ~ like100$N # one-sided formula listing the two variables
)
t.test(
x = reach100$N, # variable 1 is the "test2" scores
y = clike100$N, # variable 2 is the "test1" scores
paired = TRUE # paired test
)
t.test(
x = reach100$N, # variable 1 is the "test2" scores
y = like100$N, # variable 2 is the "test1" scores
paired = TRUE # paired test
)
webRanking[order(-N),.(FROM,N)]
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)][slice(100:200),]
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
# filter 100 biggest by criteria
portals100 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
l
# filter 100 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(100:200)
View(portals200)
View(portals100)
View(portals200)
# filter 100 biggest by criteria
portals100 <-  webRanking[head(order(-N),100),.(FROM,N)]
View(portals100)
View(portals200)
t.test(
x = portals100$N,
y = portals200$N,
paired = FALSE # paired test
)
t.test(
x = portals100$N,
y = portals200$N,
var.equal = TRUE, # Student test
paired = FALSE # paired test
)
t.test(
x = reach100$N,
y = reach200$N,
var.equal = FALSE, # Welch test
paired = FALSE # paired test
)
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[head(order(-REACH),100),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[head(order(-LIKE),100),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[head(order(-COMMENT),100),.(FROM,N, COMMENT)] %>% slice(101:201)
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
mean(reach100$N, na.rm = TRUE)
mean(reach200$N, na.rm = TRUE)
sd(reach100$N, na.rm = TRUE)
sd(reach200$N, na.rm = TRUE)
l
l
l
l
l
# filter 100 - 200 biggest by criteria
portals200 <-  webRanking[order(-N),.(FROM,N)] %>% slice(101:201)
reach200 <- webRanking[order(-REACH),.(FROM,N, REACH)] %>% slice(101:201)
like200 <- webRanking[order(-LIKE),.(FROM,N, LIKE)] %>% slice(101:201)
comment200 <- webRanking[order(-COMMENT),.(FROM,N, COMMENT)] %>% slice(101:201)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
# then run the test
t.test(
x = reach100$N,
y = reach200$N,
var.equal = TRUE, # Student test
paired = FALSE # independent test
)
# then run the test
t.test(
x = portals100$N,
y = portals200$N,
var.equal = FALSE, # Welch test
paired = FALSE # independent test
)
mean(reach100$N)
mean(reach200$N)
sd(reach100$N)
sd(reach200$N)
# then run the test
t.test(
x = reach100$N,
y = reach200$N,
var.equal = TRUE, # Student test
paired = FALSE # independent test
)
mean(portals100$N)
mean(portals200$N)
sd(portals100$N)
sd(portals200$N)
# then run the test
t.test(
x = portals100$N,
y = portals200$N,
var.equal = FALSE, # Welch test
paired = FALSE # independent test
)
names(web)
unique(all_raw$SOURCE_TYPE)
tabulate(all_raw$SOURCE_TYPE)
table(all_raw$SOURCE_TYPE)
# run the GOF test
goodnessOfFitTest( all_raw$SOURCE_TYPE )
# run the GOF test
goodnessOfFitTest(as.factor(all_raw$SOURCE_TYPE))
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", ]
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", .SOURCE_TYPE]
all_raw[SOURCE_TYPE == "web" & SOURCE_TYPE == "forum", .(SOURCE_TYPE)]
all_raw[SOURCE_TYPE == "web" .(SOURCE_TYPE)]
all_raw[SOURCE_TYPE == "web" ,]
all_raw[SOURCE_TYPE == "web&facebook" ,]
all_raw[, Network := ifelse(SOURCE_TYPE != "web", "other")]
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")]
all_raw
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
table(all_raw$Network)
# specify the probablities
probs = c(other = 0.6, web = 0.4)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.55, web = 0.45)
probs
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.65, web = 0.35)
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# specify the probablities
probs = c(other = 0.60, web = 0.40)
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
names(all_raw)
uniqe(all_raw$LOCATIONS)
uniqeu(all_raw$LOCATIONS)
unique(all_raw$LOCATIONS)
View(all_raw)
names(all_raw)
all_raw$DATE
all_raw$TIME
all_raw$DATE
all_raw$TIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%m-%d %H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%d-%m%H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%d-%m %H%M%S")]
all_raw$DTIME
View(all_raw$DATE)
paste(all_raw$DATE,all_raw$TIME)
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y%m%d %H%M%S")]
all_raw$DTIME
all_raw[,DTIME := as.POSIXct(paste(all_raw$DATE,all_raw$TIME),format = "%Y-%m-%d %H:%M:%S")]
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
library(anytime)
all_raw[,DTIME := anytime(paste(all_raw$DATE,all_raw$TIME))]
all_raw$DTIME
str(all_raw)
mydateseq<-seq(as.POSIXct("2016-01-01"), by="2 hour", length.out = 20)
mydateseq
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
library(lubridate)
# make lubridate
all_raw[,DTIME := ymd_hms(paste(all_raw$DATE,all_raw$TIME))]
all_raw$DTIME
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)]
# create breaks
breaks <- hour(hm("00:00", "6:00", "12:00", "18:00", "23:59"))
# labels for the breaks
labels <- c("Night", "Morning", "Afternoon", "Evening")
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)]
all_raw$INTERDAYTIME
table(all_raw$INTERDAYTIME)
# check the activity per network
table(all_raw$SOURCE_TYPE)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)
# we are interested in cross-tabulation
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)
include.lowest=TRUE)]
# check the result
table(all_raw$INTERDAYTIME)
# also peak into activity again (&again)
table(all_raw$SOURCE_TYPE)
# we are interested in cross-tabulated View
xtabs(~ INTERDAYTIME + SOURCE_TYPE, all_raw)
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor()]
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)]
# make a new variable
all_raw[, INTERDAYTIME := cut(x=hour(all_raw$DTIME),
breaks = breaks,
labels = labels,
include.lowest=TRUE)][
,INTERDAYTIME := as.factor(INTERDAYTIME)][
,SOURCE_TYPE := as.factor(SOURCE_TYPE)
]
associationTest( formula = ~ INTERDAYTIME + SOURCE_TYPE, data = all_raw )
# another way to run the test
chisq.test(all_raw$Network, p = probs)
all_raw$Network
# another way to run the test
chisq.test(all_raw$Network, p = c(other = 0.60, web = 0.40))
# create new column for web and everything else
all_raw[, Network := if_else(SOURCE_TYPE != "web", "other", "web")][,Network := as.factor(Network)]
# run the test
goodnessOfFitTest( all_raw$Network, p = probs )
# another way to do the test
chisq.test(as.factor(all_raw$SOURCE_TYPE))
# check the activity per network
table(all_raw$SOURCE_TYPE)
# another way to do the test
chisq.test(as.factor(table(all_raw$SOURCE_TYPE)))
all_raw$SOURCE_TYPE
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT)]
anova
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT, REACH)]
anova
anova <- all_raw[SOURCE_TYPE == "web",.(INTERDAYTIME, LIKE_COUNT, COMMENT_COUNT, REACH)]
anova
anova
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT),COMMENT = mean(COMMENT_COUNT), REACH = mean(REACH)),INTERDAYTIME]
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),COMMENT = mean(COMMENT_COUNT), REACH = mean(REACH)),INTERDAYTIME]
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),
COMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
REACH = mean(REACH, na.rm = TRUE)),
INTERDAYTIME]
anova
# summarise data
anova[,.(LIKE = mean(LIKE_COUNT, na.rm = TRUE),
COMMENT = mean(COMMENT_COUNT, na.rm = TRUE),
REACH = mean(REACH, na.rm = TRUE)),
INTERDAYTIME]
aov( formula = LIKE ~ INTERDAYTIME, data = anova )
aov( formula = LIKE_COUNT ~ INTERDAYTIME, data = anova )
summary(aov( formula = LIKE_COUNT ~ INTERDAYTIME, data = anova))
anova
# test for reach
summary(aov( formula = REACH_COUNT ~ INTERDAYTIME, data = anova))
# test for reach
summary(aov( formula = REACH ~ INTERDAYTIME, data = anova))
